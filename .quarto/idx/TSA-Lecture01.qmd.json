{"title":"25 Spring 439/639 TSA: Lecture 1","markdown":{"yaml":{"title":"25 Spring 439/639 TSA: Lecture 1","author":"Dr Sergey Kushnarev","format":{"html":{"toc":true,"number-sections":true,"code-fold":true}}},"headingText":"Basic definitions","containsRefs":false,"markdown":"\n\n\n## Observed T.S. vs. Probabilistic Model for T.S.\n* We use lower case letters to denote **Observed T.S.(Time Series)**. Example: $y_0,y_1,y_2,\\dots,$ or $\\dots,y_{-2},y_{-1},y_0,y_1,y_2,\\dots.$\n* We use capital letters to denote **Probabilistic Model for T.S.**, which is a series/sequence of RVs. Example: $Y_0,Y_1,Y_2,\\dots,$ or $\\dots,Y_{-2},Y_{-1},Y_0,Y_1,Y_2,\\dots.$ We may also use the following shorthand notations: $(Y_i), (Y_t), (Y_t)_{t=0}^{+\\infty}, (Y_t)_{t=1}^{+\\infty}, (Y_t)_{t=-\\infty}^{+\\infty},$ etc.\n\n## Formal definition of time series\n\n**Definition:** A **time series model** for the observed data $(y_t)$ is a specification of the joint distribution, or possibly just the means and covariances, of a sequence of random variables $\\{ Y(\\omega,t), t\\in T\\}$ of which $(y_t)$ is a realization. \n\nWe will frequently use the term **time series** to denote both the model and the observed data.\n\nIn this definition, the stochastic process can be seen as a set/sequence of random variables $Y(\\omega,t)$ indexed by time $t$. The $T$ is an index set for time $t$. The randomness comes from $\\omega$, which takes values from the sample space $\\Omega$. For this course TSA, in most cases, we have the following setting:\n\n* The time index set $T$ is often $\\mathbb{Z}$ (the set of integers) or $\\mathbb{Z}_{\\ge 0}$ (nonnegative integers).\n* The random variables here are continuous, which means for any fixed $t$, the random variable $Y(\\omega,t)$ follows a continuous distribution.\n\n# Some useful functions to characterize a time series\nTo characterize a time series, we look at the means, variances, autocovariances, autocorrelations of a time series.\n\n## Definitions\n* **Mean Function**: for any $t$, define\n$$\\mu_t := \\mathbb{E}[Y_t].$$\n* **Autocovariance Function(ACVF)**: for any two time indices $t,s$, define the corresponding ACVF as the covariance of $Y_t$ and $Y_s$, i.e.,\n$$\\gamma_{t,s} := Cov(Y_t, Y_s) = \\mathbb{E}[(Y_t-\\mu_t)(Y_s-\\mu_s)] = \\mathbb{E}[Y_t Y_s] -\\mu_t\\mu_s.$$\n* **Variance Function** is the special case of ACVF with $t=s$, which is equal to the variance of $Y_t$,\n$$\\gamma_{t,t} = Cov(Y_t, Y_t) = Var(Y_t).$$\n* **Autocorrelation Function(ACF)**: for any two time indices $t,s$, define the corresponding ACF as the correlation of $Y_t$ and $Y_s$, i.e.,\n$$\\rho_{t,s} := \\frac{Cov(Y_t, Y_s)}{\\sqrt{Var(Y_t) \\cdot Var(Y_s)}} = \\frac{\\gamma_{t,s}}{\\sqrt{\\gamma_{t,t} \\cdot \\gamma_{s,s}}}.$$\n\n## Properties\n* When $t=s$:\n$$\\gamma_{t,t} = Var(Y_t), \\quad \\rho_{t,t}=1.$$\n* **Symmetry**:\n$$\\gamma_{t,s} = \\gamma_{s,t}, \\quad \\rho_{t,s} = \\rho_{s,t}.$$\n* Recall that the Cauchy–Schwarz inequality $Var(X) \\cdot Var(Y) \\ge |Cov(X,Y)|^2$ holds for any two random variables $X,Y$. This immediately gives\n$$|\\gamma_{t,s}| \\le \\sqrt{\\gamma_{t,t} \\cdot \\gamma_{s,s} }, \\quad |\\rho_{t,s}| \\le 1.$$\nSome further remarks:\n    * If $\\rho_{t,s} \\approx \\pm 1$, then $Y_t$ and $Y_s$ are strongly linearly related.\n    * If $\\rho_{t,s} \\approx 0$, then $Y_t$ and $Y_s$ are weakly linearly related.\n    * If $\\rho_{t,s} = 0$, then $Y_t$ and $Y_s$ are uncorrelated.\n* If $Y_t$ and $Y_s$ are independent, then $Y_t$ and $Y_s$ are uncorrelated (assuming both have nonzero and finite second moments). This is because independence directly implies $\\gamma_{t,s}=0$, which further implies $\\rho_{t,s} = 0$.\n* **Bilinearity**: For any positive integers $n,m$, any real numbers $a_1,\\dots,a_n, b_1,\\dots,b_m \\in \\mathbb{R}$, and any time points/indices $t_1,\\dots,t_n,s_1,\\dots,s_m$, the following holds\n$$Cov\\left( \\sum_{i=1}^n a_i Y_{t_i}, \\sum_{j=1}^m b_j Y_{s_j} \\right) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j Cov(Y_{t_i}, Y_{s_j}).$$\nExamples:\n    * $Cov(a_1 Y_{t_1} + a_2 Y_{t_2}, Y_s) = a_1 Cov(Y_{t_1}, Y_s) + a_2 Cov(Y_{t_2}, Y_s).$\n    * $Cov(a_1 Y_{t_1} + a_2 Y_{t_2}, b_1 Y_{s_1} + b_2 Y_{s_2}) = a_1 b_1 Cov(Y_{t_1}, Y_{s_1}) + a_2 b_1 Cov(Y_{t_2}, Y_{s_1}) + a_1 b_2 Cov(Y_{t_1}, Y_{s_2}) + a_2 b_2 Cov(Y_{t_2}, Y_{s_2}).$\n\n# Examples of time series\n\n## Example 1: Linear Regression \nConsider the time series defined by\n$$Y_t= a+ bt + e_t, \\quad e_t \\overset{iid}{\\sim} N(0,\\sigma_e^2).$$\n*Remark: In time series models, such $e_t$ terms are called innovation terms, error terms, or noise terms.* For this example,\n\n* Mean function: $\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[a+ bt + e_t] = a+ bt.$\n* ACVF:\n\n$$\\gamma_{t,s} = Cov(a+ bt + e_t, a+ bs + e_s) = Cov(e_t, e_s) = \\begin{cases}\n0, &\\text{if } t\\neq s\\\\\n\\sigma_e^2, &\\text{if } t=s.\n\\end{cases}$$\n\n* ACF:\n\n$$\\rho_{t,s} = \\begin{cases}\n0, &\\text{if } t\\neq s\\\\\n1, &\\text{if } t=s.\n\\end{cases}$$\n\n## Example 2: Random Walk\nLet $e_1,e_2,e_3,\\dots \\sim IID(0,\\sigma_e^2)$, which means they are iid random variables with mean $0$ and variance $\\sigma_e^2$. Define Random Walk as follows:\n$$\n\\begin{split}\n&Y_0 = 0 \\\\\n&Y_1 = e_1 \\\\\n&Y_2 = e_1+e_2 \\\\\n&Y_3 = e_1+e_2+e_3 \\\\\n&\\cdots\n\\end{split}\n$$\n*Remark: Alternatively, it can be defined by $Y_0=0$ and $Y_{t+1}= Y_t+ e_{t+1}$.* For this example,\n\n* Mean function: $\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[e_1+\\cdots+e_t] = 0.$\n* Variance function: Using the dependence between the $e_t$ terms, we have\n\n$$\\gamma_{t,t} = Var(Y_t) = Var(e_1+\\cdots+e_t) = \\sum_{i=1}^t Var(e_i) + 2\\sum_{1\\le i<j\\le t} Cov(e_i,e_j) = t \\sigma_e^2.$$\nSo this variance function grows linearly with time $t$.\n\n* ACVF: Suppose $1\\le s\\le t$, then we have\n\n$$\\begin{split}\n\\gamma_{s,t} &= Cov(Y_s,Y_t) = Cov(Y_s, Y_s+(Y_t-Y_s)) \\\\\n&= Var(Y_s) + Cov(Y_s, e_{s+1}+ e_{s+2}+ \\cdots +e_t) = s \\sigma_e^2.\n\\end{split}$$\nwhere the last step is because $Y_s$ is independent of the terms $e_{s+1},\\dots,e_t$. If $1\\le t\\le s$, we can derive $\\gamma_{s,t} = t \\sigma_e^2$ in the same way. So we conclude that $\\gamma_{s,t} = \\min\\{s,t\\} \\cdot \\sigma_e^2$ for any $s,t$.\n\n* ACF: Using ACVF, $\\rho_{s,t}= \\frac{\\gamma_{s,t}}{ \\sqrt{\\gamma_{s,s}\\cdot \\gamma_{t,t}}} = \\frac{\\min\\{s,t\\} \\cdot \\sigma_e^2}{\\sqrt{s \\sigma_e^2} \\sqrt{t \\sigma_e^2}} = \\frac{\\min\\{s,t\\}}{ \\sqrt{st}}$. It can also be rewritten as $\\rho_{s,t} = \\min\\{ \\sqrt{\\frac{s}{t}}, \\sqrt{\\frac{t}{s}}\\}$. Example: $\\rho_{1,2}= \\sqrt{\\frac{1}{2}}$, $\\rho_{2,3}= \\sqrt{\\frac{2}{3}}$. Some further observations: $\\rho_{t,t+1}= \\sqrt{\\frac{t}{t+1}} \\to 1$ as $t\\to\\infty$; $\\rho_{1,t}= \\sqrt{\\frac{1}{t}} \\to 0$ as $t\\to\\infty$.\n\n```{r,echo=FALSE, fig.cap=\"Two Sample Paths of a Random Walk\", fig.width=8, fig.height=5}\n# Set seed for reproducibility\nset.seed(439)\n\n# Random walk parameters\nn <- 10  # number of steps\nY0 <- 0\n\n# Simulate two sample paths\ne1 <- rnorm(n)\ne2 <- rnorm(n)\nY1 <- cumsum(c(Y0, e1))\nY2 <- cumsum(c(Y0, e2))\n\n# Create time axis\nt <- 0:n\n\n# Plot the first sample path\nplot(t, Y1, type = \"o\", pch = 4, col = \"purple\", lwd = 2,\n     ylim = range(c(Y1, Y2))*1.3, xlab = \"t\", ylab = expression(Y[t]),\n     main = \"Two Sample Paths of a Random Walk\")\n\n# Add second sample path\nlines(t, Y2, type = \"o\", col = \"red\", lwd = 2, pch = 4)\n\n# Add annotations\ntext(5, Y1[6], \"one realization\\nsample path of a R.W.\", col = \"purple\", pos = 3)\ntext(5, Y2[6], \"another realization\\nsample path of a R.W.\", col = \"red\", pos = 1)\n\n# Add arrows for increments (e.g., e_2 and e_3)\narrows(1, Y1[1], 1, Y1[2], col = \"darkgreen\", length = 0.1)\narrows(2, Y1[2], 2, Y1[3], col = \"darkgreen\", length = 0.1)\narrows(3, Y1[3], 3, Y1[4], col = \"darkgreen\", length = 0.1)\ntext(1.1, mean(Y1[1:2]), expression(e[1]), col = \"darkgreen\", pos = 2)\ntext(2.1, mean(Y1[2:3]), expression(e[2]), col = \"darkgreen\", pos = 2)\ntext(3.1, mean(Y1[3:4]), expression(e[3]), col = \"darkgreen\", pos = 2)\n# label first realization as Y(w1,t)\ntext(5, Y1[6]-1.1, expression(Y(omega[1], t)), col = \"purple\", pos = 3)\n# label second realization as Y(w2,t)\ntext(5, Y2[6]-1.1, expression(Y(omega[2], t)), col = \"red\", pos = 1)\n```\n\n## Example 3: \"Moving Average\"\nLet $(e_t) \\sim IID(0,\\sigma_e^2)$, and define $Y_t = \\frac{1}{2} (e_t + e_{t-1})$. For this example,\n\n* Mean function: $\\mu_t = \\mathbb{E}[Y_t] = \\frac{1}{2} \\mathbb{E}[e_t + e_{t-1}] = 0.$ Note that this mean function does not depend on time $t$.\n* Variance function: $\\gamma_{t,t} = Var(Y_t) = Var(\\frac{1}{2} (e_t + e_{t-1})) = \\frac{1}{4} (Var(e_t) + Var(e_{t-1}) + 2 Cov(e_t, e_{t-1})) = \\frac{1}{2} \\sigma_e^2$. Note that this variance function does not depend on time $t$.\n* ACVF: The case $\\gamma_{t,t}$ reduces to the variance function. Next, we compute $\\gamma_{t,t-1}$.\n\n$$\\begin{split}\n\\gamma_{t,t-1} &= Cov(Y_t, Y_{t-1}) = Cov(\\frac{1}{2} (e_t + e_{t-1}), \\frac{1}{2} (e_{t-1} + e_{t-2})) \\\\\n&= \\frac{1}{4} (Cov(e_{t}, e_{t-1}) + Cov(e_{t}, e_{t-2}) + Cov(e_{t-1}, e_{t-1}) + Cov(e_{t-1}, e_{t-2})) = \\frac{1}{4} \\sigma_e^2.\n\\end{split}$$\n\n  **Exercise:** Show that $\\gamma_{t,t-k}=0$ for $k\\ge 2$.\n\n  Combining all the cases above, we get\n  \n  $$\\gamma_{t,s} = \\begin{cases}\n  \\frac{1}{2} \\sigma_e^2, &\\text{if } t=s \\\\\n  \\frac{1}{4} \\sigma_e^2, &\\text{if } |t-s|=1 \\\\\n  0, &\\text{if } |t-s|\\ge 2.\n  \\end{cases}$$\n  \n* ACF: Using ACVF, we get \n  \n  $$\\rho_{t,s} = \\begin{cases}\n  1 , &\\text{if } t=s \\\\\n  \\frac{1}{2} , &\\text{if } |t-s|=1 \\\\\n  0, &\\text{if } |t-s|\\ge 2.\n  \\end{cases}$$\n  Note that in this example, both ACVF and ACF only depend on the **lag** $t-s$.\n\n# Stationarity\nThe Moving Average example above is an example of **Stationary Time Series**. \"Stationary\" roughly means \"the probability laws for the time series do not change with time\". (This is not a formal definition, but we can get some fuzzy idea from the example above. In the Moving Average example above, the mean function and the variance function do not depend on time $t$; the ACVF and ACF also do not depend on $t$ once the lag $t-s$ is fixed.)\n\nThere are two types of stationarity.\n\n1. Strict stationarity (also called strong stationarity);\n2. Weak stationarity (also called second order stationarity).\n\nHere are the formal definitions of stationarity.\n\n## Strict stationarity\n**Definition:** A stochastic process is strictly stationary if all finite dimensional joint distributions do not change if their indices are shifted by the same amount. (In other words, finite dimensional joint distributions are time invariant.)\n\nThis definition is for general stochastic processes. For a time series (suppose the time index set is $\\mathbb{Z}$), the definition can be stated as follows.\n\n**Definition:** A time series $(\\dots,Y_{-1},Y_0,Y_1,\\dots)$ is strictly stationary if for any positive integer $n$, any distinct integers $t_1,\\dots,t_n \\in \\mathbb{Z}$, and any integer $k\\in \\mathbb{Z}$, the following holds\n$$F_{Y_{t_1}, \\dots, Y_{t_n}} = F_{Y_{t_1 -k}, \\dots, Y_{t_n -k}},$$\ni.e., the joint cdf of $(Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n})$ is same as the joint cdf of $(Y_{t_1 -k}, Y_{t_2 -k}, \\dots, Y_{t_n -k})$.\n\n*Remark:* we may also use the notation $\\overset{D}{=}$ or $\\overset{D}{\\equiv}$ to denote two random variables/vectors have same (joint) distributions.\n\nBy this definition, a strictly stationary time series must satisfy\n\n* $Y_1 \\overset{D}{=} Y_2 \\overset{D}{=} Y_3 \\overset{D}{=} Y_t$ for any $t$;\n* $(Y_1,Y_3) \\overset{D}{=} (Y_2,Y_4) \\overset{D}{=} (Y_{10},Y_{12}) \\overset{D}{=} (Y_t,Y_{t+2})$ for any $t$;\n* $(Y_1,Y_3,Y_7) \\overset{D}{=} (Y_{10},Y_{12},Y_{16}) \\overset{D}{=} (Y_{t},Y_{t+2},Y_{t+6})$ for any $t$;\n* $\\dots$\n\n(Note: the listed properties here are for illustration only. They are just some **necessary conditions** for a strictly stationary time series.)\n\n**Exercise:** Show that for a strictly stationary time series, its ACVF $\\gamma_{t,s}$ only depends on the lag $t-s$. Moreover, by symmetry of ACVF, we can immediately show $\\gamma_{t,s}$ only depends on $|t-s|$.\n\nConsequently, for a strictly stationary time series, we can replace the notation $\\gamma_{t,s}$ by $\\gamma_{|t-s|}$ to simplify the notation, since this transformation is well defined by the previous exercise.","srcMarkdownNoYaml":"\n\n# Basic definitions\n\n## Observed T.S. vs. Probabilistic Model for T.S.\n* We use lower case letters to denote **Observed T.S.(Time Series)**. Example: $y_0,y_1,y_2,\\dots,$ or $\\dots,y_{-2},y_{-1},y_0,y_1,y_2,\\dots.$\n* We use capital letters to denote **Probabilistic Model for T.S.**, which is a series/sequence of RVs. Example: $Y_0,Y_1,Y_2,\\dots,$ or $\\dots,Y_{-2},Y_{-1},Y_0,Y_1,Y_2,\\dots.$ We may also use the following shorthand notations: $(Y_i), (Y_t), (Y_t)_{t=0}^{+\\infty}, (Y_t)_{t=1}^{+\\infty}, (Y_t)_{t=-\\infty}^{+\\infty},$ etc.\n\n## Formal definition of time series\n\n**Definition:** A **time series model** for the observed data $(y_t)$ is a specification of the joint distribution, or possibly just the means and covariances, of a sequence of random variables $\\{ Y(\\omega,t), t\\in T\\}$ of which $(y_t)$ is a realization. \n\nWe will frequently use the term **time series** to denote both the model and the observed data.\n\nIn this definition, the stochastic process can be seen as a set/sequence of random variables $Y(\\omega,t)$ indexed by time $t$. The $T$ is an index set for time $t$. The randomness comes from $\\omega$, which takes values from the sample space $\\Omega$. For this course TSA, in most cases, we have the following setting:\n\n* The time index set $T$ is often $\\mathbb{Z}$ (the set of integers) or $\\mathbb{Z}_{\\ge 0}$ (nonnegative integers).\n* The random variables here are continuous, which means for any fixed $t$, the random variable $Y(\\omega,t)$ follows a continuous distribution.\n\n# Some useful functions to characterize a time series\nTo characterize a time series, we look at the means, variances, autocovariances, autocorrelations of a time series.\n\n## Definitions\n* **Mean Function**: for any $t$, define\n$$\\mu_t := \\mathbb{E}[Y_t].$$\n* **Autocovariance Function(ACVF)**: for any two time indices $t,s$, define the corresponding ACVF as the covariance of $Y_t$ and $Y_s$, i.e.,\n$$\\gamma_{t,s} := Cov(Y_t, Y_s) = \\mathbb{E}[(Y_t-\\mu_t)(Y_s-\\mu_s)] = \\mathbb{E}[Y_t Y_s] -\\mu_t\\mu_s.$$\n* **Variance Function** is the special case of ACVF with $t=s$, which is equal to the variance of $Y_t$,\n$$\\gamma_{t,t} = Cov(Y_t, Y_t) = Var(Y_t).$$\n* **Autocorrelation Function(ACF)**: for any two time indices $t,s$, define the corresponding ACF as the correlation of $Y_t$ and $Y_s$, i.e.,\n$$\\rho_{t,s} := \\frac{Cov(Y_t, Y_s)}{\\sqrt{Var(Y_t) \\cdot Var(Y_s)}} = \\frac{\\gamma_{t,s}}{\\sqrt{\\gamma_{t,t} \\cdot \\gamma_{s,s}}}.$$\n\n## Properties\n* When $t=s$:\n$$\\gamma_{t,t} = Var(Y_t), \\quad \\rho_{t,t}=1.$$\n* **Symmetry**:\n$$\\gamma_{t,s} = \\gamma_{s,t}, \\quad \\rho_{t,s} = \\rho_{s,t}.$$\n* Recall that the Cauchy–Schwarz inequality $Var(X) \\cdot Var(Y) \\ge |Cov(X,Y)|^2$ holds for any two random variables $X,Y$. This immediately gives\n$$|\\gamma_{t,s}| \\le \\sqrt{\\gamma_{t,t} \\cdot \\gamma_{s,s} }, \\quad |\\rho_{t,s}| \\le 1.$$\nSome further remarks:\n    * If $\\rho_{t,s} \\approx \\pm 1$, then $Y_t$ and $Y_s$ are strongly linearly related.\n    * If $\\rho_{t,s} \\approx 0$, then $Y_t$ and $Y_s$ are weakly linearly related.\n    * If $\\rho_{t,s} = 0$, then $Y_t$ and $Y_s$ are uncorrelated.\n* If $Y_t$ and $Y_s$ are independent, then $Y_t$ and $Y_s$ are uncorrelated (assuming both have nonzero and finite second moments). This is because independence directly implies $\\gamma_{t,s}=0$, which further implies $\\rho_{t,s} = 0$.\n* **Bilinearity**: For any positive integers $n,m$, any real numbers $a_1,\\dots,a_n, b_1,\\dots,b_m \\in \\mathbb{R}$, and any time points/indices $t_1,\\dots,t_n,s_1,\\dots,s_m$, the following holds\n$$Cov\\left( \\sum_{i=1}^n a_i Y_{t_i}, \\sum_{j=1}^m b_j Y_{s_j} \\right) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j Cov(Y_{t_i}, Y_{s_j}).$$\nExamples:\n    * $Cov(a_1 Y_{t_1} + a_2 Y_{t_2}, Y_s) = a_1 Cov(Y_{t_1}, Y_s) + a_2 Cov(Y_{t_2}, Y_s).$\n    * $Cov(a_1 Y_{t_1} + a_2 Y_{t_2}, b_1 Y_{s_1} + b_2 Y_{s_2}) = a_1 b_1 Cov(Y_{t_1}, Y_{s_1}) + a_2 b_1 Cov(Y_{t_2}, Y_{s_1}) + a_1 b_2 Cov(Y_{t_1}, Y_{s_2}) + a_2 b_2 Cov(Y_{t_2}, Y_{s_2}).$\n\n# Examples of time series\n\n## Example 1: Linear Regression \nConsider the time series defined by\n$$Y_t= a+ bt + e_t, \\quad e_t \\overset{iid}{\\sim} N(0,\\sigma_e^2).$$\n*Remark: In time series models, such $e_t$ terms are called innovation terms, error terms, or noise terms.* For this example,\n\n* Mean function: $\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[a+ bt + e_t] = a+ bt.$\n* ACVF:\n\n$$\\gamma_{t,s} = Cov(a+ bt + e_t, a+ bs + e_s) = Cov(e_t, e_s) = \\begin{cases}\n0, &\\text{if } t\\neq s\\\\\n\\sigma_e^2, &\\text{if } t=s.\n\\end{cases}$$\n\n* ACF:\n\n$$\\rho_{t,s} = \\begin{cases}\n0, &\\text{if } t\\neq s\\\\\n1, &\\text{if } t=s.\n\\end{cases}$$\n\n## Example 2: Random Walk\nLet $e_1,e_2,e_3,\\dots \\sim IID(0,\\sigma_e^2)$, which means they are iid random variables with mean $0$ and variance $\\sigma_e^2$. Define Random Walk as follows:\n$$\n\\begin{split}\n&Y_0 = 0 \\\\\n&Y_1 = e_1 \\\\\n&Y_2 = e_1+e_2 \\\\\n&Y_3 = e_1+e_2+e_3 \\\\\n&\\cdots\n\\end{split}\n$$\n*Remark: Alternatively, it can be defined by $Y_0=0$ and $Y_{t+1}= Y_t+ e_{t+1}$.* For this example,\n\n* Mean function: $\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[e_1+\\cdots+e_t] = 0.$\n* Variance function: Using the dependence between the $e_t$ terms, we have\n\n$$\\gamma_{t,t} = Var(Y_t) = Var(e_1+\\cdots+e_t) = \\sum_{i=1}^t Var(e_i) + 2\\sum_{1\\le i<j\\le t} Cov(e_i,e_j) = t \\sigma_e^2.$$\nSo this variance function grows linearly with time $t$.\n\n* ACVF: Suppose $1\\le s\\le t$, then we have\n\n$$\\begin{split}\n\\gamma_{s,t} &= Cov(Y_s,Y_t) = Cov(Y_s, Y_s+(Y_t-Y_s)) \\\\\n&= Var(Y_s) + Cov(Y_s, e_{s+1}+ e_{s+2}+ \\cdots +e_t) = s \\sigma_e^2.\n\\end{split}$$\nwhere the last step is because $Y_s$ is independent of the terms $e_{s+1},\\dots,e_t$. If $1\\le t\\le s$, we can derive $\\gamma_{s,t} = t \\sigma_e^2$ in the same way. So we conclude that $\\gamma_{s,t} = \\min\\{s,t\\} \\cdot \\sigma_e^2$ for any $s,t$.\n\n* ACF: Using ACVF, $\\rho_{s,t}= \\frac{\\gamma_{s,t}}{ \\sqrt{\\gamma_{s,s}\\cdot \\gamma_{t,t}}} = \\frac{\\min\\{s,t\\} \\cdot \\sigma_e^2}{\\sqrt{s \\sigma_e^2} \\sqrt{t \\sigma_e^2}} = \\frac{\\min\\{s,t\\}}{ \\sqrt{st}}$. It can also be rewritten as $\\rho_{s,t} = \\min\\{ \\sqrt{\\frac{s}{t}}, \\sqrt{\\frac{t}{s}}\\}$. Example: $\\rho_{1,2}= \\sqrt{\\frac{1}{2}}$, $\\rho_{2,3}= \\sqrt{\\frac{2}{3}}$. Some further observations: $\\rho_{t,t+1}= \\sqrt{\\frac{t}{t+1}} \\to 1$ as $t\\to\\infty$; $\\rho_{1,t}= \\sqrt{\\frac{1}{t}} \\to 0$ as $t\\to\\infty$.\n\n```{r,echo=FALSE, fig.cap=\"Two Sample Paths of a Random Walk\", fig.width=8, fig.height=5}\n# Set seed for reproducibility\nset.seed(439)\n\n# Random walk parameters\nn <- 10  # number of steps\nY0 <- 0\n\n# Simulate two sample paths\ne1 <- rnorm(n)\ne2 <- rnorm(n)\nY1 <- cumsum(c(Y0, e1))\nY2 <- cumsum(c(Y0, e2))\n\n# Create time axis\nt <- 0:n\n\n# Plot the first sample path\nplot(t, Y1, type = \"o\", pch = 4, col = \"purple\", lwd = 2,\n     ylim = range(c(Y1, Y2))*1.3, xlab = \"t\", ylab = expression(Y[t]),\n     main = \"Two Sample Paths of a Random Walk\")\n\n# Add second sample path\nlines(t, Y2, type = \"o\", col = \"red\", lwd = 2, pch = 4)\n\n# Add annotations\ntext(5, Y1[6], \"one realization\\nsample path of a R.W.\", col = \"purple\", pos = 3)\ntext(5, Y2[6], \"another realization\\nsample path of a R.W.\", col = \"red\", pos = 1)\n\n# Add arrows for increments (e.g., e_2 and e_3)\narrows(1, Y1[1], 1, Y1[2], col = \"darkgreen\", length = 0.1)\narrows(2, Y1[2], 2, Y1[3], col = \"darkgreen\", length = 0.1)\narrows(3, Y1[3], 3, Y1[4], col = \"darkgreen\", length = 0.1)\ntext(1.1, mean(Y1[1:2]), expression(e[1]), col = \"darkgreen\", pos = 2)\ntext(2.1, mean(Y1[2:3]), expression(e[2]), col = \"darkgreen\", pos = 2)\ntext(3.1, mean(Y1[3:4]), expression(e[3]), col = \"darkgreen\", pos = 2)\n# label first realization as Y(w1,t)\ntext(5, Y1[6]-1.1, expression(Y(omega[1], t)), col = \"purple\", pos = 3)\n# label second realization as Y(w2,t)\ntext(5, Y2[6]-1.1, expression(Y(omega[2], t)), col = \"red\", pos = 1)\n```\n\n## Example 3: \"Moving Average\"\nLet $(e_t) \\sim IID(0,\\sigma_e^2)$, and define $Y_t = \\frac{1}{2} (e_t + e_{t-1})$. For this example,\n\n* Mean function: $\\mu_t = \\mathbb{E}[Y_t] = \\frac{1}{2} \\mathbb{E}[e_t + e_{t-1}] = 0.$ Note that this mean function does not depend on time $t$.\n* Variance function: $\\gamma_{t,t} = Var(Y_t) = Var(\\frac{1}{2} (e_t + e_{t-1})) = \\frac{1}{4} (Var(e_t) + Var(e_{t-1}) + 2 Cov(e_t, e_{t-1})) = \\frac{1}{2} \\sigma_e^2$. Note that this variance function does not depend on time $t$.\n* ACVF: The case $\\gamma_{t,t}$ reduces to the variance function. Next, we compute $\\gamma_{t,t-1}$.\n\n$$\\begin{split}\n\\gamma_{t,t-1} &= Cov(Y_t, Y_{t-1}) = Cov(\\frac{1}{2} (e_t + e_{t-1}), \\frac{1}{2} (e_{t-1} + e_{t-2})) \\\\\n&= \\frac{1}{4} (Cov(e_{t}, e_{t-1}) + Cov(e_{t}, e_{t-2}) + Cov(e_{t-1}, e_{t-1}) + Cov(e_{t-1}, e_{t-2})) = \\frac{1}{4} \\sigma_e^2.\n\\end{split}$$\n\n  **Exercise:** Show that $\\gamma_{t,t-k}=0$ for $k\\ge 2$.\n\n  Combining all the cases above, we get\n  \n  $$\\gamma_{t,s} = \\begin{cases}\n  \\frac{1}{2} \\sigma_e^2, &\\text{if } t=s \\\\\n  \\frac{1}{4} \\sigma_e^2, &\\text{if } |t-s|=1 \\\\\n  0, &\\text{if } |t-s|\\ge 2.\n  \\end{cases}$$\n  \n* ACF: Using ACVF, we get \n  \n  $$\\rho_{t,s} = \\begin{cases}\n  1 , &\\text{if } t=s \\\\\n  \\frac{1}{2} , &\\text{if } |t-s|=1 \\\\\n  0, &\\text{if } |t-s|\\ge 2.\n  \\end{cases}$$\n  Note that in this example, both ACVF and ACF only depend on the **lag** $t-s$.\n\n# Stationarity\nThe Moving Average example above is an example of **Stationary Time Series**. \"Stationary\" roughly means \"the probability laws for the time series do not change with time\". (This is not a formal definition, but we can get some fuzzy idea from the example above. In the Moving Average example above, the mean function and the variance function do not depend on time $t$; the ACVF and ACF also do not depend on $t$ once the lag $t-s$ is fixed.)\n\nThere are two types of stationarity.\n\n1. Strict stationarity (also called strong stationarity);\n2. Weak stationarity (also called second order stationarity).\n\nHere are the formal definitions of stationarity.\n\n## Strict stationarity\n**Definition:** A stochastic process is strictly stationary if all finite dimensional joint distributions do not change if their indices are shifted by the same amount. (In other words, finite dimensional joint distributions are time invariant.)\n\nThis definition is for general stochastic processes. For a time series (suppose the time index set is $\\mathbb{Z}$), the definition can be stated as follows.\n\n**Definition:** A time series $(\\dots,Y_{-1},Y_0,Y_1,\\dots)$ is strictly stationary if for any positive integer $n$, any distinct integers $t_1,\\dots,t_n \\in \\mathbb{Z}$, and any integer $k\\in \\mathbb{Z}$, the following holds\n$$F_{Y_{t_1}, \\dots, Y_{t_n}} = F_{Y_{t_1 -k}, \\dots, Y_{t_n -k}},$$\ni.e., the joint cdf of $(Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n})$ is same as the joint cdf of $(Y_{t_1 -k}, Y_{t_2 -k}, \\dots, Y_{t_n -k})$.\n\n*Remark:* we may also use the notation $\\overset{D}{=}$ or $\\overset{D}{\\equiv}$ to denote two random variables/vectors have same (joint) distributions.\n\nBy this definition, a strictly stationary time series must satisfy\n\n* $Y_1 \\overset{D}{=} Y_2 \\overset{D}{=} Y_3 \\overset{D}{=} Y_t$ for any $t$;\n* $(Y_1,Y_3) \\overset{D}{=} (Y_2,Y_4) \\overset{D}{=} (Y_{10},Y_{12}) \\overset{D}{=} (Y_t,Y_{t+2})$ for any $t$;\n* $(Y_1,Y_3,Y_7) \\overset{D}{=} (Y_{10},Y_{12},Y_{16}) \\overset{D}{=} (Y_{t},Y_{t+2},Y_{t+6})$ for any $t$;\n* $\\dots$\n\n(Note: the listed properties here are for illustration only. They are just some **necessary conditions** for a strictly stationary time series.)\n\n**Exercise:** Show that for a strictly stationary time series, its ACVF $\\gamma_{t,s}$ only depends on the lag $t-s$. Moreover, by symmetry of ACVF, we can immediately show $\\gamma_{t,s}$ only depends on $|t-s|$.\n\nConsequently, for a strictly stationary time series, we can replace the notation $\\gamma_{t,s}$ by $\\gamma_{|t-s|}$ to simplify the notation, since this transformation is well defined by the previous exercise."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"number-sections":true,"output-file":"TSA-Lecture01.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.41","theme":["cosmo","brand"],"title":"25 Spring 439/639 TSA: Lecture 1","author":"Dr Sergey Kushnarev"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}