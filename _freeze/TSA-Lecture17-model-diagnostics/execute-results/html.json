{
  "hash": "9138defe5936c8eb128b4b3d778c4c4a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"R notebook: Model Diagnostics\"\nauthor: \"Dr Sergey Kushnarev\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n    code-fold: true\n---\n\n\n\n\n## Steps in Time Series Analysis\n\n|   |   |   |   |\n|:---:|:---:|:---:|:---:|\n| Step 1: Model Specification |  Step 2: Parameter Estimation  | **Step 3: Model Checking** | Step 4: Forecasting |\n\n# Model Diagnostics\n\nThis R notebook is based on Chapter 8, Cryer and Chan, *Time Series Analysis with Applications in R*, Springer.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(TSA)\n```\n:::\n\n\n\n\nTwo main approaches to model diagnostics are:\n\n- Residual analysis\n- Analysis of overparameterized models\n\n The residuals are the difference between the observed values and the fitted values from the model. The residuals should be white noise, i.e., they should not exhibit any autocorrelation or other patterns.\n\nThe residuals are calculated as follows:\n$$ \\hat{e}_t = Y_t - \\hat{Y}_t $$\nwhere $Y_t$ is the observed value at time $t$, and $\\hat{Y}_t$ is the fitted value at time $t$.\nThe fitted value is calculated as follows:\n\n$$ \\hat{Y}_t = \\hat{\\phi_0} + \\hat{\\phi}_1 Y_{t-1} + \\hat{\\phi}_2 Y_{t-2} + ... + \\hat{\\phi}_p Y_{t-p} $$\nwhere $\\hat{\\phi_0}$ is the estimated intercept term, and $\\hat{\\phi}_i$ are the estimated AR coefficients.\n\nThe residuals for MA and ARMA models are calculated using the inverted AR($\\infty$) representation of the model. The residuals are calculated as follows:\n$$ \n\\hat{e}_t = Y_t - \\hat{\\pi}_1 Y_{t-1} - \\hat{\\pi}_2 Y_{t-2} - \\hat{\\pi}_3 Y_{t-3} - ... \n$$\nwhere $\\hat{\\pi}_i$ are estimated implicitly as a function of the $\\hat\\theta_i$ and $\\hat\\phi_i$ coefficients.\n\nIn general, we have the following:\n$$\nresidual = actual - predicted\n$$\n\nResiduals should be:\n\n- Uncorrelated (no autocorrelation)\n- identically distributed normal\n- zero mean\n- constant variance\n\n# Residual Analysis\n\n## Color Property Dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exhibit 8.1\ndata(color)\nm1.color=arima(color,order=c(1,0,0), method = \"ML\")\nm1.color\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = color, order = c(1, 0, 0), method = \"ML\")\n\nCoefficients:\n         ar1  intercept\n      0.5706    74.3293\ns.e.  0.1435     1.9151\n\nsigma^2 estimated as 24.83:  log likelihood = -106.07,  aic = 216.15\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(rstandard(m1.color),ylab='Standardized residuals',\n    type='b', \n    main = 'Standardized Residuals from AR(1) Model for Color Property')\nabline(h=0)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n### Assessing Normality of Residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(residuals(m1.color), main='QQ Plot of Residuals from AR(1) Model for Color Property')\nqqline(residuals(m1.color))\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Shapiro-Wilk test for normality, H0: normal, H1: not normal\nshapiro.test(residuals(m1.color)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m1.color)\nW = 0.97536, p-value = 0.6057\n```\n\n\n:::\n:::\n\n\n\n\n### Assessing Independence of Residuals\n\nTo distinguish sample ACF of the time series from the sample ACF of the residuals we use the following notation:\n$$\n\\hat{r}_k = \\text{sample ACF of the residuals},\\qquad\n{r}_k = \\text{sample ACF of $Y_t$}\n$$\n\n\nUnfortunately, the sample autocorrelation function of residuals has a complex structure. We will compare residual ACF with the theoretical ACF of white noise, even though this is not entirely correct. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacf(residuals(m1.color),main='Sample ACF of Residuals from AR(1) Model for Color')\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n### Ljung-Box Test\n\nThe Ljung-Box test is a statistical test that checks for the presence of autocorrelation as a whole in a time series. The null hypothesis of the test is that there is no autocorrelation in the residuals.\n\n$$\nH_0: \\text{all residual ACF is zero for lags $k, k>0$}\\quad\\text{ vs }\\quad H_1: \\text{at least one residual ACF is not zero}\n$$\n\nThe test statistic for a specific parameter value $K$ is given by:\n$$\nQ = n(n+2)\\sum_{k=1}^{h}\\left[\\frac{\\hat{r}_1^2}{n-1}+\\frac{\\hat{r}_2^2}{n-2}+\\ldots+\\frac{\\hat{r}_K^2}{n-K}\\right]\\sim \\chi^2(K-p-q)\n$$\nwhere $n$ is the number of observations, $K$ is the number of lags, and $\\hat{r}_k$ is the sample autocorrelation of residuals at lag $k$.\n\nHere the maximum lag $K$ is selected somewhat arbitrarily but large enough that the $\\psi$-weights are negligible for $j > K$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stats)\nBox.test(residuals(m1.color), lag=6, type=\"Ljung-Box\", fitdf=1) # fitdf is the number of parameters in the model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBox-Ljung test\n\ndata:  residuals(m1.color)\nX-squared = 0.28032, df = 5, p-value = 0.998\n```\n\n\n:::\n:::\n\n\n\n\nWe can show all the diagnostics at once using the `tsdiag` function. The `gof` argument specifies the number of lags to be used in the Ljung-Box test.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsdiag(m1.color,gof=15,omit.initial=F)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-6-1.png){width=1152}\n:::\n:::\n\n\n\n\n## Hare Dataset\n\nRecall, that the most appropriate model for the hare dataset is AR(3) model with $\\phi_2=0$ fitted to the square root of the data. The model is given by:\n$$  \n\\sqrt{Y_t} = \\phi_0 + \\phi_1 \\sqrt{Y_{t-1}} + \\phi_3 \\sqrt{Y_{t-3}} + e_t\n$$\nwhere $\\phi_0$ is the intercept term.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(hare)\nm1.hare=arima(sqrt(hare),order=c(3,0,0))\nm1.hare # the AR(2) coefficient is not significant; it is second in the\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = sqrt(hare), order = c(3, 0, 0))\n\nCoefficients:\n         ar1      ar2      ar3  intercept\n      1.0519  -0.2292  -0.3931     5.6923\ns.e.  0.1877   0.2942   0.1915     0.3371\n\nsigma^2 estimated as 1.066:  log likelihood = -46.54,  aic = 101.08\n```\n\n\n:::\n\n```{.r .cell-code}\n# list of coefficients.\nm2.hare=arima(sqrt(hare),order=c(3,0,0),fixed=c(NA,0,NA,NA)) # fixed the AR(2)\n# coefficient to be 0 via the fixed argument.\nm2.hare\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = sqrt(hare), order = c(3, 0, 0), fixed = c(NA, 0, NA, NA))\n\nCoefficients:\n         ar1  ar2      ar3  intercept\n      0.9190    0  -0.5313     5.6889\ns.e.  0.0791    0   0.0697     0.3179\n\nsigma^2 estimated as 1.088:  log likelihood = -46.85,  aic = 99.69\n```\n\n\n:::\n\n```{.r .cell-code}\n# Note that the intercept term is actually the mean in the centered form\n# of the ARMA model, i.e. if Y(t)=sqrt(hare)-intercept, then the model is\n# Y(t)=0.919*Y(t-1)-0.5313*Y(t-3)+e(t) \n# So the \"true\" intercept equals 5.6889*(1-0.919+0.5313)=3.483, as stated in the book\nplot(rstandard(m2.hare),ylab='Standardized residuals',\n    type='b',\n    main = 'Standardized Residuals from AR(3) Model for sqrt(Hare)')\nabline(h=0)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Bonferoni correction for the number of tests\nalpha=0.05/length(hare) # Bonferroni correction for the number of tests\n# critical value for the two-tailed test\nqnorm(1-alpha/2) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.153563\n```\n\n\n:::\n:::\n\n\n\n\n### Assessing Normality of Residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exhibit 8.5\nqqnorm(residuals(m1.hare), main = 'QQ Plot of Residuals from AR(3) Model for sqrt(Hare)')\nqqline(residuals(m1.hare))\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Shapiro-Wilk test for normality, H0: normal, H1: not normal\nshapiro.test(residuals(m1.hare)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m1.hare)\nW = 0.93509, p-value = 0.06043\n```\n\n\n:::\n:::\n\n\n\n\n### Residual ACF\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsdiag(m1.hare,gof=15,omit.initial=F) \n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-9-1.png){width=1152}\n:::\n:::\n\n\n\n\n## Oil prices\n\nRecall that the most appropriate model for the oil prices dataset is ARIMA(0,1,1) model, aka IMA(1,1) model, fitted to the $\\log Y_t$. The model is given by:\n\n$$\n\\nabla \\log(Y_t) =  e_t - \\theta e_{t-1}\n$$\nwhere $\\nabla \\log(Y_t) = \\log(Y_t) - \\log(Y_{t-1})$ is the first difference of the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(oil.price)\nm1.oil=arima(log(oil.price),order=c(0,1,1))\nm1.oil # IMA(1,1) model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = log(oil.price), order = c(0, 1, 1))\n\nCoefficients:\n         ma1\n      0.2956\ns.e.  0.0693\n\nsigma^2 estimated as 0.006689:  log likelihood = 260.29,  aic = -518.58\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get standardized residuals\nstd_resid <- rstandard(m1.oil)\n\n# Bonferroni-corrected alpha\nalpha <- 0.05 / length(std_resid)  # number of tests = number of residuals\ncrit <- qnorm(1 - alpha / 2)\n\n# Plot\nplot(std_resid, ylab = 'Standardized residuals', type = 'l',\n     main = 'Standardized Residuals from IMA(1,1) Model for log(Oil Price)')\nabline(h = 0)\nabline(h = crit, col = 'red', lty = 2)\nabline(h = -crit, col = 'red', lty = 2)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Assessing Normality of Residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exhibit 8.6\nqqnorm(residuals(m1.oil), main = 'QQ Plot of Residuals from IMA(1,1) Model for log(Oil Price)')\nqqline(residuals(m1.oil))\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Shapiro-Wilk test for normality, H0: normal, H1: not normal\nshapiro.test(residuals(m1.oil)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m1.oil)\nW = 0.96883, p-value = 3.937e-05\n```\n\n\n:::\n:::\n\n\n\n\nOutliers seem to affect the normality of the residuals. \n\n### Residual ACF\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsdiag(m1.oil,gof=15,omit.initial=F)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture17-model-diagnostics_files/figure-html/unnamed-chunk-12-1.png){width=1152}\n:::\n:::\n\n\n\n\n\\newpage\n\n# Overfitting and parameter redundancy\n\nOur second diagnostic tool is **overfitting**. In other words, we can fit a more complex model to the data and check if the additional parameters are significant. If they are not, we can conclude that the simpler model is sufficient.\n\nE.g., if an AR(2) model is fitted to the data, we can check if the AR(3) model is significantly better than the AR(2) model. The original AR(2) model is confirmed if\n\n1. The estimate of $\\phi_3$ is not significantly different from 0.\n2. The estimates of $\\phi_1$ and $\\phi_2$ are not significantly different from the estimates of $\\phi_1$ and $\\phi_2$ from the original AR(2) model.\n\n## Color Property Dataset\n\nLet's fit an AR(2) model to the color property dataset. The original suggested model is AR(1).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exhibit 8.13\nm1.color \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = color, order = c(1, 0, 0), method = \"ML\")\n\nCoefficients:\n         ar1  intercept\n      0.5706    74.3293\ns.e.  0.1435     1.9151\n\nsigma^2 estimated as 24.83:  log likelihood = -106.07,  aic = 216.15\n```\n\n\n:::\n\n```{.r .cell-code}\n# Exhibit 8.14\nm2.color=arima(color,order=c(2,0,0))\nm2.color\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = color, order = c(2, 0, 0))\n\nCoefficients:\n         ar1     ar2  intercept\n      0.5173  0.1005    74.1551\ns.e.  0.1717  0.1815     2.1463\n\nsigma^2 estimated as 24.6:  log likelihood = -105.92,  aic = 217.84\n```\n\n\n:::\n:::\n\n\n\n\nWe can alternatively fit an ARMA(1,1) model to the data. The original suggested model is AR(1).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exhibit 8.15\nm3.color=arima(color,order=c(1,0,1))\nm3.color\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = color, order = c(1, 0, 1))\n\nCoefficients:\n         ar1      ma1  intercept\n      0.6721  -0.1467    74.1730\ns.e.  0.2147   0.2742     2.1357\n\nsigma^2 estimated as 24.63:  log likelihood = -105.94,  aic = 217.88\n```\n\n\n:::\n:::\n\n\n\n\nFor AR(2) and ARMA(1,1) models, the estimates of $\\phi_2$ and $\\theta$ are not significantly different from zero. The estimates of $\\phi_1$  are not significantly different from the estimates of $\\phi_1$ from the original AR(1) model.\n\n## Parameter Redundancy and Lack of Identifiability\n\nAny ARMA($p,q$) model can be expressed as an unidentifiable ARMA($p+1,q+1$) model. \n$$\n\\Phi(B)Y_t=\\Theta(B)e_t\n$$\n\nMultiplying both sides by $(1-cB)$ for any constant $c$ gives us an ARMA($p+1,q+1$) model:\n$$\n(1-cB)\\Phi(B)Y_t=(1-cB)\\Theta(B)e_t\n$$\n\nParameters in this model are not unique (we can take any $c$), and thus are not identifiable. \n\nFollow the rules of a good Time Series Analyst:\n\n1. Specify the original model carefully. Do not overparameterize the model. Start with simpler models.\n2. When overfitting, do not increase the order of both AR and MA terms at the same time. Increase one at a time.\n3. Extend the model in the direction suggested by diagnostics of residuals. E.g., if after fitting an MA(1) model, substantial correlation remains at lag 2, then try an MA(2) model, not an ARMA(1,1) model.\n\n### Color Property Dataset\n\nThe original suggested model is AR(1). Let's fit an ARMA(2,1) model to the data. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exhibit 8.16\nm4.color=arima(color,order=c(2,0,1))\nm4.color\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = color, order = c(2, 0, 1))\n\nCoefficients:\n         ar1     ar2     ma1  intercept\n      0.2189  0.2735  0.3036    74.1653\ns.e.  2.0056  1.1376  2.0650     2.1121\n\nsigma^2 estimated as 24.58:  log likelihood = -105.91,  aic = 219.82\n```\n\n\n:::\n:::\n\n\n\n\nEven though AIC values are close to the original AR(1) model, the estimates of $\\phi_1, \\phi_2, \\theta$ are not significantly different from zero.\n\n\n\n",
    "supporting": [
      "TSA-Lecture17-model-diagnostics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}