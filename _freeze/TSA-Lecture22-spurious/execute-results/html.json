{
  "hash": "d20d61e66c95d67ba54f5e5c231c90c9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"439/639: Vector Time Series and Spurious Correlation\"\nauthor: \"Dr Sergey Kushnarev\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n    code-fold: true\n---\n\n\n\n\n# Cross-covariance and Cross-correlation Functions\n\nCross-covariance and cross-correlation functions are used to analyze the relationship between two time series. The cross-covariance function measures the covariance between two time series at different lags, while the cross-correlation function measures the correlation between two time series at different lags.\n$$\n\\gamma_m(x,y) = Cov(X_t, Y_{t+m})\n$$\n\nCross-correlation function is defined as:\n$$\nr_m(x,y) = \\frac{\\gamma_m(x,y)}{\\sqrt{\\gamma_0(x,x)\\gamma_0(y,y)}}\n$$\n\nwhere $\\gamma_0(x,x)$ and $\\gamma_0(y,y)$ are the variances of the time series $x$ and $y$, respectively.\nThe cross-correlation function is a normalized version of the cross-covariance function, which allows for easier interpretation and comparison between different time series.\n\nOften cross-correlation function is displayed in a grid of plots, where each plot shows the cross-correlation function between two time series at different lags. This allows for a visual representation of the relationship between the two time series and can help identify any patterns or trends in the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(TSA)\n\nset.seed(639)\nX=rnorm(105) \nY=zlag(X,2)+0.5*rnorm(105)\nX=ts(X[-(1:5)],start=1,freq=1)\nY=ts(Y[-(1:5)],start=1,freq=1)\n\n# Make X, Y a multivariate time series\nlibrary(astsa)\nX=ts(X,start=1,freq=1)\nY=ts(Y,start=1,freq=1)\nXY <- ts(cbind(X, Y), start = 1, frequency = 1)\n# Cross-covariance function\nacfm(XY)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n## Bartlett's Theorem for Sample Cross-Correlation Function\n\nBartlett's theorem states that the sample cross-correlation function converges to the true cross-correlation function as the sample size increases. This means that as the number of observations in the time series increases, the sample cross-correlation function will become a more accurate estimate of the true cross-correlation function.\n\n$$\nr_m(X,Y)\\sim N(\\rho_m(X,Y),\\frac{1}{n}\\left[1+2\\sum_{k=1}^\\infty\\rho_k(X)\\rho_k(Y)\\right])\n$$\n\nwhere $\\rho_k(X)$ and $\\rho_k(Y)$ are the autocorrelation functions of the time series $X$ and $Y$, respectively. \n\nThe term $2\\sum_{k=1}^\\infty\\rho_k(X)\\rho_k(Y)$ often result in inflation of the variance of the sample cross-correlation function, which can lead to misleading conclusions about the relationship between the two time series. This is particularly important when analyzing time series data with long memory or strong autocorrelation, as these characteristics can significantly affect the sample cross-correlation function.\n\nIn the example of $X_t\\sim AR(1)$ and $Y_t\\sim AR(1)$ the variance becomes:\n$$\n\\frac{1}{n}\\frac{1+\\phi_X\\phi_Y}{1-\\phi_X\\phi_Y}\n$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(639)\nlibrary(astsa)\nx <- rnorm(100)\ny <- rnorm(100)\nxy <- ts(cbind(x, y), start = 1, frequency = 1)\nacfm(xy)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nccf(xy[,1], xy[,2], lag.max = 20, main = 'Cross-correlation function of X and Y')\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- arima.sim(model = list(order = c(1, 0, 0), ar = 0.8), n = 100)\ny <- arima.sim(model = list(order = c(1, 0, 0), ar = 0.8), n = 100)\nxy <- ts(cbind(x, y), start = 1, frequency = 1)\nacfm(xy)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n## Prewhitening\nPrewhitening is a technique used to remove the effects of autocorrelation from a time series before analyzing the cross-correlation between two time series. This is particularly important when the two time series have strong autocorrelation, as this can lead to misleading conclusions about the relationship between the two time series.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nx <- arima.sim(n = 100, model = list(ar = 0.9))\ny <- 0.5 * x + arima.sim(n = 100, model = list(ar = 0.9))\n\nccf(x,y)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Prewhiten x and filter y\nprewhiten(x, y)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\n\n\n\n## Milk vs Electricity production example\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(milk) \ndata(electricity)\nmilk.electricity <- ts.intersect(milk,log(electricity))\nplot(milk.electricity,yax.flip=T)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nacfm(milk.electricity)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\nccf(milk.electricity[,1],milk.electricity[,2],\n    lag.max=20,main='CCF of Milk and Electricity')\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n:::\n\n\n\n\nLet's prewhiten the data \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nme.dif <- ts.intersect(diff(diff(milk,12)),\n                    diff(diff(log(electricity),12)))\nprewhiten(as.vector(me.dif[,1]),\n        as.vector(me.dif[,2]),\n        ylab='CCF')\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Bluebird potato chips example\n\nOur first example of this section is a sales and price dataset of a certain potato chip\nfrom Bluebird Foods Ltd., New Zealand. The data consist of the log-transformed\nweekly unit sales of large packages of standard potato chips sold and the weekly average\nprice over a period of 104 weeks.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(bluebird)\nplot(bluebird,yax.flip=T)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nacfm(bluebird)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\nprewhiten(y=diff(bluebird)[,1],x=diff(bluebird)[,2],ylab='CCF')\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n:::\n\n\n\n\nWe see strong contemporaneous correlation between the two series. Therefore the model should be\n$$\nLog.sales_t = \\beta_0 + \\beta_1 Price_t + \\epsilon_t\n$$\nThe model can be estimated using OLS regression.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsales=bluebird[,1] \nprice=bluebird[,2]\nchip.m1=lm(sales~price,data=bluebird)\nsummary(chip.m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sales ~ price, data = bluebird)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54950 -0.12373  0.00667  0.13136  0.45170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.890      0.217   73.22   <2e-16 ***\nprice         -2.489      0.126  -19.75   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.188 on 102 degrees of freedom\nMultiple R-squared:  0.7926,\tAdjusted R-squared:  0.7906 \nF-statistic: 389.9 on 1 and 102 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nAnalyzing the residuals of the model, we see that they are not white noise. This indicates that there is still some autocorrelation present in the data, which suggests that the model may not be fully capturing the relationship between the two time series.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacf(residuals(chip.m1),ci.type='ma')\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\npacf(residuals(chip.m1))\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\neacf(residuals(chip.m1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAR/MA\n  0 1 2 3 4 5 6 7 8 9 10 11 12 13\n0 x x x x o o x x o o o  o  o  o \n1 x o o x o o o o o o o  o  o  o \n2 x x o x o o o o o o o  o  o  o \n3 x x o x o o o o o o o  o  o  o \n4 o x x o o o o o o o o  o  o  o \n5 x x x o x o o o o o o  o  o  o \n6 x x o x x x o o o o o  o  o  o \n7 x o x o o o o o o o o  o  o  o \n```\n\n\n:::\n:::\n\n\n\n\nThe diagnostics suggest the model: _______\n\n\\newpage\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchip.m2=arima(sales,order=c(1,0,4),xreg=data.frame(price))\nchip.m2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = sales, order = c(1, 0, 4), xreg = data.frame(price))\n\nCoefficients:\n         ar1      ma1     ma2     ma3     ma4  intercept    price\n      0.1989  -0.0554  0.2521  0.0735  0.5269    15.7792  -2.4234\ns.e.  0.1843   0.1660  0.0865  0.1084  0.1376     0.2166   0.1247\n\nsigma^2 estimated as 0.02556:  log likelihood = 42.35,  aic = -70.69\n```\n\n\n:::\n\n```{.r .cell-code}\nchip.m3=arima(sales,order=c(1,0,4),xreg=data.frame(price),\nfixed=c(NA,0,NA,0,NA,NA,NA)) \nchip.m3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = sales, order = c(1, 0, 4), xreg = data.frame(price), fixed = c(NA, \n    0, NA, 0, NA, NA, NA))\n\nCoefficients:\n         ar1  ma1     ma2  ma3     ma4  intercept    price\n      0.1444    0  0.2676    0  0.5210    15.8396  -2.4588\ns.e.  0.0985    0  0.0858    0  0.1171     0.2027   0.1166\n\nsigma^2 estimated as 0.02572:  log likelihood = 42.09,  aic = -74.18\n```\n\n\n:::\n\n```{.r .cell-code}\nchip.m4=arima(sales,order=c(0,0,4),xreg=data.frame(price),\nfixed=c(0,NA,0,NA,NA,NA)) \nchip.m4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\narima(x = sales, order = c(0, 0, 4), xreg = data.frame(price), fixed = c(0, \n    NA, 0, NA, NA, NA))\n\nCoefficients:\n      ma1     ma2  ma3     ma4  intercept    price\n        0  0.2884    0  0.5416    15.8559  -2.4682\ns.e.    0  0.0794    0  0.1167     0.1909   0.1100\n\nsigma^2 estimated as 0.02623:  log likelihood = 41.02,  aic = -74.05\n```\n\n\n:::\n:::\n\n\n\n\nNote that the regression coefficient estimate on Price is similar to that from the OLS\nregression fit earlier, but the standard error of the estimate is about 10% lower than that\nfrom the simple OLS regression. This illustrates the general result that the simple OLS\nestimator is consistent but the associated standard error is generally not trustworthy.\n\nDiagnosing the residuals from this model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsdiag(chip.m4)\n```\n\n::: {.cell-output-display}\n![](TSA-Lecture22-spurious_files/figure-html/unnamed-chunk-12-1.png){width=864}\n:::\n:::\n",
    "supporting": [
      "TSA-Lecture22-spurious_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}