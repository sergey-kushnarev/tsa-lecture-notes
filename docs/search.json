[
  {
    "objectID": "TSA-Lecture09.html",
    "href": "TSA-Lecture09.html",
    "title": "25 Spring 439/639 TSA: Lecture 9",
    "section": "",
    "text": "Recall that we use regression method to deal with the additive model \\[\n\\underbrace{Y_t}_{observed} = \\underbrace{\\mu_t}_{deterministic} + \\underbrace{X_t}_{stochastic} .\n\\] \\(\\mu_t\\) is estimated (via regression) as \\(\\widehat{\\mu}_t\\). Then we fit some time series model for \\(\\widehat{X}_t = Y_t - \\widehat{\\mu}_t\\).\nLogically, there is an issue here. In regression, errors \\(\\varepsilon_t\\) are assumed iid and normal in the models \\(Y_t = \\mu_t + \\varepsilon_t\\). Typically, we have \\[\n\\varepsilon_t \\overset{iid}\\sim N(0,\\sigma) \\implies \\vec\\varepsilon \\sim MN(0, \\sigma^2 I) \\implies \\operatorname{Var}(\\widehat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}.\n\\] But for time series model, the \\((X_t)\\) are not modeled as iid, (so the covariance matrix \\(V\\) of the error vector in regression step is no longer diagonal) \\[\n\\vec\\varepsilon \\sim MN(0, V)\n\\implies \\operatorname{Var}(\\widehat{\\beta}) = (X^\\top X)^{-1} (X^\\top V X) (X^\\top X)^{-1} .\n\\] So the (co)variance and standard errors of estimates \\(\\widehat{\\beta}\\) are not reliable (in the sense that they are not correctly reported by regression software since regression model has different assumptions).\nDespite this issue, we still have the following result.\nClaim: If the trend is polynomial, trigonometric, trigonometric polynomial, seasonal means, or a linear combination of the above, then for a stationary stochastic component \\((X_t)\\): The least square estimate of the trend has the same variance as the Best linear Unbiased Estimator for large sample sizes."
  },
  {
    "objectID": "TSA-Lecture09.html#differencing-operators",
    "href": "TSA-Lecture09.html#differencing-operators",
    "title": "25 Spring 439/639 TSA: Lecture 9",
    "section": "3.1 Differencing operators",
    "text": "3.1 Differencing operators\nDefine the differencing operator \\(\\nabla\\) (pronounced as nabla) as \\[\n\\nabla Y_t = (1 - B) Y_t = Y_t - Y_{t-1}.\n\\] Example: we can take difference twice: \\[\n\\begin{split}\n\\nabla^2 Y_t &= \\nabla \\left( Y_t - Y_{t-1} \\right) = \\left( Y_t - Y_{t-1} \\right) - \\left( Y_{t-1} - Y_{t-2} \\right) \\\\\n             &= Y_t - 2Y_{t-1} + Y_{t-2} \\\\\n             &= \\left(1 - 2B + B^2\\right) Y_t = (1 - B)^2 Y_t .\n\\end{split}\n\\] Another different but related operation is lag \\(d\\) differencing (which is useful in seasonal models), defined as \\[\n\\nabla_d Y_t = Y_t - Y_{t-d} = \\left( 1 - B^d \\right) Y_t.\n\\] This is different from taking difference \\(d\\) times: \\[\n\\nabla^d Y_t = (1 - B)^d Y_t.\n\\] As a combined example, in a seasonal model, we may take lag \\(s\\) differencing \\(d\\) times: \\[\n\\nabla_s^d Y_t = (1 - B^s)^d Y_t.\n\\]"
  },
  {
    "objectID": "TSA-Lecture09.html#more-examples-of-using-differencing-operators",
    "href": "TSA-Lecture09.html#more-examples-of-using-differencing-operators",
    "title": "25 Spring 439/639 TSA: Lecture 9",
    "section": "3.2 More examples of using differencing operators",
    "text": "3.2 More examples of using differencing operators\nExample 1: consider a trend + stationary model, where the trend is linear in \\(t\\). \\[\nY_t = \\beta_0 + \\beta_1 t + X_t.\n\\] Suppose \\((X_t)\\) is stationary, then \\((Y_t)\\) is not stationary.\nExercise: Why is \\((Y_t)\\) not stationary?\nIf we take the difference: \\[\n\\nabla Y_t = \\left( \\beta_0 + \\beta_1 t + X_t \\right) - \\left( \\beta_0 + \\beta_1 (t-1) + X_{t-1} \\right) = \\beta_1 + \\left( X_t - X_{t-1} \\right).\n\\] We can show \\(\\nabla X_t = X_t - X_{t-1}\\) is stationary from the stationarity of \\((X_t)\\). So in this example, \\(\\nabla Y_t\\) is staionary although \\((Y_t)\\) is not stationary.\nExample 2: consider a random walk model \\[\nY_1=e_1, \\quad Y_t = Y_{t-1} + e_t, \\quad e_t \\sim \\mathrm{iid}(0,\\sigma_e^2).\n\\] As we have seen many times in the course, \\((Y_t)\\) is not stationary. If we take the difference, \\[\n\\nabla Y_t= Y_t -Y_{t-1} = e_t\n\\] \\(\\nabla Y_t\\) is stationary in this example."
  },
  {
    "objectID": "TSA-Lecture09.html#arimapdq",
    "href": "TSA-Lecture09.html#arimapdq",
    "title": "25 Spring 439/639 TSA: Lecture 9",
    "section": "3.3 ARIMA(\\(p,d,q\\))",
    "text": "3.3 ARIMA(\\(p,d,q\\))\nIn Example 2 (random walk) above, we had \\(\\nabla Y_t= e_t\\). Note that \\((e_t)\\) can be seen as an AR(\\(0\\)) or MA(\\(0\\)) or ARMA(\\(0,0\\)) model.\nAlso observe that (suppose \\(W_t = \\nabla Y_t\\)) \\[\n\\begin{split}\nY_t &= Y_{t-1} + W_t = Y_{t-2} + W_{t-1} + W_t \\\\\n&= \\cdots \\\\\n&=\n\\begin{cases}\n\\sum_{i=1}^{t} W_i, & \\text{as a special case, in random walk, } Y_0 = 0\\\\\n\\sum_{i=-m}^{t} W_i, & \\text{more generally, start at } (-m)\n\\end{cases}\n\\end{split}\n\\] which looks like a integrated sum of \\((W_i)\\). So the idea here is: \\(Y_t\\) is ``integrated” \\(W_t\\). In the random walk example above, \\(W_t = e_t\\) is ARMA(\\(0,0\\)) (or AR(\\(0\\)) or MA(\\(0\\))), so we say the random walk \\((Y_t)\\) is ARIMA(\\(0,1,0\\)) (or ARI(\\(0,1\\)) or IMA(\\(1,0\\))). The letter I stands for integrated. In general, we have the following definition.\nDefinition: If \\(\\nabla^d Y_t\\) is an ARMA(\\(p,q\\)), then \\(Y_t\\) is ARIMA(\\(p,d,q\\)).\nLet’s look at another example. Consider the random walk + noise model \\[\nY_t = X_t + \\eta_t = \\sum_{j=1}^t e_j + \\eta_t, \\quad \\eta_t \\sim \\mathrm{iid}(0,\\sigma_\\eta^2),\\quad e_t \\sim \\mathrm{iid}(0,\\sigma_e^2)\n\\] where \\((X_t)\\) is a random walk defined as usual, \\((\\eta_t)\\) is another sequence of noise and \\((\\eta_t)\\) is independent of \\((e_t)\\).\nExercise: show \\((Y_t)\\) is not stationary. (Hint: show \\(Var(Y_t) = t\\sigma_e^2 + \\sigma_\\eta^2\\).)\nBy taking the difference, \\[\n\\begin{split}\nW_t = \\nabla Y_t\n    &= (X_t + \\eta_t) - (X_{t-1} + \\eta_{t-1}) \\\\\n    &= (X_t - X_{t-1}) + \\eta_t - \\eta_{t-1} \\\\\n    &= e_t + \\eta_t - \\eta_{t-1}.\n\\end{split}\n\\] We can see that the ACVF of \\((W_t)\\) satisfies \\(\\gamma_k = 0\\) for \\(k\\ge 2\\). This structure look like the ACVF of an MA(\\(1\\)) process.\nIn fact \\((W_t)\\) is indeed an MA(\\(1\\)) process: The ACVF of \\((W_t)\\) are all zero for lag \\(k\\ge 2\\), so \\((W_t)\\) is \\(1\\)-correlated. Recall an earlier theorem (see lecture 3 when we first defined MA(\\(q\\))), there exist an uncorrelated stationary process \\((\\widetilde{\\epsilon}_t)\\) and a constant \\(\\widetilde{\\theta}\\) such that the time series \\(Z_t = \\widetilde{\\epsilon}_t - \\widetilde{\\theta} \\widetilde{\\epsilon}_{t-1}\\) will have the same ACVF as \\(W_t\\).\nSo \\(W_t \\sim \\mathrm{MA}(1)\\), which implies \\(Y_t \\sim \\mathrm{ARIMA}(0,1,1)\\) (or IMA(\\(1,1\\)))."
  },
  {
    "objectID": "TSA-Lecture08-regression-methods.html",
    "href": "TSA-Lecture08-regression-methods.html",
    "title": "Regression Methods in Time Series Analysis",
    "section": "",
    "text": "Random Walk\nUS Population: Quadratic Trend\nSeasonal Means\nResidual Analysis\nAssessing Normality\nThe Sample Autocorrelation Function\nQuantile-Quantile Plot of Los Angeles Annual Rainfall Series"
  },
  {
    "objectID": "TSA-Lecture08-regression-methods.html#linear-and-quadratic-trends-in-time-series",
    "href": "TSA-Lecture08-regression-methods.html#linear-and-quadratic-trends-in-time-series",
    "title": "Regression Methods in Time Series Analysis",
    "section": "2.1 Linear and Quadratic Trends in Time Series",
    "text": "2.1 Linear and Quadratic Trends in Time Series\n\n2.1.1 Random Walk: apparent linear trend\nWe are going to fit a linear trend to a random walk\n\n\nCode\n# Set seed for reproducibility\nset.seed(439)\n# Generate a random walk\nrw &lt;- cumsum(rnorm(60))\n# Fit a linear trend to the random walk\nmodel1 &lt;- lm(rw ~ time(rw))\n# Print the summary of the model\nsummary(model1)\n\n\n\nCall:\nlm(formula = rw ~ time(rw))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.101 -2.414  1.064  1.970  4.063 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.50344    0.70526  -0.714    0.478    \ntime(rw)     0.14957    0.02011   7.438 5.37e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.697 on 58 degrees of freedom\nMultiple R-squared:  0.4882,    Adjusted R-squared:  0.4794 \nF-statistic: 55.33 on 1 and 58 DF,  p-value: 5.371e-10\n\n\nCode\n# Plot the random walk\nplot(rw,\n     type='o',\n     ylab='y',\n     xlab='Time',\n     main='Random Walk',\n     col='blue',\n     pch=20,\n     cex=0.5)\nabline(model1) # add the fitted least squares line from model1\nlegend('topleft',\n       legend=c('Fitted Trend Line','Random Walk'),\n       col=c('black','blue'),\n       lty=1)\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 US population: quadratic trend\n\n\nCode\n# Load the US population data\ndata(\"uspop\")\n# Fit a quadratic trend to the US population data\nmodel.pop &lt;- lm(uspop~time(uspop)+I(time(uspop)^2)) \n# Extract the coefficients\ncoef &lt;- model.pop$coefficients\n# Print the summary of the model\nsummary(model.pop)\n\n\n\nCall:\nlm(formula = uspop ~ time(uspop) + I(time(uspop)^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5997 -0.7105  0.2669  1.4065  3.9879 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       2.045e+04  8.431e+02   24.25 4.81e-14 ***\ntime(uspop)      -2.278e+01  8.974e-01  -25.38 2.36e-14 ***\nI(time(uspop)^2)  6.345e-03  2.387e-04   26.58 1.14e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.78 on 16 degrees of freedom\nMultiple R-squared:  0.9983,    Adjusted R-squared:  0.9981 \nF-statistic:  4645 on 2 and 16 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Plot the US population data\nplot(y=uspop,\n     x=as.vector(time(uspop)),\n     xlab='Time',\n     ylab='US population 1790-1970',\n     type='o',\n     col = 'blue',\n     main='US Population 1790-1970')\n# Add the fitted quadratic trend line\nlines(x=as.vector(time(uspop)),\n      y=coef[1]+coef[2]*as.vector(time(uspop))+coef[3]*as.vector(time(uspop))^2,\n      col='red')\n\n# Add the legend\nlegend('topleft',\n       legend=c('Fitted Quadratic Trend Line','US Population'),\n       col=c('red','blue'),\n       lty=1)\n\n\n\n\n\n\n\n\n\n\n\n2.1.3 Seasonal (or Cyclical) Means\n\\[\nY_t = \\mu_{t} + X_t\n\\] Here, \\(\\mu_{t}\\) is the seasonal mean and \\(X_t\\) is the seasonal deviation.\n\\[\n\\mu_1 = \\text{mean for January}, \\mu_2 = \\text{mean for February}, \\ldots, \\mu_{12} = \\text{mean for December}\n\\]\nDataset: Average monthly temperatures, Dubuque, Iowa.\n\n\nCode\ndata(\"tempdub\")\nplot(tempdub,\n     ylab='Temperature',\n     type='o',\n     xlab='Time',\n     main='Average Monthly Temperatures, Dubuque, Iowa')\n\n\n\n\n\n\n\n\n\n\n\n2.1.4 Fitting a seasonal means model (without intercept)\n\n\nCode\nmonth. &lt;- season(tempdub) # period added to improve table display\nmodel2 &lt;- lm(tempdub ~ month. + 0) # 0 removes the intercept term\n#model2 &lt;- lm(tempdub ~ month. - 1) # -1 also removes the intercept term\nsummary(model2)\n\n\n\nCall:\nlm(formula = tempdub ~ month. + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.2750 -2.2479  0.1125  1.8896  9.8250 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \nmonth.January     16.608      0.987   16.83   &lt;2e-16 ***\nmonth.February    20.650      0.987   20.92   &lt;2e-16 ***\nmonth.March       32.475      0.987   32.90   &lt;2e-16 ***\nmonth.April       46.525      0.987   47.14   &lt;2e-16 ***\nmonth.May         58.092      0.987   58.86   &lt;2e-16 ***\nmonth.June        67.500      0.987   68.39   &lt;2e-16 ***\nmonth.July        71.717      0.987   72.66   &lt;2e-16 ***\nmonth.August      69.333      0.987   70.25   &lt;2e-16 ***\nmonth.September   61.025      0.987   61.83   &lt;2e-16 ***\nmonth.October     50.975      0.987   51.65   &lt;2e-16 ***\nmonth.November    36.650      0.987   37.13   &lt;2e-16 ***\nmonth.December    23.642      0.987   23.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.419 on 132 degrees of freedom\nMultiple R-squared:  0.9957,    Adjusted R-squared:  0.9953 \nF-statistic:  2569 on 12 and 132 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nplot(tempdub,\n     ylab='Temperature',\n     type='o',\n     xlab='Time',\n     main='Average Monthly Temperatures, Dubuque, Iowa')\npoints(time(tempdub),fitted(model2), col = \"red\", type='o')\n\n\n\n\n\n\n\n\n\nAbove, we have \\[\n\\mu_{t} = \\beta_{t}\n\\]\n\n\n2.1.5 With the intercept term\n\n\nCode\nmodel3 &lt;- lm(tempdub ~ month.) # January is dropped automatically\nsummary(model3)\n\n\n\nCall:\nlm(formula = tempdub ~ month.)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.2750 -2.2479  0.1125  1.8896  9.8250 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       16.608      0.987  16.828  &lt; 2e-16 ***\nmonth.February     4.042      1.396   2.896  0.00443 ** \nmonth.March       15.867      1.396  11.368  &lt; 2e-16 ***\nmonth.April       29.917      1.396  21.434  &lt; 2e-16 ***\nmonth.May         41.483      1.396  29.721  &lt; 2e-16 ***\nmonth.June        50.892      1.396  36.461  &lt; 2e-16 ***\nmonth.July        55.108      1.396  39.482  &lt; 2e-16 ***\nmonth.August      52.725      1.396  37.775  &lt; 2e-16 ***\nmonth.September   44.417      1.396  31.822  &lt; 2e-16 ***\nmonth.October     34.367      1.396  24.622  &lt; 2e-16 ***\nmonth.November    20.042      1.396  14.359  &lt; 2e-16 ***\nmonth.December     7.033      1.396   5.039 1.51e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.419 on 132 degrees of freedom\nMultiple R-squared:  0.9712,    Adjusted R-squared:  0.9688 \nF-statistic: 405.1 on 11 and 132 DF,  p-value: &lt; 2.2e-16\n\n\nWhen we fit with the intercept term, we have \\[\n\\mu_1 = \\beta_0,\\qquad \\mu_{t} = \\beta_0 + \\beta_{t}, \\quad t=2,3,\\ldots,12\n\\]\n\n\n2.1.6 Fitting a seasonal means model with a cosine trend\n\n\nCode\n# Fitting a cosine trend model\nhar. &lt;- harmonic(tempdub,1)\nmodel4 &lt;- lm(tempdub ~ har.)\nsummary(model4)\n\n\n\nCall:\nlm(formula = tempdub ~ har.)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.1580  -2.2756  -0.1457   2.3754  11.2671 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      46.2660     0.3088 149.816  &lt; 2e-16 ***\nhar.cos(2*pi*t) -26.7079     0.4367 -61.154  &lt; 2e-16 ***\nhar.sin(2*pi*t)  -2.1697     0.4367  -4.968 1.93e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.706 on 141 degrees of freedom\nMultiple R-squared:  0.9639,    Adjusted R-squared:  0.9634 \nF-statistic:  1882 on 2 and 141 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Plot the data and the fitted cosine trend\nplot(tempdub,\n     ylab='Temperature',\n     type='o',\n     xlab='Time',\n     main='Average Monthly Temperatures, Dubuque, Iowa')\npoints(time(tempdub),fitted(model4),col='red', type='o')"
  },
  {
    "objectID": "TSA-Lecture08-regression-methods.html#residual-analysis",
    "href": "TSA-Lecture08-regression-methods.html#residual-analysis",
    "title": "Regression Methods in Time Series Analysis",
    "section": "2.2 Residual Analysis",
    "text": "2.2 Residual Analysis\nAfter estimating the trend by \\(\\hat\\mu_t\\), we can predict the unobserved values of the stochastic component \\(X_t\\) by \\(\\hat{X}_t = Y_t - \\hat{\\mu}_t\\).\n\n2.2.1 Seasonal model without the intercept term\n\n\nCode\nplot(y=rstudent(model2),\n     x=as.vector(time(tempdub)),\n     xlab='Time',\n     ylab='Standardized Residuals',\n     type='o',\n     main = 'Residuals from Seasonal Means Model w/o Intercept')\n\n\n\n\n\n\n\n\n\nCode\nplot(y=rstudent(model3),\n     x=as.vector(time(tempdub)),\n     xlab='Time',\n     ylab='Standardized Residuals',\n     type='l',\n     main = 'Residuals from Seasonal Means Model')\n\npoints(y=rstudent(model3),\n       x=as.vector(time(tempdub)),\n       pch=as.vector(season(tempdub)),\n       col = 1:4)\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 Residuals versus Fitted Values from Seasonal Means Model\n\n\nCode\nplot(y=rstudent(model3),\n     x=as.vector(fitted(model3)),\n     xlab='Fitted Trend Values',\n     ylab='Standardized Residuals',type='n',\n     main='Residuals vs Fitted Values from Seasonal Means Model')\n     points(y=rstudent(model3),\n     x=as.vector(fitted(model3)),\n     pch=as.vector(season(tempdub)), \n     col = 1:4)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(y=rstudent(model4),\n     x=as.vector(fitted(model4)),\n     xlab='Fitted Trend Values',\n     ylab='Standardized Residuals',type='n',\n     main='Residuals vs Fitted Values from Cosine Trends')\n     points(y=rstudent(model4),\n     x=as.vector(fitted(model4)),\n     pch=as.vector(season(tempdub)), \n     col = 1:4)\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 Assessing normality\nHistogram is a very rough tool to assess normality\n\n\nCode\nhist(rstudent(model3),\n     xlab='Standardized Residuals',\n     main='Histogram of Residuals from Seasonal Means Model')\n\n\n\n\n\n\n\n\n\nQQ-plot is a much better tool:\n\n\nCode\n# Plotting the QQ-plot\nqqnorm(rstudent(model3),\n       main='QQ-plot of Residuals from Seasonal Means Model')\nqqline(rstudent(model3),col='red',lty=2)\n\n\n\n\n\n\n\n\n\nQQ-plot is an excellent visual diagnostic. We can use a Shapiro-Wilk test to assess normality:\n\n\nCode\n# Shapiro-Wilk test for normality. H0: data is normally distributed\nshapiro.test(rstudent(model3))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rstudent(model3)\nW = 0.9929, p-value = 0.6954\n\n\nTest for independence: runs test\n\n\nCode\n# Runs test for independence. H0: data is independent\nruns(rstudent(model3))\n\n\n$pvalue\n[1] 0.216\n\n$observed.runs\n[1] 65\n\n$expected.runs\n[1] 72.875\n\n$n1\n[1] 69\n\n$n2\n[1] 75\n\n$k\n[1] 0"
  },
  {
    "objectID": "TSA-Lecture08-regression-methods.html#the-sample-autocorrelation-function",
    "href": "TSA-Lecture08-regression-methods.html#the-sample-autocorrelation-function",
    "title": "Regression Methods in Time Series Analysis",
    "section": "2.3 The Sample Autocorrelation Function",
    "text": "2.3 The Sample Autocorrelation Function\nAnother tool for assessing dependency is a sample ACF.\nSample autocorrelation function, for \\(k=1, 2, 3,\\ldots\\):\n\\[\nr_k=\\dfrac{\\sum_{t=k+1}^n(Y_t-\\bar{Y})(Y_{t-k}-\\bar{Y})}{\\sum_{t=1}^n(Y_t-\\bar{Y})^2}\n\\]\n\n2.3.1 Plot of Sample ACF of residuals for seasonal means model versus \\(k\\) (correlogram)\n\n\nCode\nacf(rstudent(model3),\n    main='ACF of Residuals from Seasonal Means Model')\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Sample ACF for residuals of a linear fit to a random walk\n\n\nCode\nplot(y=rstudent(model1),\n     x=as.vector(time(rw)),\n     ylab='Standardized Residuals',\n     xlab='Time',\n     type='o',\n     main='Residuals from Straight Line Fit to Random Walk')\n\n\n\n\n\n\n\n\n\nResiduals versus Fitted Values from Straight Line Fit: larger values of residuals correspond to larger fitted values.\n\n\nCode\nplot(y=rstudent(model1),\n     x=fitted(model1),\n     ylab='Standardized Residuals',\n     xlab='Fitted Trend Line Values',\n     type='p')\n\n\n\n\n\n\n\n\n\nSample Autocorrelation of Residuals from the Straight Line fit to the random walk\n\n\nCode\nacf(rstudent(model1),\n    main='ACF of Residuals from Straight Line Fit to Random Walk')\n\n\n\n\n\n\n\n\n\nWhat if we try completely different approach to the random walk data. Let’s difference the data instead and plot the sample ACF of the differenced data?\n\\[\n\\nabla Y_t = Y_t-Y_{t-1}\n\\]\n\n\nCode\nacf(diff(rw),\n    main='ACF of Differenced Random Walk')\n\n\n\n\n\n\n\n\n\n\n\n2.3.3 Quantile-Quantile Plot of Los Angeles Annual Rainfall Series\nTurns out it’s an iid noise:\n\n\nCode\ndata(\"larain\")\nacf(larain,\n    main='ACF of Los Angeles Annual Rainfall Series')\n\n\n\n\n\n\n\n\n\nIs it normal? Let’s check the QQ-plot:\n\n\nCode\nqqnorm(larain,\n       main='QQ-plot of Los Angeles Annual Rainfall Series') \nqqline(larain)\n\n\n\n\n\n\n\n\n\nQ: is it left-skewed or right-skewed?\n\n\nCode\n# Shapiro Wilk test\nshapiro.test(larain)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  larain\nW = 0.94617, p-value = 0.0001614\n\n\nIf we log-transform the data, we can see that it becomes more symmetric:\n\n\nCode\nqqnorm(log(larain),\n       main='QQ-plot of log-transformed Los Angeles Annual Rainfall Series') \nqqline(log(larain))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro Wilk test\nshapiro.test(log(larain))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  log(larain)\nW = 0.98742, p-value = 0.3643"
  },
  {
    "objectID": "TSA-Lecture06.html",
    "href": "TSA-Lecture06.html",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "",
    "text": "Last time we claimed that: Suppose \\(z_1,...,z_p\\) are roots of the AR polynomial \\(\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p\\), and assume these roots are distinct. Then there exist complex numbers \\(A_1,...,A_p\\), such that the solution to \\[\n\\begin{cases}\n\\text{initial conditions for } (\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\\\\n\\text{recursion equation: } \\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2} + \\cdots + \\phi_p \\gamma_{k-p}, \\quad \\forall k\\ge p\n\\end{cases}\n\\] is given by \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}, \\quad \\forall k \\ge 0 .\n\\]\nHere we prove part of this claim. We can verify \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) (\\(\\forall k \\ge 0\\)) indeed satisfy the recursion, by plugging it into the recursion equations. Suppose \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) for some \\(A_1,...,A_p\\). Then for any \\(k\\ge p\\), \\[\n\\begin{split}\n& \\gamma_k - \\phi_1 \\gamma_{k-1} - \\phi_2 \\gamma_{k-2} - \\cdots - \\phi_p \\gamma_{k-p} \\\\\n=& \\left(A_1 z_1^{-k} + \\cdots + A_p z_p^{-k} \\right) - \\phi_1 \\left(A_1 z_1^{-k+1} + \\cdots + A_p z_p^{-k+1} \\right) - \\phi_2 \\left(A_1 z_1^{-k+2} + \\cdots + A_p z_p^{-k+2} \\right) \\\\\n&- \\cdots - \\phi_p \\left(A_1 z_1^{-k+p} + \\cdots + A_p z_p^{-k+p} \\right) \\\\\n=& A_1 z_1^{-k} (1 - \\phi_1 z_1^1 - \\phi_2 z_1^2 - \\cdots - \\phi_p z_1^p) + \\cdots + A_p z_p^{-k} (1 - \\phi_1 z_p^1 - \\phi_2 z_p^2 - \\cdots - \\phi_p z_p^p) \\\\\n=& A_1 z_1^{-k} \\Phi(z_1) + \\cdots + A_p z_p^{-k} \\Phi(z_p) = 0.\n\\end{split}\n\\] So the constructed \\(\\{\\gamma_k\\}\\) satisfy the recursion equations.\nTo determine the \\((A_1,...,A_p)\\), we use the initial conditions for \\((\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\) (solved from the first \\(p+1\\) YW equations) to solve \\((A_1,...,A_p)\\).\n\n\n\nAssume \\(\\phi_1,\\phi_2 \\in \\mathbb{R}\\) and \\(\\phi_2 \\neq 0\\). Consider the AR(\\(2\\)) equation: \\[\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t .\n\\] The AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x - \\phi_2 x^2\\), which has two roots \\[\nz_{1,2} = \\frac{-\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{2\\phi_2} .\n\\] There are 3 cases for the roots \\(z_1,z_2\\):\n\n\ntwo distinct real roots.\n\n\ntwo repeated real roots. (Note: we can show that if \\(z_1=z_2\\), then they must be real.)\n\n\ntwo distinct (non-real) complex roots.\n\n\nCase (a): \\(z_1, z_2\\in \\mathbb{R}\\) and \\(z_1 \\neq z_2\\). By the claim above, \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k}, \\quad \\forall k \\ge 0 .\n\\] We can use \\(\\gamma_0,\\gamma_1\\) to determine \\((A_1,A_2)\\).\nCase (b): \\(z_1= z_2\\in \\mathbb{R}\\). We claim without proof that, in this case, there exist complex numbers \\((A_1,A_2)\\) such that \\[\n\\gamma_k = (A_1 + A_2 k) z_1^{-k} , \\quad \\forall k \\ge 0 .\n\\] Exercise: verify that \\(\\gamma_k = (A_1 + A_2 k) z_1^{-k}\\) satisfy the recursion equation \\(\\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2}\\).\nIn this case, we still have \\(\\gamma_k \\to 0\\) as \\(k\\to \\infty\\) (assuming \\(|z_1|&gt;1\\), i.e., the causality condition holds) and it decays exponentially.\nWe can also derive the following result: \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\left(1+ \\frac{1+\\phi_2}{1-\\phi_2} k \\right) \\left(\\frac{\\phi_1}{2} \\right)^k,\n\\] and we also have \\(\\rho_k \\to 0\\) exponentially as \\(k\\to \\infty\\) under the causality condition.\nCase (c): \\(z_1 \\neq z_2\\) and \\(z_1,z_2 \\notin \\mathbb{R}\\). In this case, \\(z_1,z_2 \\in \\mathbb{C}\\), and \\(z_2 = \\overline{z_1}\\) since \\(\\phi_1,\\phi_2 \\in \\mathbb{R}\\). By the earlier claim, there exist \\(A_1,A_2 \\in \\mathbb{C}\\) such that \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k}, \\quad \\forall k \\ge 0 .\n\\] We can show that \\(A_2 = \\overline{A_1}\\) using the fact that all the ACVFs \\(\\gamma_k\\) are real.\nWe can also derive the following result: \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = R^k \\cdot \\frac{\\sin(k\\Theta + \\Phi)}{\\sin(\\Phi)} ,\n\\] where the amplitude term \\(R = \\sqrt{-\\phi_2}\\), the frequency term \\(\\Theta\\) satisfies \\(\\cos \\Theta = \\frac{\\phi_1}{2}/ \\sqrt{-\\phi_2}\\), and the phase term \\(\\Phi\\) satisfies \\(\\tan \\Phi = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2}}{\\phi_1} \\cdot \\frac{1-\\phi_2}{1+\\phi_2}\\).\nFrom this result, we can see that under the causality condition, \\(\\{\\gamma_k\\}\\) and \\(\\{\\rho_k\\}\\) both converge to \\(0\\) exponentially as \\(k\\to \\infty\\), since \\(|R|&lt;1\\) (see the exercise below) and \\(\\sin(k\\Theta + \\Phi)\\) is bounded.\nNote: A small issue here is \\(\\Theta\\) and \\(\\Phi\\) are not uniquely determined (modulo \\(2\\pi\\)) by \\(\\cos \\Theta\\) and \\(\\tan \\Phi\\). To make them well defined, they should also satisfy \\(\\sin \\Theta = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\sqrt{-\\phi_2}\\) (or \\(\\tan \\Theta = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\frac{\\phi_1}{2}\\)) and \\(\\sin \\Phi = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\sqrt{\\frac{\\phi_2 (\\phi_1^2 - (1-\\phi_2)^2)}{(1-\\phi_2)^2}}\\) (or \\(\\cos \\Phi = \\frac{\\phi_1(1+\\phi_2)}{2(1-\\phi_2)} \\Big/ \\sqrt{\\frac{\\phi_2 (\\phi_1^2 - (1-\\phi_2)^2)}{(1-\\phi_2)^2}}\\)).\nNote: Our results are similar to the textbook (page 73 of Cryan and Chan) with slight difference in \\(\\tan \\Phi\\).\nExercise: Why \\(-\\phi_2 &gt;0\\)? Why \\(|R|&lt;1\\) under the causality condition? (A harder exercise: try to derive the results above.)\n\n\n\n\nFor AR(\\(p\\)), if the AR polynomial has one root \\(z_1\\) with multiplicity \\(r\\) (i.e. this root repeated \\(r\\) times, \\(z_1 = \\cdots = z_r\\)) and all the other roots are distinct (\\(z_1, z_{r+1}, z_{r+2},\\dots,z_p\\) are distinct), then the ACVFs are in the following form: \\[\n\\gamma_k = (A_1 + A_2 k + \\cdots + A_r k^{r-1}) z_1^{-k} + A_{r+1} z_{r+1}^{-k} + \\cdots + A_{p} z_{p}^{-k}, \\quad \\forall k \\ge 0 .\n\\] which is analogous to a combination of the repeated roots case (see case (b)) and the distinct roots case (the claim at the beginning, or cases (a)(c)).\nFor a generic AR(\\(p\\)), the AR polynomial may have different roots with various multiplicities (the previous case is a special example where the multiplicities of all distinct roots are \\((r,1,\\dots,1)\\)). Then the form of ACVF is a linear combination of cases (a)(b)(c) in a more general way.\nFor MA(\\(q\\)) process, we always have \\[\n\\gamma_k = 0, \\text{ for all } k\\ge q+1.\n\\]"
  },
  {
    "objectID": "TSA-Lecture06.html#acvf-for-arp",
    "href": "TSA-Lecture06.html#acvf-for-arp",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "",
    "text": "Last time we claimed that: Suppose \\(z_1,...,z_p\\) are roots of the AR polynomial \\(\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p\\), and assume these roots are distinct. Then there exist complex numbers \\(A_1,...,A_p\\), such that the solution to \\[\n\\begin{cases}\n\\text{initial conditions for } (\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\\\\n\\text{recursion equation: } \\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2} + \\cdots + \\phi_p \\gamma_{k-p}, \\quad \\forall k\\ge p\n\\end{cases}\n\\] is given by \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}, \\quad \\forall k \\ge 0 .\n\\]\nHere we prove part of this claim. We can verify \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) (\\(\\forall k \\ge 0\\)) indeed satisfy the recursion, by plugging it into the recursion equations. Suppose \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) for some \\(A_1,...,A_p\\). Then for any \\(k\\ge p\\), \\[\n\\begin{split}\n& \\gamma_k - \\phi_1 \\gamma_{k-1} - \\phi_2 \\gamma_{k-2} - \\cdots - \\phi_p \\gamma_{k-p} \\\\\n=& \\left(A_1 z_1^{-k} + \\cdots + A_p z_p^{-k} \\right) - \\phi_1 \\left(A_1 z_1^{-k+1} + \\cdots + A_p z_p^{-k+1} \\right) - \\phi_2 \\left(A_1 z_1^{-k+2} + \\cdots + A_p z_p^{-k+2} \\right) \\\\\n&- \\cdots - \\phi_p \\left(A_1 z_1^{-k+p} + \\cdots + A_p z_p^{-k+p} \\right) \\\\\n=& A_1 z_1^{-k} (1 - \\phi_1 z_1^1 - \\phi_2 z_1^2 - \\cdots - \\phi_p z_1^p) + \\cdots + A_p z_p^{-k} (1 - \\phi_1 z_p^1 - \\phi_2 z_p^2 - \\cdots - \\phi_p z_p^p) \\\\\n=& A_1 z_1^{-k} \\Phi(z_1) + \\cdots + A_p z_p^{-k} \\Phi(z_p) = 0.\n\\end{split}\n\\] So the constructed \\(\\{\\gamma_k\\}\\) satisfy the recursion equations.\nTo determine the \\((A_1,...,A_p)\\), we use the initial conditions for \\((\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\) (solved from the first \\(p+1\\) YW equations) to solve \\((A_1,...,A_p)\\)."
  },
  {
    "objectID": "TSA-Lecture06.html#acvf-for-ar2",
    "href": "TSA-Lecture06.html#acvf-for-ar2",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "",
    "text": "Assume \\(\\phi_1,\\phi_2 \\in \\mathbb{R}\\) and \\(\\phi_2 \\neq 0\\). Consider the AR(\\(2\\)) equation: \\[\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t .\n\\] The AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x - \\phi_2 x^2\\), which has two roots \\[\nz_{1,2} = \\frac{-\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{2\\phi_2} .\n\\] There are 3 cases for the roots \\(z_1,z_2\\):\n\n\ntwo distinct real roots.\n\n\ntwo repeated real roots. (Note: we can show that if \\(z_1=z_2\\), then they must be real.)\n\n\ntwo distinct (non-real) complex roots.\n\n\nCase (a): \\(z_1, z_2\\in \\mathbb{R}\\) and \\(z_1 \\neq z_2\\). By the claim above, \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k}, \\quad \\forall k \\ge 0 .\n\\] We can use \\(\\gamma_0,\\gamma_1\\) to determine \\((A_1,A_2)\\).\nCase (b): \\(z_1= z_2\\in \\mathbb{R}\\). We claim without proof that, in this case, there exist complex numbers \\((A_1,A_2)\\) such that \\[\n\\gamma_k = (A_1 + A_2 k) z_1^{-k} , \\quad \\forall k \\ge 0 .\n\\] Exercise: verify that \\(\\gamma_k = (A_1 + A_2 k) z_1^{-k}\\) satisfy the recursion equation \\(\\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2}\\).\nIn this case, we still have \\(\\gamma_k \\to 0\\) as \\(k\\to \\infty\\) (assuming \\(|z_1|&gt;1\\), i.e., the causality condition holds) and it decays exponentially.\nWe can also derive the following result: \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\left(1+ \\frac{1+\\phi_2}{1-\\phi_2} k \\right) \\left(\\frac{\\phi_1}{2} \\right)^k,\n\\] and we also have \\(\\rho_k \\to 0\\) exponentially as \\(k\\to \\infty\\) under the causality condition.\nCase (c): \\(z_1 \\neq z_2\\) and \\(z_1,z_2 \\notin \\mathbb{R}\\). In this case, \\(z_1,z_2 \\in \\mathbb{C}\\), and \\(z_2 = \\overline{z_1}\\) since \\(\\phi_1,\\phi_2 \\in \\mathbb{R}\\). By the earlier claim, there exist \\(A_1,A_2 \\in \\mathbb{C}\\) such that \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k}, \\quad \\forall k \\ge 0 .\n\\] We can show that \\(A_2 = \\overline{A_1}\\) using the fact that all the ACVFs \\(\\gamma_k\\) are real.\nWe can also derive the following result: \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = R^k \\cdot \\frac{\\sin(k\\Theta + \\Phi)}{\\sin(\\Phi)} ,\n\\] where the amplitude term \\(R = \\sqrt{-\\phi_2}\\), the frequency term \\(\\Theta\\) satisfies \\(\\cos \\Theta = \\frac{\\phi_1}{2}/ \\sqrt{-\\phi_2}\\), and the phase term \\(\\Phi\\) satisfies \\(\\tan \\Phi = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2}}{\\phi_1} \\cdot \\frac{1-\\phi_2}{1+\\phi_2}\\).\nFrom this result, we can see that under the causality condition, \\(\\{\\gamma_k\\}\\) and \\(\\{\\rho_k\\}\\) both converge to \\(0\\) exponentially as \\(k\\to \\infty\\), since \\(|R|&lt;1\\) (see the exercise below) and \\(\\sin(k\\Theta + \\Phi)\\) is bounded.\nNote: A small issue here is \\(\\Theta\\) and \\(\\Phi\\) are not uniquely determined (modulo \\(2\\pi\\)) by \\(\\cos \\Theta\\) and \\(\\tan \\Phi\\). To make them well defined, they should also satisfy \\(\\sin \\Theta = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\sqrt{-\\phi_2}\\) (or \\(\\tan \\Theta = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\frac{\\phi_1}{2}\\)) and \\(\\sin \\Phi = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\sqrt{\\frac{\\phi_2 (\\phi_1^2 - (1-\\phi_2)^2)}{(1-\\phi_2)^2}}\\) (or \\(\\cos \\Phi = \\frac{\\phi_1(1+\\phi_2)}{2(1-\\phi_2)} \\Big/ \\sqrt{\\frac{\\phi_2 (\\phi_1^2 - (1-\\phi_2)^2)}{(1-\\phi_2)^2}}\\)).\nNote: Our results are similar to the textbook (page 73 of Cryan and Chan) with slight difference in \\(\\tan \\Phi\\).\nExercise: Why \\(-\\phi_2 &gt;0\\)? Why \\(|R|&lt;1\\) under the causality condition? (A harder exercise: try to derive the results above.)"
  },
  {
    "objectID": "TSA-Lecture06.html#some-other-remarks-on-acvfs",
    "href": "TSA-Lecture06.html#some-other-remarks-on-acvfs",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "",
    "text": "For AR(\\(p\\)), if the AR polynomial has one root \\(z_1\\) with multiplicity \\(r\\) (i.e. this root repeated \\(r\\) times, \\(z_1 = \\cdots = z_r\\)) and all the other roots are distinct (\\(z_1, z_{r+1}, z_{r+2},\\dots,z_p\\) are distinct), then the ACVFs are in the following form: \\[\n\\gamma_k = (A_1 + A_2 k + \\cdots + A_r k^{r-1}) z_1^{-k} + A_{r+1} z_{r+1}^{-k} + \\cdots + A_{p} z_{p}^{-k}, \\quad \\forall k \\ge 0 .\n\\] which is analogous to a combination of the repeated roots case (see case (b)) and the distinct roots case (the claim at the beginning, or cases (a)(c)).\nFor a generic AR(\\(p\\)), the AR polynomial may have different roots with various multiplicities (the previous case is a special example where the multiplicities of all distinct roots are \\((r,1,\\dots,1)\\)). Then the form of ACVF is a linear combination of cases (a)(b)(c) in a more general way.\nFor MA(\\(q\\)) process, we always have \\[\n\\gamma_k = 0, \\text{ for all } k\\ge q+1.\n\\]"
  },
  {
    "objectID": "TSA-Lecture06.html#a-simple-example-arma11",
    "href": "TSA-Lecture06.html#a-simple-example-arma11",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "3.1 A simple example: ARMA(\\(1,1\\))",
    "text": "3.1 A simple example: ARMA(\\(1,1\\))\nConsider the ARMA(\\(1,1\\)) model \\[\nY_t - \\phi Y_{t-1} = e_t - \\theta e_{t-1}.\n\\] Assume \\(\\phi \\ne \\theta\\).\nQuestion: what happens if \\(\\phi = \\theta\\)? (Answer: the ARMA equation \\((1-\\phi B)Y_t = (1-\\phi B)e_t\\) reduces to \\(Y_t=e_t\\), which is the white noise model.)\nWhen is this process \\((Y_t)\\) causal? By the causality condition above, we need to look at the AR polynomial \\(\\Phi(x) = 1-\\phi x\\). It has a single root \\(z= \\frac{1}{\\phi}\\). So the process is causal if and only if \\(|\\phi|&lt;1\\).\nHereafter, we only consider \\(|\\phi|&lt;1\\), which makes the process causal. Let’s derive the causal GLP representation of \\((Y_t)\\). Suppose \\(Y_t = \\sum_{j=0}^{\\infty} \\psi_j e_{t-j}\\). In last lecture, there were two methods to find \\(\\{\\psi_j\\}\\), (i) inverting the operator, or (ii) plugging into the equation. Here we can simply plug the desired GLP form into the ARMA equation \\(Y_t - \\phi Y_{t-1} = e_t - \\theta e_{t-1}\\): \\[\n\\left( \\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots \\right)\n- \\phi \\left( \\psi_0 e_{t-1} + \\psi_1 e_{t-2} + \\psi_2 e_{t-3} + \\cdots \\right)\n= e_t - \\theta e_{t-1} ,\n\\] \\[\n\\psi_0 e_t\n+ (\\psi_1 - \\phi \\psi_0) e_{t-1}\n+ (\\psi_2 - \\phi \\psi_1) e_{t-2}\n+ (\\psi_3 - \\phi \\psi_2) e_{t-3} + \\cdots\n= e_t - \\theta e_{t-1} .\n\\] So we get \\[\n\\begin{cases}\n\\psi_0 = 1 \\\\\n\\psi_1 - \\phi \\psi_0 = -\\theta \\quad \\implies \\quad \\psi_1 = \\phi - \\theta \\\\\n\\psi_{k} - \\phi \\psi_{k-1} = 0,\\ k \\geq 2\n\\quad \\implies \\quad\n\\psi_{k} = \\phi^{k-1} (\\phi - \\theta) \\text{ for } k \\geq 2 .\n\\end{cases}\n\\] So the GLP representation of ARMA(\\(1,1\\)) is \\[\nY_t = e_t + \\sum_{k=1}^{\\infty} \\phi^{k-1} (\\phi - \\theta) e_{t-k}.\n\\]\nExercise: verify that \\(\\sum_{j=0}^{\\infty} |\\psi_j| &lt; \\infty\\).\nNote: if \\(\\theta=0\\), then the GLP above becomes \\(Y_t = \\sum_{k=0}^{\\infty} \\phi^k e_{t-k}\\), which recovers the same result of AR(\\(1\\))."
  },
  {
    "objectID": "TSA-Lecture04.html",
    "href": "TSA-Lecture04.html",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "",
    "text": "Last time, we defined AR(\\(1\\)), autoregressive of order \\(1\\), as \\[\nY_t = \\phi Y_{t-1} + e_t, \\quad e_t \\sim \\text{iid}(0, \\sigma_e^2)\n\\] and we derived the GLP representaion of AR(1) \\[\nY_t = \\sum_{i=0}^{\\infty} \\phi^i e_{t-i}.\n\\]\n\n\nAssume \\(|\\phi|&lt;1\\). Using GLP results, we can get the mean function, variance function, ACVF, ACF of AR(1).\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = 0\\).\nVariance function: \\[\n\\sigma_t^2 = \\operatorname{Var}(Y_t) = \\sigma_e^2 \\sum_{j=0}^{\\infty} \\psi_j^2\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\phi^{2j} = \\frac{\\sigma_e^2}{1 - \\phi^2}.\n\\]\nACVF: \\[\n\\gamma_k = \\operatorname{Cov}(Y_t, Y_{t-k})\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\psi_j \\psi_{j+k}\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\phi^j \\phi^{j+k}\n= \\sigma_e^2 \\phi^k \\sum_{j=0}^{\\infty} \\phi^{2j}\n= \\frac{\\sigma_e^2 \\phi^k}{1 - \\phi^2} .\n\\]\nACF: \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\phi^k\\).\n\nRemark: \\(\\rho_k \\to 0\\) geometrically as \\(k \\to \\infty\\). And we can see AR(\\(1\\)) is not \\(q\\)-dependent for any finite \\(q\\), since \\(\\rho_k \\ne 0\\) for any \\(k\\) (consider generic \\(\\phi \\neq 0\\)).\nFurther Remark: All AR(\\(p\\)) have this type of behavior, \\(\\gamma_k\\) and \\(\\rho_k\\) decays geometrically.\n\n\n\nUsing the backshift operator \\(B\\) from last lecture, for AR(\\(1\\)), we have \\[\n\\begin{split}\nY_t = \\phi Y_{t-1} + e_t &\\implies e_t = Y_t - \\phi B Y_t = (1-\\phi B) Y_t \\\\\n& \\implies Y_t = (1-\\phi B)^{-1} e_t .\n\\end{split}\n\\] Note that the basic operation \\((1-x)^{-1} = \\frac{1}{1-x} = 1+x+x^2+\\cdots\\) for \\(|x|&lt;1\\). For \\(|\\phi|&lt;1\\), we can do a similar expansion for \\((1-\\phi B)^{-1}\\) here: \\[\n\\begin{split}\nY_t &= (1-\\phi B)^{-1} e_t\n= \\left(1 + \\phi B + (\\phi B)^2 + (\\phi B)^3 + \\cdots \\right) e_t \\\\\n& = \\left(1 + \\phi B + \\phi^2 B^2 + \\phi^3 B^3 + \\cdots \\right) e_t \\\\\n& = e_t + \\phi (B e_t) + \\phi^2 (B^2 e_t) + \\cdots \\\\\n& = e_t + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\cdots \\\\\n& = \\sum_{j=0}^{\\infty} \\phi^j e_{t-j}\n\\end{split}\n\\] which gives the same GLP we derived before.\n\n\n\nIn the previous derivation, we assumed \\(|\\phi|&lt;1\\). What happens if \\(|\\phi|&gt;1\\) for AR(\\(1\\))? In this case, we cannot directly expand the term \\((1-\\phi B)^{-1}\\) since it will not converge. In comparison to \\((1-x)^{-1} = 1+x+x^2+\\cdots\\) for \\(|x|&lt;1\\), we have the following convergent reformulation for the \\(|x|&gt;1\\) case: \\[\n\\begin{split}\n\\frac{1}{1-x}\n& = \\frac{1}{x} \\cdot \\frac{1}{\\frac{1}{x} - 1}\n= -\\frac{1}{x} \\cdot \\frac{1}{1 - \\frac{1}{x}} \\\\\n& = -\\frac{1}{x} -\\frac{1}{x^2} -\\frac{1}{x^3} -\\frac{1}{x^4} - \\cdots \\\\\n& = -\\sum_{j=1}^{\\infty} x^{-j} .\n\\end{split}\n\\] Then we can apply this idea to \\(Y_t = (1-\\phi B)^{-1} e_t\\): \\[\nY_t = (1-\\phi B)^{-1} e_t\n= -\\sum_{j=1}^{\\infty}(\\phi B)^{-j} e_t\n= -\\sum_{j=1}^{\\infty} \\phi^{-j} \\left(B^{-j} e_t\\right)\n= -\\sum_{j=1}^{\\infty} \\phi^{-j} e_{t+j},\n\\] which is still a GLP, but it is a .\nRemark: to see this is still a GLP, \\(Y_t = -\\sum_{j=1}^{\\infty} \\phi^{-j} e_{t+j}\\) can be written in the form \\(Y_t = \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\) and it satisfy \\(\\sum_{j=-\\infty}^{+\\infty} |\\psi_j| &lt; \\infty\\).\nIn summary, considering the parameter \\(\\phi\\) in AR(\\(1\\)), we have:\n\nIf \\(|\\phi|&lt;1\\): it can be represented as a causal GLP, and it is stationary.\nIf \\(|\\phi|&gt;1\\): it can be represented as a non-causal GLP, and it is stationary.\nIf \\(|\\phi|=1\\): it can be shown that this process is not stationary."
  },
  {
    "objectID": "TSA-Lecture04.html#applying-the-glp-results-on-ar1",
    "href": "TSA-Lecture04.html#applying-the-glp-results-on-ar1",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "",
    "text": "Assume \\(|\\phi|&lt;1\\). Using GLP results, we can get the mean function, variance function, ACVF, ACF of AR(1).\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = 0\\).\nVariance function: \\[\n\\sigma_t^2 = \\operatorname{Var}(Y_t) = \\sigma_e^2 \\sum_{j=0}^{\\infty} \\psi_j^2\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\phi^{2j} = \\frac{\\sigma_e^2}{1 - \\phi^2}.\n\\]\nACVF: \\[\n\\gamma_k = \\operatorname{Cov}(Y_t, Y_{t-k})\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\psi_j \\psi_{j+k}\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\phi^j \\phi^{j+k}\n= \\sigma_e^2 \\phi^k \\sum_{j=0}^{\\infty} \\phi^{2j}\n= \\frac{\\sigma_e^2 \\phi^k}{1 - \\phi^2} .\n\\]\nACF: \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\phi^k\\).\n\nRemark: \\(\\rho_k \\to 0\\) geometrically as \\(k \\to \\infty\\). And we can see AR(\\(1\\)) is not \\(q\\)-dependent for any finite \\(q\\), since \\(\\rho_k \\ne 0\\) for any \\(k\\) (consider generic \\(\\phi \\neq 0\\)).\nFurther Remark: All AR(\\(p\\)) have this type of behavior, \\(\\gamma_k\\) and \\(\\rho_k\\) decays geometrically."
  },
  {
    "objectID": "TSA-Lecture04.html#another-method-using-the-operator",
    "href": "TSA-Lecture04.html#another-method-using-the-operator",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "",
    "text": "Using the backshift operator \\(B\\) from last lecture, for AR(\\(1\\)), we have \\[\n\\begin{split}\nY_t = \\phi Y_{t-1} + e_t &\\implies e_t = Y_t - \\phi B Y_t = (1-\\phi B) Y_t \\\\\n& \\implies Y_t = (1-\\phi B)^{-1} e_t .\n\\end{split}\n\\] Note that the basic operation \\((1-x)^{-1} = \\frac{1}{1-x} = 1+x+x^2+\\cdots\\) for \\(|x|&lt;1\\). For \\(|\\phi|&lt;1\\), we can do a similar expansion for \\((1-\\phi B)^{-1}\\) here: \\[\n\\begin{split}\nY_t &= (1-\\phi B)^{-1} e_t\n= \\left(1 + \\phi B + (\\phi B)^2 + (\\phi B)^3 + \\cdots \\right) e_t \\\\\n& = \\left(1 + \\phi B + \\phi^2 B^2 + \\phi^3 B^3 + \\cdots \\right) e_t \\\\\n& = e_t + \\phi (B e_t) + \\phi^2 (B^2 e_t) + \\cdots \\\\\n& = e_t + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\cdots \\\\\n& = \\sum_{j=0}^{\\infty} \\phi^j e_{t-j}\n\\end{split}\n\\] which gives the same GLP we derived before."
  },
  {
    "objectID": "TSA-Lecture04.html#a-brief-discussion-on-the-case-phi1",
    "href": "TSA-Lecture04.html#a-brief-discussion-on-the-case-phi1",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "",
    "text": "In the previous derivation, we assumed \\(|\\phi|&lt;1\\). What happens if \\(|\\phi|&gt;1\\) for AR(\\(1\\))? In this case, we cannot directly expand the term \\((1-\\phi B)^{-1}\\) since it will not converge. In comparison to \\((1-x)^{-1} = 1+x+x^2+\\cdots\\) for \\(|x|&lt;1\\), we have the following convergent reformulation for the \\(|x|&gt;1\\) case: \\[\n\\begin{split}\n\\frac{1}{1-x}\n& = \\frac{1}{x} \\cdot \\frac{1}{\\frac{1}{x} - 1}\n= -\\frac{1}{x} \\cdot \\frac{1}{1 - \\frac{1}{x}} \\\\\n& = -\\frac{1}{x} -\\frac{1}{x^2} -\\frac{1}{x^3} -\\frac{1}{x^4} - \\cdots \\\\\n& = -\\sum_{j=1}^{\\infty} x^{-j} .\n\\end{split}\n\\] Then we can apply this idea to \\(Y_t = (1-\\phi B)^{-1} e_t\\): \\[\nY_t = (1-\\phi B)^{-1} e_t\n= -\\sum_{j=1}^{\\infty}(\\phi B)^{-j} e_t\n= -\\sum_{j=1}^{\\infty} \\phi^{-j} \\left(B^{-j} e_t\\right)\n= -\\sum_{j=1}^{\\infty} \\phi^{-j} e_{t+j},\n\\] which is still a GLP, but it is a .\nRemark: to see this is still a GLP, \\(Y_t = -\\sum_{j=1}^{\\infty} \\phi^{-j} e_{t+j}\\) can be written in the form \\(Y_t = \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\) and it satisfy \\(\\sum_{j=-\\infty}^{+\\infty} |\\psi_j| &lt; \\infty\\).\nIn summary, considering the parameter \\(\\phi\\) in AR(\\(1\\)), we have:\n\nIf \\(|\\phi|&lt;1\\): it can be represented as a causal GLP, and it is stationary.\nIf \\(|\\phi|&gt;1\\): it can be represented as a non-causal GLP, and it is stationary.\nIf \\(|\\phi|=1\\): it can be shown that this process is not stationary."
  },
  {
    "objectID": "TSA-Lecture04.html#y-w-method-applied-on-causal-ar1",
    "href": "TSA-Lecture04.html#y-w-method-applied-on-causal-ar1",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "3.1 Y-W method applied on causal AR(\\(1\\))",
    "text": "3.1 Y-W method applied on causal AR(\\(1\\))\nFirst write down the AR(\\(1\\)) equation \\[\nY_t - \\phi Y_{t-1} = e_t .\n\\] Step 1: multiply by \\(Y_{t-k}\\) (for some \\(k\\ge 0\\)) \\[\nY_t Y_{t-k} - \\phi Y_{t-1} Y_{t-k} = e_t Y_{t-k} .\n\\] Step 2: take expectation, and we call the following \\[\n\\mathbb{E}[Y_t Y_{t-k}] - \\phi \\mathbb{E}[Y_{t-1} Y_{t-k}] = \\mathbb{E}[e_t Y_{t-k}] .\n\\] Note that the AR(\\(1\\)) process \\((Y_t)\\) we considered here is mean zero and stationary, we have \\(\\mathbb{E}[Y_t Y_{t-k}] = \\gamma_k\\) and \\(\\mathbb{E}[Y_{t-1} Y_{t-k}] = \\gamma_{k-1}\\). So the \\(k\\)-th YW equation become \\[\n\\gamma_k - \\phi \\gamma_{k-1} = \\mathbb{E}[e_t Y_{t-k}].\n\\] It remains to deal with the term \\(\\mathbb{E}[e_t Y_{t-k}]\\) in the equation above. For \\(k=0\\), (the \\(0\\)-th YW equation) \\[\n\\mathbb{E}[e_t Y_t]\n    = \\mathbb{E} \\left[ e_t \\sum_{j=0}^{\\infty} \\phi^j e_{t-j} \\right] = \\mathbb{E} \\left[ e_t^2 + \\phi e_t e_{t-1} + \\phi^2 e_t e_{t-2} + \\cdots \\right] = \\mathbb{E}[e_t^2] = \\sigma_e^2 .\n\\] For \\(k\\ge 1\\), note that \\((Y_t)\\) is causal, \\[\n\\mathbb{E}[e_t Y_{t-k}] = \\mathbb{E}\\left[ e_t \\left( e_{t-k} + \\phi e_{t-k-1} + \\cdots \\right) \\right] = 0.\n\\] So the Y-W equations are: (also note \\(\\gamma_{-1} = \\gamma_1\\) in the \\(0\\)-th YW equation) \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 ,\\quad &\\text{($0$th YW eq)}\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = 0 ,\\quad &\\text{($1$st YW eq)}\\\\\n\\gamma_{2} - \\phi \\gamma_{1} = 0 ,\\quad &\\text{($2$nd YW eq)}\\\\\n\\gamma_{3} - \\phi \\gamma_{2} = 0 ,\\quad &\\text{($3$rd YW eq)}\\\\\n\\cdots &\n\\end{cases}\n\\] Then we can solve the \\(\\gamma_k\\) from this system. \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 \\\\\n\\gamma_{1} - \\phi \\gamma_{0} = 0\n\\end{cases}\n\\implies\n\\gamma_{0} = \\frac{\\sigma_e^2}{1-\\phi^2} .\n\\] The remaining equations give the recursive result: \\(\\gamma_{k} = \\phi^{k} \\frac{\\sigma_e^2}{1-\\phi^2}\\) for any \\(k\\ge 0\\).\nRemark: for YW method to work, the process should be causal, as we highlighted above. Question: what would go wrong if causality do not hold?"
  },
  {
    "objectID": "TSA-Lecture04.html#ar-polynomial",
    "href": "TSA-Lecture04.html#ar-polynomial",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "5.1 AR polynomial",
    "text": "5.1 AR polynomial\nNote that the AR(\\(p\\)) equation is \\(Y_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2} - \\cdots - \\phi_p Y_{t-p} = e_t\\), we define the following useful tool.\nDefinition: the AR polynomial (for the AR(\\(p\\)) above) is defined as the following \\(p\\)-th order polynomial (i.e., polynomial with order/degree \\(p\\)) \\[\n\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p .\n\\]\nUsing this notation and the backshift operator, the AR(\\(p\\)) equation can be reformulated as \\[\ne_t = Y_t - \\phi_1 B Y_t - \\cdots - \\phi_p B^p Y_t = \\left(1 - \\phi_1 B - \\phi_2 B^2 - \\cdots - \\phi_p B^p \\right) Y_t = \\Phi(B) Y_t\n\\] so we reach the simple form \\(\\Phi(B) Y_t = e_t\\)."
  },
  {
    "objectID": "TSA-Lecture04.html#causality-condition-for-an-arp-process",
    "href": "TSA-Lecture04.html#causality-condition-for-an-arp-process",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "5.2 Causality condition for an AR(\\(p\\)) process",
    "text": "5.2 Causality condition for an AR(\\(p\\)) process\nThe sufficient and necessary condition for an AR(\\(p\\)) process to be causal is:\nCausality condition: All (complex) roots of the AR polynomial \\(\\Phi(x)\\), are strictly greater than \\(1\\) in absolute value (the modulus of complex number).\nExplanation: \\(\\Phi(x)\\) is a real polynomial of degree \\(p\\), so it has \\(p\\) complex roots, namely \\(z_1,...,z_p \\in \\mathbb{C}\\). Then the conditions says \\(|z_i| &gt;1\\) for all \\(i=1,...,p\\). This also means all the \\(p\\) roots are outside the unit disk in \\(\\mathbb{C}\\).\nExample: consider \\(p=1\\). The AR polynomial AR(\\(1\\)) is \\(\\Phi(x) = 1-\\phi x\\). This \\(\\Phi(x)\\) only has one root \\(z_1 = \\frac{1}{\\phi}\\). The causality condition above reduces to \\(|z_1|&gt;1\\), i.e., \\(|\\frac{1}{\\phi}| &gt; 1\\), which is equivalent to \\(|\\phi| &lt;1\\). This is same as our earlier discussion in this lecture (see the ``\\(|\\phi|&lt;1, |\\phi|&gt;1, |\\phi|=1\\) part”).\nLet’s look at the causality condition above. Assume \\(\\phi_p \\neq 0\\), so the AR polynomial is of order \\(p\\). Consider the AR polynomial with \\(p\\) roots \\(z_1,...,z_p \\in \\mathbb{C}\\). Then we have \\[\n\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p = - \\phi_p (x-z_1) (x-z_2) \\cdots (x-z_p) .\n\\] Note that \\(1 = \\Phi(0) = - \\phi_p (0-z_1) (0-z_2) \\cdots (0-z_p) = (-1)^{p+1} \\phi_p z_1\\cdots z_p\\), which also implies \\(z_1,...,z_p \\neq 0\\). So \\[\n\\begin{split}\n\\Phi(x) &= - \\phi_p (x-z_1) (x-z_2) \\cdots (x-z_p) \\\\\n&= -\\phi_p (-z_1) \\cdots (-z_p) \\left(1-\\frac{x}{z_1}\\right) \\cdots \\left(1-\\frac{x}{z_p}\\right) \\\\\n&= \\left(1-\\frac{x}{z_1}\\right) \\cdots \\left(1-\\frac{x}{z_p}\\right) .\n\\end{split}\n\\] So \\(\\Phi(x) = \\left(1-\\frac{x}{z_1}\\right) \\cdots \\left(1-\\frac{x}{z_p}\\right)\\), and the AR(\\(p\\)) equation becomes \\[\ne_t = \\phi(B) Y_t = \\left(1-\\frac{B}{z_1}\\right) \\cdots \\left(1-\\frac{B}{z_p}\\right) Y_t .\n\\] Since the causality condition requires \\(|z_i|&gt;1\\), so \\(|\\frac{1}{z_i}|&lt;1\\), and each \\(\\left(1-\\frac{B}{z_i}\\right)\\) is invertible in the same way as before, \\(\\left(1-\\frac{B}{z_i}\\right) ^{-1} = 1+ \\frac{B}{z_i} + (\\frac{B}{z_i})^2 + \\cdots\\). The main idea here is, we can invert each \\(\\left(1-\\frac{B}{z_i}\\right)\\), then multiplying them together gives a GLP form of \\(Y_t\\): \\[\n\\begin{split}\nY_t &= \\left(1-\\frac{B}{z_1}\\right)^{-1} \\cdots \\left(1-\\frac{B}{z_p}\\right)^{-1} e_t \\\\\n&= \\left(\\sum_{j=0}^\\infty \\frac{B^j}{z_1^j} \\right) \\cdots \\left(\\sum_{j=0}^\\infty \\frac{B^j}{z_p^j} \\right) e_t \\\\\n&= \\sum_{j=0}^\\infty \\psi_j e_{t-j} .\n\\end{split}\n\\] We will discuss more on this next time."
  },
  {
    "objectID": "TSA-Lecture02.html",
    "href": "TSA-Lecture02.html",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "",
    "text": "Lecture 1: A time series is strictly stationary if all finite dimensional joint distributions are time invariant, i.e., \\[\nF_{Y_{t_1}, \\dots, Y_{t_n}} = F_{Y_{t_1 -k}, \\dots, Y_{t_n -k}}, \\quad \\forall n, \\forall t_1,\\dots,t_n, \\forall k.\n\\]\n\n\n\nSuppose \\((Y_t)\\) is a strictly stationary time series.\n\n\\(Y_t \\overset{D}{=} Y_0\\) for all \\(t\\), i.e., all \\(Y_t\\) are identically distributed. Proof: take \\(n=1\\) in the definition.\nIf the distribution (of \\(Y_t\\)) has finite 2nd moment (which implies finite first moment by \\((\\mathbb{E}[Y])^2 \\le \\mathbb{E}[Y^2] &lt; \\infty\\)), then for any \\(t\\),\n\n\\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[Y_0]\\) does not depend on \\(t\\).\n\\(\\sigma_t^2 = Var(Y_t) = Var(Y_0)\\) does not depend on \\(t\\).\n\nLet \\(n=2\\) in the definition, we have \\((Y_s,Y_t) \\overset{D}{=} (Y_{s-k},Y_{t-k}) \\overset{D}{=} (Y_0, Y_{t-s})\\). Consider the covariance, \\(Cov(Y_s,Y_t) = Cov(Y_0, Y_{t-s})\\), i.e., \\(\\gamma_{s,t} = \\gamma_{0, t-s}\\) which only depends on the lag \\(t-s\\). So for strictly stationary time series, we can simplify the notation by the following definition\n\n\\[\n\\gamma_{t-s} := \\gamma_{0, t-s} = \\gamma_{s,t}.\n\\] By the symmetry of ACVF, \\(\\gamma_{s-t} = \\gamma_{t,s} = \\gamma_{s,t} = \\gamma_{t-s}\\), so \\(\\gamma_k\\) is an even function of \\(k\\) in the sense that \\(\\gamma_{s-t} = \\gamma_{t-s} = \\gamma_{|t-s|}\\). Example: for a strictly stationary time series, \\(\\gamma_5 = \\gamma_{-5}= Cov(Y_0,Y_5)= Cov(Y_1,Y_6) = Cov(Y_t,Y_{t+5}), \\forall t\\).\nIn summary, for a strictly stationary time series with finite 2nd moment,\n\nMean Function: \\(\\mu_t=\\mu\\), \\(\\forall t\\in \\mathbb{Z}\\).\nVariance Function: \\(Var(Y_t) = \\gamma_{t,t}= \\gamma_0 = \\sigma^2\\), \\(\\forall t\\in \\mathbb{Z}\\).\nACVF: \\(\\gamma_k = Cov(Y_t,Y_{t-k}) = Cov(Y_{t-k},Y_t) = \\gamma_{-k}\\), \\(\\forall k,t\\in \\mathbb{Z}\\).\nACF: \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\\) (assuming \\(\\gamma_0 \\neq 0\\)).\n\n\n\n\nDefinition: A time series \\((Y_t)\\), \\(t\\in \\mathbb{Z}\\), satisfying the following three conditions,\n\n\\(\\mu_t=\\mu\\) for some (finite) constant \\(\\mu\\), \\(\\forall t\\in \\mathbb{Z}\\),\n\\(Var(Y_t) =\\sigma^2\\) for some (finite) constant \\(\\sigma^2\\), \\(\\forall t\\in \\mathbb{Z}\\),\n\\(Cov(Y_t,Y_{t-k}) = \\gamma_k\\) for some function \\(\\gamma_k\\) that only depends on the lag \\(k\\) and does not depend on the time \\(t\\),\n\nis called weakly stationary/ stationary / second order stationary / covariance stationary."
  },
  {
    "objectID": "TSA-Lecture02.html#strict-stationarity",
    "href": "TSA-Lecture02.html#strict-stationarity",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "",
    "text": "Lecture 1: A time series is strictly stationary if all finite dimensional joint distributions are time invariant, i.e., \\[\nF_{Y_{t_1}, \\dots, Y_{t_n}} = F_{Y_{t_1 -k}, \\dots, Y_{t_n -k}}, \\quad \\forall n, \\forall t_1,\\dots,t_n, \\forall k.\n\\]"
  },
  {
    "objectID": "TSA-Lecture02.html#properties-of-strict-stationarity",
    "href": "TSA-Lecture02.html#properties-of-strict-stationarity",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "",
    "text": "Suppose \\((Y_t)\\) is a strictly stationary time series.\n\n\\(Y_t \\overset{D}{=} Y_0\\) for all \\(t\\), i.e., all \\(Y_t\\) are identically distributed. Proof: take \\(n=1\\) in the definition.\nIf the distribution (of \\(Y_t\\)) has finite 2nd moment (which implies finite first moment by \\((\\mathbb{E}[Y])^2 \\le \\mathbb{E}[Y^2] &lt; \\infty\\)), then for any \\(t\\),\n\n\\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[Y_0]\\) does not depend on \\(t\\).\n\\(\\sigma_t^2 = Var(Y_t) = Var(Y_0)\\) does not depend on \\(t\\).\n\nLet \\(n=2\\) in the definition, we have \\((Y_s,Y_t) \\overset{D}{=} (Y_{s-k},Y_{t-k}) \\overset{D}{=} (Y_0, Y_{t-s})\\). Consider the covariance, \\(Cov(Y_s,Y_t) = Cov(Y_0, Y_{t-s})\\), i.e., \\(\\gamma_{s,t} = \\gamma_{0, t-s}\\) which only depends on the lag \\(t-s\\). So for strictly stationary time series, we can simplify the notation by the following definition\n\n\\[\n\\gamma_{t-s} := \\gamma_{0, t-s} = \\gamma_{s,t}.\n\\] By the symmetry of ACVF, \\(\\gamma_{s-t} = \\gamma_{t,s} = \\gamma_{s,t} = \\gamma_{t-s}\\), so \\(\\gamma_k\\) is an even function of \\(k\\) in the sense that \\(\\gamma_{s-t} = \\gamma_{t-s} = \\gamma_{|t-s|}\\). Example: for a strictly stationary time series, \\(\\gamma_5 = \\gamma_{-5}= Cov(Y_0,Y_5)= Cov(Y_1,Y_6) = Cov(Y_t,Y_{t+5}), \\forall t\\).\nIn summary, for a strictly stationary time series with finite 2nd moment,\n\nMean Function: \\(\\mu_t=\\mu\\), \\(\\forall t\\in \\mathbb{Z}\\).\nVariance Function: \\(Var(Y_t) = \\gamma_{t,t}= \\gamma_0 = \\sigma^2\\), \\(\\forall t\\in \\mathbb{Z}\\).\nACVF: \\(\\gamma_k = Cov(Y_t,Y_{t-k}) = Cov(Y_{t-k},Y_t) = \\gamma_{-k}\\), \\(\\forall k,t\\in \\mathbb{Z}\\).\nACF: \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\\) (assuming \\(\\gamma_0 \\neq 0\\))."
  },
  {
    "objectID": "TSA-Lecture02.html#weak-stationarity",
    "href": "TSA-Lecture02.html#weak-stationarity",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "",
    "text": "Definition: A time series \\((Y_t)\\), \\(t\\in \\mathbb{Z}\\), satisfying the following three conditions,\n\n\\(\\mu_t=\\mu\\) for some (finite) constant \\(\\mu\\), \\(\\forall t\\in \\mathbb{Z}\\),\n\\(Var(Y_t) =\\sigma^2\\) for some (finite) constant \\(\\sigma^2\\), \\(\\forall t\\in \\mathbb{Z}\\),\n\\(Cov(Y_t,Y_{t-k}) = \\gamma_k\\) for some function \\(\\gamma_k\\) that only depends on the lag \\(k\\) and does not depend on the time \\(t\\),\n\nis called weakly stationary/ stationary / second order stationary / covariance stationary."
  },
  {
    "objectID": "TSA-Lecture02.html#example-1",
    "href": "TSA-Lecture02.html#example-1",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.1 Example 1",
    "text": "2.1 Example 1\nConsider the time series \\((e_t)\\) where \\(e_t \\sim IID(0,\\sigma_e^2)\\).\n\nShow the joint cdf of \\((e_{t_1},\\cdots,e_{t_n})\\) and \\((e_{t_1-k},\\cdots,e_{t_n-k})\\) are the same:\n\n\\[\n\\begin{split}\nF_{e_{t_1},\\cdots,e_{t_n}}(a_1,\\cdots,a_n) &= \\mathbb{P}(e_{t_1}\\le a_1, \\cdots, e_{t_n}\\le a_n) \\\\\n&= \\mathbb{P}(e_{t_1}\\le a_1) \\mathbb{P}(e_{t_2}\\le a_2) \\cdots \\mathbb{P}(e_{t_n}\\le a_n) \\\\\n&= \\mathbb{P}(e_{t_1-k}\\le a_1) \\mathbb{P}(e_{t_2-k}\\le a_2) \\cdots \\mathbb{P}(e_{t_n-k}\\le a_n) \\\\\n&= \\mathbb{P}(e_{t_1-k}\\le a_1, \\cdots, e_{t_n-k}\\le a_n) = F_{e_{t_1-k},\\cdots,e_{t_n-k}}(a_1,\\cdots,a_n)\n\\end{split}\n\\] By definition, \\((e_t)\\) is strictly stationary.\n\nIt is also weakly stationary, since\n\n\\(\\mu_t = 0\\), does not depend on \\(t\\),\n\\(Var(e_t) = \\sigma_e^2\\), does not depend on \\(t\\),\nThe ACVF only depends on the lag \\(k\\) as follows\n\n\\[\nCov(e_t, e_{t-k}) = \\begin{cases}\n0, & k\\neq 0\\\\\n\\sigma_e^2, & k=0.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "TSA-Lecture02.html#example-2",
    "href": "TSA-Lecture02.html#example-2",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.2 Example 2",
    "text": "2.2 Example 2\nLet \\(U_t \\overset{iid}{\\sim} N(0,1)\\). Define \\(X_t\\) as follows \\[\nX_t = \\begin{cases}\nU_t, & t \\text{ is even}\\\\\n\\frac{1}{\\sqrt{2}}(U_t^2 - 1), & t \\text{ is odd}.\n\\end{cases}\n\\] It is weakly stationary since\n\nFor even \\(t\\), \\(\\mu_t= \\mathbb{E}[U_t] = 0\\). For odd \\(t\\), \\(\\mu_t= \\mathbb{E}[\\frac{1}{\\sqrt{2}}(U_t^2 - 1)] = \\frac{1}{\\sqrt{2}}(\\mathbb{E}[U_t^2]-1) = 0\\). So \\(\\mu_t=0\\) for all \\(t\\).\nFor even \\(t\\), \\(Var(X_t) = Var(U_t)=1\\). For odd \\(t\\), \\(Var(X_t) = Var(\\frac{1}{\\sqrt{2}}(U_t^2 - 1)) = \\frac{1}{2} Var(U_t^2) = \\frac{1}{2} (\\mathbb{E}[U_t^4] - (\\mathbb{E}[U_t^2])^2 ) = \\frac{1}{2}(3-1) = 1\\). So \\(Var(X_t) = 1\\) for all \\(t\\).\n\n\n\\[\nCov(Y_t,Y_{t-k}) = \\begin{cases}\n0, \\quad k\\neq 0 \\\\\n1, \\quad k=0.\n\\end{cases}\n\\] Exercise: show that \\(Cov(Y_t,Y_{t-k}) = 0\\) for \\(k\\neq 0\\).\nFor this example \\((X_t)\\), we can show that it is not strictly stationary by proving \\(X_1\\) and \\(X_2\\) are not identically distributed.\nExercise: prove the claim above. (Hint: find some real number \\(a\\) such that \\(\\mathbb{P}(\\frac{1}{\\sqrt{2}}(U_1^2 - 1) \\le a) \\neq \\mathbb{P}(U_2 \\le a)\\).)"
  },
  {
    "objectID": "TSA-Lecture02.html#clarification-on-some-notations-and-concepts",
    "href": "TSA-Lecture02.html#clarification-on-some-notations-and-concepts",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.3 Clarification on some notations and concepts",
    "text": "2.3 Clarification on some notations and concepts\n\nIID noise: In the previous Example 1, \\(e_t \\sim IID(0,\\sigma_e^2)\\). In this course (TSA), we call a time series \\((e_t)\\) iid noise, denoted by \\(e_t \\sim IID(0,\\sigma_e^2)\\), if it satisfies: mean \\(0\\), variance \\(\\sigma_e^2\\), all \\(e_t\\) are iid (independently identically distributed).\nWhite noise: In this course, we call a time series \\((e_t)\\) white noise, denoted by \\(e_t \\sim WN(0,\\sigma_e^2)\\), if it satisfies: mean \\(0\\), variance \\(\\sigma_e^2\\), all \\(e_t\\) are uncorrelated (pairwise uncorrelated).\nIn general, iid noise implies white noise, and the inverse is not true. Under the assumption of normality, they are equivalent. (Note: here normality refers to the assumption that the time series \\((e_t)\\) is a Gaussian process.)\n\n\\[\n\\begin{split}\n\\text{iid noise} \\quad & \\Rightarrow \\quad \\text{white noise}, \\\\\n\\text{iid noise} \\quad & \\not\\Leftarrow \\quad \\text{white noise}, \\\\\n\\text{iid Normal(Gaussian) noise} \\quad & \\Leftrightarrow \\quad \\text{Normal(Gaussian) white noise}.\n\\end{split}\n\\]\n\nWarning: in Cryer and Chan, they often use iid noise and white noise interchangeably."
  },
  {
    "objectID": "TSA-Lecture02.html#more-examples-details-omitted",
    "href": "TSA-Lecture02.html#more-examples-details-omitted",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.4 More examples (details omitted)",
    "text": "2.4 More examples (details omitted)\n\n“Linear regression” example from lecture 1, where \\(Y_t = a+bt+e_t\\), and \\(e_t \\sim WN(0,\\sigma_e^2)\\). This is not stationary since \\(\\mu_t = a+bt\\) (whenever \\(b\\neq 0\\)).\n“Random walk” example from lecture 1. For positive integer \\(t\\), \\(Y_t = \\sum_{i}^t e_i\\), where \\(e_t \\sim WN(0,\\sigma_e^2)\\). We have \\(\\mu_t=0\\), \\(Var(Y_t) = t \\sigma_e^2\\). This \\((Y_t)\\) is not stationary.\n“Moving average” example from lecture 1, where \\(Y_t = \\frac{e_t+ e_{t-1}}{2}\\), and \\(e_t \\sim WN(0,\\sigma_e^2)\\). In lecture 1, we already calculated its mean function, variance function and ACVF. This \\((Y_t)\\) is stationary."
  },
  {
    "objectID": "TSA-Lecture02.html#example-3",
    "href": "TSA-Lecture02.html#example-3",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.5 Example 3",
    "text": "2.5 Example 3\nLet \\(A,B\\) be two iid random variables, with \\(\\mathbb{E}[A]=\\mathbb{E}[B]=0\\) and \\(Var(A)= Var(B) = \\sigma^2\\). Let \\(w\\in \\mathbb{R}\\) be a fixed real number. For each \\(t\\in \\mathbb{Z}\\), define \\(Y_t = A \\cos(wt) + B \\sin(wt)\\). Remark: note that \\(A,B\\) are random, but they are “same” for all \\(t\\).\n\n\\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[A \\cos(wt) + B \\sin(wt)] = \\cos(wt)\\mathbb{E}[A] + \\sin(wt) \\mathbb{E}[B] =0\\).\n\\(Var(Y_t)= Var(A \\cos(wt) + B \\sin(wt)) = \\cos^2(wt)Var(A) + \\sin^2(wt)Var(B) + 0 = \\sigma^2\\).\nThe ACVF only depends on the lag \\(k\\) as follows\n\n\\[\n\\begin{split}\nCov(Y_t,Y_{t-k}) &= Cov(A \\cos(wt) + B \\sin(wt), A \\cos(w(t-k)) + B \\sin(w(t-k))) \\\\\n&= \\cos(wt)\\cos(w(t-k)) Var(A) + \\sin(wt)\\sin(w(t-k)) Var(B) + 0 + 0 \\\\\n&= \\sigma^2 (\\cos(wt)\\cos(w(t-k)) + \\sin(wt)\\sin(w(t-k))) \\\\\n&= \\sigma^2 \\cos(wt - w(t-k)) = \\sigma^2 \\cos(wk)\n\\end{split}\n\\] So \\((Y_t)\\) is weakly stationary. In general (for generic choice of \\(w\\), generic distribution of \\(A,B\\), etc.), \\((Y_t)\\) is not strictly stationary.\nExercise: show that \\(Y_0\\) and \\(Y_1\\) are not identically distributed in general."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis: Lecture Notes",
    "section": "",
    "text": "These are the lecture notes for Time Series Analysis, based on the Cryer and Chen textbook “Time Series Analysis: With Applications in R” and Time Series Lecture notes of Dr Torcaso.\nThe notes are typed by Yue Wu. The notes are authored and maintained by Sergey Kushnarev, and include:\n\nTopic summaries\nMathematical derivations\nR code examples\nFigures and simulation results\n\n\nThese notes are meant to supplement, not replace, in-class lectures and discussions. There are typos and errors in the notes. Please refer to the textbook and lecture materials for the most accurate information. Please email me (or consult TAs/me in the Office Hours) if you find any issues or have questions."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Time Series Analysis: Lecture Notes",
    "section": "",
    "text": "These are the lecture notes for Time Series Analysis, based on the Cryer and Chen textbook “Time Series Analysis: With Applications in R” and Time Series Lecture notes of Dr Torcaso.\nThe notes are typed by Yue Wu. The notes are authored and maintained by Sergey Kushnarev, and include:\n\nTopic summaries\nMathematical derivations\nR code examples\nFigures and simulation results\n\n\nThese notes are meant to supplement, not replace, in-class lectures and discussions. There are typos and errors in the notes. Please refer to the textbook and lecture materials for the most accurate information. Please email me (or consult TAs/me in the Office Hours) if you find any issues or have questions."
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Time Series Analysis: Lecture Notes",
    "section": "📚 Lecture Notes",
    "text": "📚 Lecture Notes\n\nLecture 1: Introduction to Time Series Analysis\nLecture 2: Stationarity\nLecture 3: Q-dependent CLT. MA(q), General Linear Process. Causality\nLecture 4: AR(1), Yule-Walker method, AR-polynomial\nLecture 5: AR(p): causality, YW method, GLP representation,\nLecture 6: Recursive equations, ACVF for AR(2), AR(p), Invertibility\nLecture 7: ARMA(1,1), ARMA(p,q): GLP representation, YW method, ACVF. Visualizing Time Series\nLecture 8: Trends, Estimation of the mean Yt, Rnotebook: regression approach, residual analysis\n\nRnotebook: Regression Methods in Time Series\n\nLecture 9:"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Time Series Analysis: Lecture Notes",
    "section": "🔗 Resources",
    "text": "🔗 Resources\n\n[Course Syllabus (PDF)] TBD\nRStudio Cloud Project"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this Course",
    "section": "",
    "text": "Dr Sergey Kushnarev\nSenior Lecturer, AMS JHU"
  },
  {
    "objectID": "about.html#instructor",
    "href": "about.html#instructor",
    "title": "About this Course",
    "section": "",
    "text": "Dr Sergey Kushnarev\nSenior Lecturer, AMS JHU"
  },
  {
    "objectID": "about.html#course-info",
    "href": "about.html#course-info",
    "title": "About this Course",
    "section": "Course Info",
    "text": "Course Info\nCourse Title: Time Series Analysis\nCourse Code: EN.553.439/639"
  },
  {
    "objectID": "about.html#what-this-site-contains",
    "href": "about.html#what-this-site-contains",
    "title": "About this Course",
    "section": "What This Site Contains",
    "text": "What This Site Contains\nThese are live lecture notes created with Quarto. They include:\n\nSummaries of lecture topics\nMath and derivations\nCode examples in R and/or Python\nFigures and visualizations\n\nThey are meant to reinforce material covered in class and serve as a review resource. Some material may be added after class depending on questions and discussion."
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "About this Course",
    "section": "License",
    "text": "License\nContent is © 2025 Sergey Kushnarev unless otherwise noted. You may share or adapt the materials for non-commercial purposes with attribution."
  },
  {
    "objectID": "TSA-Lecture01.html",
    "href": "TSA-Lecture01.html",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "",
    "text": "We use lower case letters to denote Observed T.S.(Time Series). Example: \\(y_0,y_1,y_2,\\dots,\\) or \\(\\dots,y_{-2},y_{-1},y_0,y_1,y_2,\\dots.\\)\nWe use capital letters to denote Probabilistic Model for T.S., which is a series/sequence of RVs. Example: \\(Y_0,Y_1,Y_2,\\dots,\\) or \\(\\dots,Y_{-2},Y_{-1},Y_0,Y_1,Y_2,\\dots.\\) We may also use the following shorthand notations: \\((Y_i), (Y_t), (Y_t)_{t=0}^{+\\infty}, (Y_t)_{t=1}^{+\\infty}, (Y_t)_{t=-\\infty}^{+\\infty},\\) etc.\n\n\n\n\nDefinition: A time series model for the observed data \\((y_t)\\) is a specification of the joint distribution, or possibly just the means and covariances, of a sequence of random variables \\(\\{ Y(\\omega,t), t\\in T\\}\\) of which \\((y_t)\\) is a realization.\nWe will frequently use the term time series to denote both the model and the observed data.\nIn this definition, the stochastic process can be seen as a set/sequence of random variables \\(Y(\\omega,t)\\) indexed by time \\(t\\). The \\(T\\) is an index set for time \\(t\\). The randomness comes from \\(\\omega\\), which takes values from the sample space \\(\\Omega\\). For this course TSA, in most cases, we have the following setting:\n\nThe time index set \\(T\\) is often \\(\\mathbb{Z}\\) (the set of integers) or \\(\\mathbb{Z}_{\\ge 0}\\) (nonnegative integers).\nThe random variables here are continuous, which means for any fixed \\(t\\), the random variable \\(Y(\\omega,t)\\) follows a continuous distribution."
  },
  {
    "objectID": "TSA-Lecture01.html#observed-t.s.-vs.-probabilistic-model-for-t.s.",
    "href": "TSA-Lecture01.html#observed-t.s.-vs.-probabilistic-model-for-t.s.",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "",
    "text": "We use lower case letters to denote Observed T.S.(Time Series). Example: \\(y_0,y_1,y_2,\\dots,\\) or \\(\\dots,y_{-2},y_{-1},y_0,y_1,y_2,\\dots.\\)\nWe use capital letters to denote Probabilistic Model for T.S., which is a series/sequence of RVs. Example: \\(Y_0,Y_1,Y_2,\\dots,\\) or \\(\\dots,Y_{-2},Y_{-1},Y_0,Y_1,Y_2,\\dots.\\) We may also use the following shorthand notations: \\((Y_i), (Y_t), (Y_t)_{t=0}^{+\\infty}, (Y_t)_{t=1}^{+\\infty}, (Y_t)_{t=-\\infty}^{+\\infty},\\) etc."
  },
  {
    "objectID": "TSA-Lecture01.html#formal-definition-of-time-series",
    "href": "TSA-Lecture01.html#formal-definition-of-time-series",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "",
    "text": "Definition: A time series model for the observed data \\((y_t)\\) is a specification of the joint distribution, or possibly just the means and covariances, of a sequence of random variables \\(\\{ Y(\\omega,t), t\\in T\\}\\) of which \\((y_t)\\) is a realization.\nWe will frequently use the term time series to denote both the model and the observed data.\nIn this definition, the stochastic process can be seen as a set/sequence of random variables \\(Y(\\omega,t)\\) indexed by time \\(t\\). The \\(T\\) is an index set for time \\(t\\). The randomness comes from \\(\\omega\\), which takes values from the sample space \\(\\Omega\\). For this course TSA, in most cases, we have the following setting:\n\nThe time index set \\(T\\) is often \\(\\mathbb{Z}\\) (the set of integers) or \\(\\mathbb{Z}_{\\ge 0}\\) (nonnegative integers).\nThe random variables here are continuous, which means for any fixed \\(t\\), the random variable \\(Y(\\omega,t)\\) follows a continuous distribution."
  },
  {
    "objectID": "TSA-Lecture01.html#definitions",
    "href": "TSA-Lecture01.html#definitions",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "2.1 Definitions",
    "text": "2.1 Definitions\n\nMean Function: for any \\(t\\), define \\[\\mu_t := \\mathbb{E}[Y_t].\\]\nAutocovariance Function(ACVF): for any two time indices \\(t,s\\), define the corresponding ACVF as the covariance of \\(Y_t\\) and \\(Y_s\\), i.e., \\[\\gamma_{t,s} := Cov(Y_t, Y_s) = \\mathbb{E}[(Y_t-\\mu_t)(Y_s-\\mu_s)] = \\mathbb{E}[Y_t Y_s] -\\mu_t\\mu_s.\\]\nVariance Function is the special case of ACVF with \\(t=s\\), which is equal to the variance of \\(Y_t\\), \\[\\gamma_{t,t} = Cov(Y_t, Y_t) = Var(Y_t).\\]\nAutocorrelation Function(ACF): for any two time indices \\(t,s\\), define the corresponding ACF as the correlation of \\(Y_t\\) and \\(Y_s\\), i.e., \\[\\rho_{t,s} := \\frac{Cov(Y_t, Y_s)}{\\sqrt{Var(Y_t) \\cdot Var(Y_s)}} = \\frac{\\gamma_{t,s}}{\\sqrt{\\gamma_{t,t} \\cdot \\gamma_{s,s}}}.\\]"
  },
  {
    "objectID": "TSA-Lecture01.html#properties",
    "href": "TSA-Lecture01.html#properties",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "2.2 Properties",
    "text": "2.2 Properties\n\nWhen \\(t=s\\): \\[\\gamma_{t,t} = Var(Y_t), \\quad \\rho_{t,t}=1.\\]\nSymmetry: \\[\\gamma_{t,s} = \\gamma_{s,t}, \\quad \\rho_{t,s} = \\rho_{s,t}.\\]\nRecall that the Cauchy–Schwarz inequality \\(Var(X) \\cdot Var(Y) \\ge |Cov(X,Y)|^2\\) holds for any two random variables \\(X,Y\\). This immediately gives \\[|\\gamma_{t,s}| \\le \\sqrt{\\gamma_{t,t} \\cdot \\gamma_{s,s} }, \\quad |\\rho_{t,s}| \\le 1.\\] Some further remarks:\n\nIf \\(\\rho_{t,s} \\approx \\pm 1\\), then \\(Y_t\\) and \\(Y_s\\) are strongly linearly related.\nIf \\(\\rho_{t,s} \\approx 0\\), then \\(Y_t\\) and \\(Y_s\\) are weakly linearly related.\nIf \\(\\rho_{t,s} = 0\\), then \\(Y_t\\) and \\(Y_s\\) are uncorrelated.\n\nIf \\(Y_t\\) and \\(Y_s\\) are independent, then \\(Y_t\\) and \\(Y_s\\) are uncorrelated (assuming both have nonzero and finite second moments). This is because independence directly implies \\(\\gamma_{t,s}=0\\), which further implies \\(\\rho_{t,s} = 0\\).\nBilinearity: For any positive integers \\(n,m\\), any real numbers \\(a_1,\\dots,a_n, b_1,\\dots,b_m \\in \\mathbb{R}\\), and any time points/indices \\(t_1,\\dots,t_n,s_1,\\dots,s_m\\), the following holds \\[Cov\\left( \\sum_{i=1}^n a_i Y_{t_i}, \\sum_{j=1}^m b_j Y_{s_j} \\right) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j Cov(Y_{t_i}, Y_{s_j}).\\] Examples:\n\n\\(Cov(a_1 Y_{t_1} + a_2 Y_{t_2}, Y_s) = a_1 Cov(Y_{t_1}, Y_s) + a_2 Cov(Y_{t_2}, Y_s).\\)\n\\(Cov(a_1 Y_{t_1} + a_2 Y_{t_2}, b_1 Y_{s_1} + b_2 Y_{s_2}) = a_1 b_1 Cov(Y_{t_1}, Y_{s_1}) + a_2 b_1 Cov(Y_{t_2}, Y_{s_1}) + a_1 b_2 Cov(Y_{t_1}, Y_{s_2}) + a_2 b_2 Cov(Y_{t_2}, Y_{s_2}).\\)"
  },
  {
    "objectID": "TSA-Lecture01.html#example-1-linear-regression",
    "href": "TSA-Lecture01.html#example-1-linear-regression",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "3.1 Example 1: Linear Regression",
    "text": "3.1 Example 1: Linear Regression\nConsider the time series defined by \\[Y_t= a+ bt + e_t, \\quad e_t \\overset{iid}{\\sim} N(0,\\sigma_e^2).\\] Remark: In time series models, such \\(e_t\\) terms are called innovation terms, error terms, or noise terms. For this example,\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[a+ bt + e_t] = a+ bt.\\)\nACVF:\n\n\\[\\gamma_{t,s} = Cov(a+ bt + e_t, a+ bs + e_s) = Cov(e_t, e_s) = \\begin{cases}\n0, &\\text{if } t\\neq s\\\\\n\\sigma_e^2, &\\text{if } t=s.\n\\end{cases}\\]\n\nACF:\n\n\\[\\rho_{t,s} = \\begin{cases}\n0, &\\text{if } t\\neq s\\\\\n1, &\\text{if } t=s.\n\\end{cases}\\]"
  },
  {
    "objectID": "TSA-Lecture01.html#example-2-random-walk",
    "href": "TSA-Lecture01.html#example-2-random-walk",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "3.2 Example 2: Random Walk",
    "text": "3.2 Example 2: Random Walk\nLet \\(e_1,e_2,e_3,\\dots \\sim IID(0,\\sigma_e^2)\\), which means they are iid random variables with mean \\(0\\) and variance \\(\\sigma_e^2\\). Define Random Walk as follows: \\[\n\\begin{split}\n&Y_0 = 0 \\\\\n&Y_1 = e_1 \\\\\n&Y_2 = e_1+e_2 \\\\\n&Y_3 = e_1+e_2+e_3 \\\\\n&\\cdots\n\\end{split}\n\\] Remark: Alternatively, it can be defined by \\(Y_0=0\\) and \\(Y_{t+1}= Y_t+ e_{t+1}\\). For this example,\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[e_1+\\cdots+e_t] = 0.\\)\nVariance function: Using the dependence between the \\(e_t\\) terms, we have\n\n\\[\\gamma_{t,t} = Var(Y_t) = Var(e_1+\\cdots+e_t) = \\sum_{i=1}^t Var(e_i) + 2\\sum_{1\\le i&lt;j\\le t} Cov(e_i,e_j) = t \\sigma_e^2.\\] So this variance function grows linearly with time \\(t\\).\n\nACVF: Suppose \\(1\\le s\\le t\\), then we have\n\n\\[\\begin{split}\n\\gamma_{s,t} &= Cov(Y_s,Y_t) = Cov(Y_s, Y_s+(Y_t-Y_s)) \\\\\n&= Var(Y_s) + Cov(Y_s, e_{s+1}+ e_{s+2}+ \\cdots +e_t) = s \\sigma_e^2.\n\\end{split}\\] where the last step is because \\(Y_s\\) is independent of the terms \\(e_{s+1},\\dots,e_t\\). If \\(1\\le t\\le s\\), we can derive \\(\\gamma_{s,t} = t \\sigma_e^2\\) in the same way. So we conclude that \\(\\gamma_{s,t} = \\min\\{s,t\\} \\cdot \\sigma_e^2\\) for any \\(s,t\\).\n\nACF: Using ACVF, \\(\\rho_{s,t}= \\frac{\\gamma_{s,t}}{ \\sqrt{\\gamma_{s,s}\\cdot \\gamma_{t,t}}} = \\frac{\\min\\{s,t\\} \\cdot \\sigma_e^2}{\\sqrt{s \\sigma_e^2} \\sqrt{t \\sigma_e^2}} = \\frac{\\min\\{s,t\\}}{ \\sqrt{st}}\\). It can also be rewritten as \\(\\rho_{s,t} = \\min\\{ \\sqrt{\\frac{s}{t}}, \\sqrt{\\frac{t}{s}}\\}\\). Example: \\(\\rho_{1,2}= \\sqrt{\\frac{1}{2}}\\), \\(\\rho_{2,3}= \\sqrt{\\frac{2}{3}}\\). Some further observations: \\(\\rho_{t,t+1}= \\sqrt{\\frac{t}{t+1}} \\to 1\\) as \\(t\\to\\infty\\); \\(\\rho_{1,t}= \\sqrt{\\frac{1}{t}} \\to 0\\) as \\(t\\to\\infty\\).\n\n\n\n\n\n\nTwo Sample Paths of a Random Walk"
  },
  {
    "objectID": "TSA-Lecture01.html#example-3-moving-average",
    "href": "TSA-Lecture01.html#example-3-moving-average",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "3.3 Example 3: “Moving Average”",
    "text": "3.3 Example 3: “Moving Average”\nLet \\((e_t) \\sim IID(0,\\sigma_e^2)\\), and define \\(Y_t = \\frac{1}{2} (e_t + e_{t-1})\\). For this example,\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = \\frac{1}{2} \\mathbb{E}[e_t + e_{t-1}] = 0.\\) Note that this mean function does not depend on time \\(t\\).\nVariance function: \\(\\gamma_{t,t} = Var(Y_t) = Var(\\frac{1}{2} (e_t + e_{t-1})) = \\frac{1}{4} (Var(e_t) + Var(e_{t-1}) + 2 Cov(e_t, e_{t-1})) = \\frac{1}{2} \\sigma_e^2\\). Note that this variance function does not depend on time \\(t\\).\nACVF: The case \\(\\gamma_{t,t}\\) reduces to the variance function. Next, we compute \\(\\gamma_{t,t-1}\\).\n\n\\[\\begin{split}\n\\gamma_{t,t-1} &= Cov(Y_t, Y_{t-1}) = Cov(\\frac{1}{2} (e_t + e_{t-1}), \\frac{1}{2} (e_{t-1} + e_{t-2})) \\\\\n&= \\frac{1}{4} (Cov(e_{t}, e_{t-1}) + Cov(e_{t}, e_{t-2}) + Cov(e_{t-1}, e_{t-1}) + Cov(e_{t-1}, e_{t-2})) = \\frac{1}{4} \\sigma_e^2.\n\\end{split}\\]\nExercise: Show that \\(\\gamma_{t,t-k}=0\\) for \\(k\\ge 2\\).\nCombining all the cases above, we get\n\\[\\gamma_{t,s} = \\begin{cases}\n  \\frac{1}{2} \\sigma_e^2, &\\text{if } t=s \\\\\n  \\frac{1}{4} \\sigma_e^2, &\\text{if } |t-s|=1 \\\\\n  0, &\\text{if } |t-s|\\ge 2.\n  \\end{cases}\\]\n\nACF: Using ACVF, we get\n\\[\\rho_{t,s} = \\begin{cases}\n1 , &\\text{if } t=s \\\\\n\\frac{1}{2} , &\\text{if } |t-s|=1 \\\\\n0, &\\text{if } |t-s|\\ge 2.\n\\end{cases}\\] Note that in this example, both ACVF and ACF only depend on the lag \\(t-s\\)."
  },
  {
    "objectID": "TSA-Lecture01.html#strict-stationarity",
    "href": "TSA-Lecture01.html#strict-stationarity",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "4.1 Strict stationarity",
    "text": "4.1 Strict stationarity\nDefinition: A stochastic process is strictly stationary if all finite dimensional joint distributions do not change if their indices are shifted by the same amount. (In other words, finite dimensional joint distributions are time invariant.)\nThis definition is for general stochastic processes. For a time series (suppose the time index set is \\(\\mathbb{Z}\\)), the definition can be stated as follows.\nDefinition: A time series \\((\\dots,Y_{-1},Y_0,Y_1,\\dots)\\) is strictly stationary if for any positive integer \\(n\\), any distinct integers \\(t_1,\\dots,t_n \\in \\mathbb{Z}\\), and any integer \\(k\\in \\mathbb{Z}\\), the following holds \\[F_{Y_{t_1}, \\dots, Y_{t_n}} = F_{Y_{t_1 -k}, \\dots, Y_{t_n -k}},\\] i.e., the joint cdf of \\((Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n})\\) is same as the joint cdf of \\((Y_{t_1 -k}, Y_{t_2 -k}, \\dots, Y_{t_n -k})\\).\nRemark: we may also use the notation \\(\\overset{D}{=}\\) or \\(\\overset{D}{\\equiv}\\) to denote two random variables/vectors have same (joint) distributions.\nBy this definition, a strictly stationary time series must satisfy\n\n\\(Y_1 \\overset{D}{=} Y_2 \\overset{D}{=} Y_3 \\overset{D}{=} Y_t\\) for any \\(t\\);\n\\((Y_1,Y_3) \\overset{D}{=} (Y_2,Y_4) \\overset{D}{=} (Y_{10},Y_{12}) \\overset{D}{=} (Y_t,Y_{t+2})\\) for any \\(t\\);\n\\((Y_1,Y_3,Y_7) \\overset{D}{=} (Y_{10},Y_{12},Y_{16}) \\overset{D}{=} (Y_{t},Y_{t+2},Y_{t+6})\\) for any \\(t\\);\n\\(\\dots\\)\n\n(Note: the listed properties here are for illustration only. They are just some necessary conditions for a strictly stationary time series.)\nExercise: Show that for a strictly stationary time series, its ACVF \\(\\gamma_{t,s}\\) only depends on the lag \\(t-s\\). Moreover, by symmetry of ACVF, we can immediately show \\(\\gamma_{t,s}\\) only depends on \\(|t-s|\\).\nConsequently, for a strictly stationary time series, we can replace the notation \\(\\gamma_{t,s}\\) by \\(\\gamma_{|t-s|}\\) to simplify the notation, since this transformation is well defined by the previous exercise."
  },
  {
    "objectID": "TSA-Lecture03.html",
    "href": "TSA-Lecture03.html",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "",
    "text": "Last time we defined \\(q\\)-dependent and \\(q\\)-correlated time series.\n\n\n\\(q\\)-dependent CLT: Let \\((Y_t)\\) be a \\(q\\)-dependent (\\(q \\geq 0\\)) stationary time series with \\(\\mu = \\mathbb{E}[Y_t]\\) and \\(\\sigma^2 = Var(Y_t)\\). Then as long as \\[\n\\sigma^2 + 2\\operatorname{Cov}(Y_1, Y_2) + \\cdots + 2\\operatorname{Cov}(Y_1, Y_{q+1}) &gt; 0,\n\\]\nthen \\[\n\\frac{\\sum_{i=1}^n Y_i - n\\mu}{\\sqrt{\\mathrm{Var}\\left(\\sum_{i=1}^n Y_i\\right)}} \\xrightarrow{D} N(0, 1), \\quad \\text{as } n\\to \\infty,\n\\] or equivalently, \\[\n\\frac{\\overline{Y} - \\mu}{\\sqrt{\\mathrm{Var}(\\overline{Y})}} \\xrightarrow{D} N(0, 1), \\quad \\text{as } n \\to \\infty.\n\\]\nRemark: the last statement can be loosely thought of as \\(\\overline{Y} \\approx N(\\mu, \\mathrm{Var}(\\overline{Y}))\\).\n\n\n\nSuppose \\(n &gt; q\\). Since \\((Y_t)\\) is stationary, using ACVF we have \\[\n\\begin{split}\n\\operatorname{Var} \\left( \\sum_{i=1}^n Y_i \\right) &=\n    \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(Y_i, Y_j) = \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_{|i-j|} \\\\\n&=n\\gamma_0 + 2(n-1)\\gamma_1 + 2(n-2)\\gamma_2 + \\cdots + 2\\gamma_{n-1} \\\\\n&= n\\gamma_0 + 2\\sum_{j=1}^q (n-j)\\gamma_j\n\\end{split}\n\\] where the last step is because \\(\\gamma_j = 0\\) for \\(j &gt; q\\) (by \\(q\\)-dependence). So \\[\n\\mathrm{Var}(\\overline{Y}) = \\frac{1}{n^2} \\mathrm{Var} \\left( \\sum_{i=1}^n Y_i \\right)\n= \\frac{1}{n} \\gamma_0 + \\frac{2}{n^2} \\sum_{j=1}^{q} (n-j) \\gamma_j .\n\\] As \\(n \\to \\infty\\), this variance \\(\\mathrm{Var}(\\overline{Y}) \\approx \\frac{1}{n}(\\sigma^2 + 2\\operatorname{Cov}(Y_1, Y_2) + \\cdots + 2\\operatorname{Cov}(Y_1, Y_{q+1}))\\), which goes to \\(0\\) at the rate \\(O(\\frac{1}{n})\\). This behavior looks similar to the standard CLT (with iid setting). Then, (following the idea of standard CLT) \\[\n\\frac{\\overline{Y} - \\mu}{\\sqrt{\\mathrm{Var}(\\overline{Y})}} \\xrightarrow{D} N(0,1), \\quad \\text{as } n \\to \\infty.\n\\]\nRemark: the CLT can be even generalized to some time series that are not \\(q\\)-dependent, as long as \\(\\gamma_k\\) decays to zero (as \\(k\\to \\infty\\)) sufficiently fast."
  },
  {
    "objectID": "TSA-Lecture03.html#statement-of-the-theorem",
    "href": "TSA-Lecture03.html#statement-of-the-theorem",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "",
    "text": "\\(q\\)-dependent CLT: Let \\((Y_t)\\) be a \\(q\\)-dependent (\\(q \\geq 0\\)) stationary time series with \\(\\mu = \\mathbb{E}[Y_t]\\) and \\(\\sigma^2 = Var(Y_t)\\). Then as long as \\[\n\\sigma^2 + 2\\operatorname{Cov}(Y_1, Y_2) + \\cdots + 2\\operatorname{Cov}(Y_1, Y_{q+1}) &gt; 0,\n\\]\nthen \\[\n\\frac{\\sum_{i=1}^n Y_i - n\\mu}{\\sqrt{\\mathrm{Var}\\left(\\sum_{i=1}^n Y_i\\right)}} \\xrightarrow{D} N(0, 1), \\quad \\text{as } n\\to \\infty,\n\\] or equivalently, \\[\n\\frac{\\overline{Y} - \\mu}{\\sqrt{\\mathrm{Var}(\\overline{Y})}} \\xrightarrow{D} N(0, 1), \\quad \\text{as } n \\to \\infty.\n\\]\nRemark: the last statement can be loosely thought of as \\(\\overline{Y} \\approx N(\\mu, \\mathrm{Var}(\\overline{Y}))\\)."
  },
  {
    "objectID": "TSA-Lecture03.html#sketch-of-the-proof",
    "href": "TSA-Lecture03.html#sketch-of-the-proof",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "",
    "text": "Suppose \\(n &gt; q\\). Since \\((Y_t)\\) is stationary, using ACVF we have \\[\n\\begin{split}\n\\operatorname{Var} \\left( \\sum_{i=1}^n Y_i \\right) &=\n    \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(Y_i, Y_j) = \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_{|i-j|} \\\\\n&=n\\gamma_0 + 2(n-1)\\gamma_1 + 2(n-2)\\gamma_2 + \\cdots + 2\\gamma_{n-1} \\\\\n&= n\\gamma_0 + 2\\sum_{j=1}^q (n-j)\\gamma_j\n\\end{split}\n\\] where the last step is because \\(\\gamma_j = 0\\) for \\(j &gt; q\\) (by \\(q\\)-dependence). So \\[\n\\mathrm{Var}(\\overline{Y}) = \\frac{1}{n^2} \\mathrm{Var} \\left( \\sum_{i=1}^n Y_i \\right)\n= \\frac{1}{n} \\gamma_0 + \\frac{2}{n^2} \\sum_{j=1}^{q} (n-j) \\gamma_j .\n\\] As \\(n \\to \\infty\\), this variance \\(\\mathrm{Var}(\\overline{Y}) \\approx \\frac{1}{n}(\\sigma^2 + 2\\operatorname{Cov}(Y_1, Y_2) + \\cdots + 2\\operatorname{Cov}(Y_1, Y_{q+1}))\\), which goes to \\(0\\) at the rate \\(O(\\frac{1}{n})\\). This behavior looks similar to the standard CLT (with iid setting). Then, (following the idea of standard CLT) \\[\n\\frac{\\overline{Y} - \\mu}{\\sqrt{\\mathrm{Var}(\\overline{Y})}} \\xrightarrow{D} N(0,1), \\quad \\text{as } n \\to \\infty.\n\\]\nRemark: the CLT can be even generalized to some time series that are not \\(q\\)-dependent, as long as \\(\\gamma_k\\) decays to zero (as \\(k\\to \\infty\\)) sufficiently fast."
  },
  {
    "objectID": "TSA-Lecture03.html#backshift-operator",
    "href": "TSA-Lecture03.html#backshift-operator",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "3.1 Backshift operator",
    "text": "3.1 Backshift operator\nDefinition: For a sequence \\((y_t)\\), the backshift operator \\(B\\) is defined by \\(B Y_t = Y_{t-1}\\).\nExample: \\(B^2 Y_t = B(Y_{t-1}) = Y_{t-2}\\), \\(B^k Y_t = Y_{t-k}\\) for \\(k \\ge 0\\).\nThe inverse of \\(B\\) is considered as forwardshift: \\(B^{-1} Y_t = Y_{t+1}\\), \\(B^{-2} Y_t = Y_{t+2}\\), etc.\nDefinition: A linear filter is an operator defined as \\[\n\\Psi(B) = \\sum_{j=-\\infty}^{+\\infty} \\psi_j B^j.\n\\] By this definition, the GLP process \\(Y_t = \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\) can be written as \\(Y_t = \\Psi(B) e_t\\), since \\[\n\\left(\\sum_{j=-\\infty}^{+\\infty} \\psi_j B^j\\right) e_t\n= \\sum_{j=-\\infty}^{+\\infty} \\psi_j B^j e_t\n= \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j} = Y_t .\n\\]"
  },
  {
    "objectID": "TSA-Lecture03.html#glp-is-stationary",
    "href": "TSA-Lecture03.html#glp-is-stationary",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "3.2 GLP is stationary",
    "text": "3.2 GLP is stationary\nIn this part, we will derive the mean function, variance function, ACVF, ACF of a GLP. Then we can see that a GLP is stationary.\n\nMean function \\[\n\\mu_t = \\mathbb{E} [Y_t] = \\mathbb{E}\\left[\\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\right]\n= \\sum_{j=-\\infty}^{+\\infty} \\psi_j \\mathbb{E}[e_{t-j}] = 0.\n\\]\nVariance function \\[\n\\begin{split}\n\\operatorname{Var}(Y_t) &= \\operatorname{Var}\\left(\\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\right)\n= \\sum_{j=-\\infty}^{+\\infty} \\psi_j^2 \\operatorname{Var}(e_{t-j}) + \\ 2 \\sum_{i&lt;j} \\psi_i \\psi_j \\operatorname{Cov}(e_{t-i}, e_{t-j}) = \\sigma_e^2 \\left( \\sum_{j=-\\infty}^{+\\infty} \\psi_j^2 \\right)\n\\end{split}\n\\] which is a finite constant (see the following exercise).\n\nExercise: Show that \\(\\sum_{j=-\\infty}^{+\\infty} |\\psi_j| &lt; \\infty\\) implies \\(\\sum_{j=-\\infty}^{+\\infty} \\psi_j^2 &lt; \\infty\\) (i.e., absolute summability implies square summability/convergence).\n\nACVF \\[\n\\begin{split}\n\\operatorname{Cov}(Y_t, Y_{t+k}) &= \\mathbb{E}\\left[Y_t Y_{t+k}\\right] - \\left(\\mathbb{E} Y_t\\right)\\left(\\mathbb{E} Y_{t+k}\\right) = \\mathbb{E}\\left[Y_t Y_{t+k}\\right]\n= \\mathbb{E}\\left[\n  \\left( \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j} \\right)\n  \\left( \\sum_{i=-\\infty}^{+\\infty} \\psi_i e_{t+k-i} \\right)\n\\right] \\\\\n& = \\sum_{i=-\\infty}^{+\\infty} \\sum_{j=-\\infty}^{+\\infty} \\psi_i \\psi_j \\mathbb{E}[e_{t-j} e_{t+k-i}] .\n\\end{split}\n\\] Note that \\(\\mathbb{E}[e_{t-j} e_{t+k-i}] = \\mathbb{E}[e_{t-j}] \\cdot \\mathbb{E}[e_{t+k-i}] = 0\\) if \\(i \\neq j+k\\), and \\(\\mathbb{E}[e_{t-j} e_{t+k-i}] = \\sigma_e^2\\) if \\(i=j+k\\). So the ACVF depends only on the lag \\(k\\): \\[\n\\gamma_k = \\sum_{j=-\\infty}^{+\\infty} \\psi_{k+j}\\psi_j \\sigma_e^2 .\n\\]\nACF \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\n= \\frac{ \\displaystyle\\sum_{j=-\\infty}^{+\\infty} \\psi_{k+j} \\psi_j }\n      { \\displaystyle\\sum_{j=-\\infty}^{+\\infty} \\psi_j^2 } .\n\\]\n\nRemark: If we want a GLP (\\(Y_t\\)) with mean \\(\\mu\\), then just add \\(\\mu\\). Let \\(Y_t = \\mu + \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\). ACVF, ACF remain the same."
  },
  {
    "objectID": "TSA-Lecture05.html",
    "href": "TSA-Lecture05.html",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "Recall last time, from the AR(\\(p\\)) \\[\nY_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2} - \\cdots - \\phi_p Y_{t-p} = e_t, \\quad\ne_t \\sim \\mathrm{iid}(0, \\sigma_e^2),\n\\] we got the reformulated form \\(\\Phi(B) Y_t = e_t\\), where the AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p\\). If all the \\(p\\) complex roots of the AR polynomial satisfy \\(|z_i| &gt;1\\) for all \\(i=1,...,p\\), i.e., all the \\(p\\) roots are outside the unit disc in \\(\\mathbb{C}\\), then we showed that the AR(\\(p\\)) equation became \\[\ne_t = \\left(1-\\frac{B}{z_1}\\right) \\cdots \\left(1-\\frac{B}{z_p}\\right) Y_t .\n\\] Since each \\(|\\frac{1}{z_i}|&lt;1\\), we have \\(\\left(1-\\frac{B}{z_i}\\right) ^{-1} = \\sum_{j=0}^\\infty z_i^{-j} B^j\\). Then \\[\n\\begin{split}\nY_t &= \\left(1-\\frac{B}{z_1}\\right)^{-1} \\cdots \\left(1-\\frac{B}{z_p}\\right)^{-1} e_t \\\\\n&= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) e_t .\n\\end{split}\n\\] As we mentioned last time, this product can be written as a GLP. To see this, we first consider the simple case \\(p=2\\). Suppose \\(a_j = z_1^{-j}\\) and \\(b_j = z_2^{-j}\\). Then \\[\n\\begin{split}\n& \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\left(\\sum_{j=0}^\\infty z_2^{-j} B^j \\right) = \\left(\\sum_{j=0}^\\infty a_j B^j \\right) \\left(\\sum_{j=0}^\\infty b_j B^j \\right) \\\\\n&= \\left(a_0 + a_1 B^1 + a_2 B^2 + \\cdots \\right) \\left(b_0 + b_1 B^1 + b_2 B^2 + \\cdots \\right) \\\\\n&= a_0 b_0 + (a_0 b_1 + a_1 b_0) B^1 + (a_0 b_2 + a_1 b_1 + a_2 b_0) B^2 + (a_0 b_3 + a_1 b_2 + a_2 b_1 + a_3 b_0) B^3 + \\cdots \\\\\n&= \\sum_{n=0}^\\infty \\underbrace{\\left( \\sum_{j_1+j_2 = n} a_{j_1} b_{j_2} \\right)}_{c_n} B^n .\n\\end{split}\n\\] For general \\(p\\), we have the similar result \\[\n\\left(\\sum_{j=0}^\\infty a_{1,j} B^j \\right) \\left(\\sum_{j=0}^\\infty a_{2,j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty a_{p,j} B^j \\right) = \\sum_{n=0}^\\infty c_n B^n\n\\] where \\(c_n = \\sum_{j_1+j_2+\\cdots + j_p = n} a_{1,j_1} a_{2,j_2} \\cdots a_{p,j_p}\\). Note: this can be seen as a \\(p\\)-fold convolution. Using this result, \\[\n\\begin{split}\n& \\Phi(B)^{-1}\n= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) = \\sum_{n=0}^\\infty \\psi_n B^n ,\\\\\n& \\text{ where}\\quad \\psi_n = \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p} .\n\\end{split}\n\\] So the AR(\\(p\\)) can be written as \\[\n\\begin{split}\nY_t\n&= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) e_t = \\left(\\sum_{n=0}^\\infty \\psi_n B^n \\right) e_t = \\sum_{n=0}^\\infty \\psi_n e_{t-n}\n\\end{split}\n\\] which looks like a GLP, with the \\(\\{\\psi_n\\}\\) specified above. To ensure this is a GLP, we still need to verify \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\) (see the definition of GLP).\n\n\n\nThe sketch of the proof for \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\): \\[\n\\begin{split}\n\\sum_{n=0}^\\infty |\\psi_n| &= \\sum_{n=0}^\\infty \\left| \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p} \\right| \\le \\sum_{n=0}^\\infty \\sum_{j_1+j_2+\\cdots + j_p = n} |z_1|^{-j_i} |z_2|^{-j_2} \\cdots |z_p|^{-j_p} \\\\\n&\\le \\sum_{n=0}^\\infty \\sum_{j_1+j_2+\\cdots + j_p = n} |z^*|^{-j_1-j_2-\\cdots - j_p}\n\\le \\sum_{n=0}^\\infty c(p) n^p |z^*|^{-n} &lt; \\infty .\n\\end{split}\n\\] Notes on the missing details (without proof):\n\nLet \\(z^*\\) be the root among \\(\\{z_1,\\dots, z_p\\}\\) with the smallest modulus, so \\(1&lt;|z^*| \\le \\min\\{ |z_1|,\\dots, |z_p| \\}\\).\nThe number of terms in the summation \\(\\sum_{j_1+j_2+\\cdots + j_p = n}\\) can be upper bounded by \\(c(p) n^p\\) where \\(c(p)\\) is a constant that only depends on \\(p\\). (Since \\({n+p-1\\choose p-1} = \\frac{(n+p-1)(n+p-2) \\cdots (n+1)}{ (p-1)!}\\) is a polynomial of \\(n\\) with degree less than \\(p\\) and coefficients only depend on \\(p\\).)\nThe last step is because \\(\\sum_{n=0}^\\infty n^p |z^*|^{-n}\\) converges for \\(|z^*| &gt;1\\).\n\nSo we just showed \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\), which finishes the proof that \\(Y_t = \\Phi(B)^{-1} e_t\\) is a GLP (as long as all roots of \\(\\Phi(x)\\) are outside the unit disc in \\(\\mathbb{C}\\)).\n\n\n\nIn summary, for AR(\\(p\\)) process, we have the following results. (Look similar to the three cases for AR(\\(1\\)) from last lecture.)\n\nIf all roots of the AR polynomial are outside the unit disc (\\(|z_i| &gt; 1\\) for all \\(i\\)), then \\((Y_t)\\) is causal and stationary.\nIf at least one of the roots is inside the unit disc (\\(|z_i| &lt; 1\\) for one \\(z_i\\)), and none of the roots is on the unit circle (\\(|z_i| \\neq 1\\) for all \\(i\\)), then \\((Y_t)\\) is non-causal (future-dependent) and stationary.\nIf at least one root is a unit root (i.e. \\(|z_i| = 1\\) for one \\(z_i\\)), then \\((Y_t)\\) is not stationary.\n\n\n\n\nWe start with a concrete AR(\\(2\\)) example. Suppose the AR(\\(2\\)) equation is \\[\nY_t = Y_{t-1} + Y_{t-2} + e_t .\n\\] We can use the previous results to find whether this process is causal or stationary. The AR polynomial for this example is \\(\\Phi(x) = 1 - x - x^2\\). Solving \\(\\Phi(x)=0\\), we get two roots \\[\nz_{1,2} = \\frac{-1 \\pm \\sqrt{1 + 4}}{2} = \\frac{-1 \\pm \\sqrt{5}}{2} .\n\\] Since \\(\\left| \\frac{-1 - \\sqrt{5}}{2} \\right| &gt;1\\) and \\(\\left| \\frac{-1 + \\sqrt{5}}{2} \\right| &lt;1\\), this process is stationary but non-causal. Note: in this example \\(z_{1,2}\\) are both real, so the modulus \\(|z|\\) reduces to the absolute value of real number.\nIn general, for a generic AR(\\(2\\)) equation \\[\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t ,\n\\] the AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x - \\phi_2 x^2\\), which has two roots \\[\nz_{1,2} = \\frac{-\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{2\\phi_2} .\n\\] The causality condition can be explicitly characterized by \\[\n|z_{1,2}| &gt; 1 \\iff\n\\begin{cases}\n\\phi_1 + \\phi_2 &lt; 1 \\\\\n\\phi_2 - \\phi_1 &lt; 1 \\\\\n|\\phi_2| &lt; 1\n\\end{cases}\n\\]"
  },
  {
    "objectID": "TSA-Lecture05.html#derive-the-glp-form",
    "href": "TSA-Lecture05.html#derive-the-glp-form",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "Recall last time, from the AR(\\(p\\)) \\[\nY_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2} - \\cdots - \\phi_p Y_{t-p} = e_t, \\quad\ne_t \\sim \\mathrm{iid}(0, \\sigma_e^2),\n\\] we got the reformulated form \\(\\Phi(B) Y_t = e_t\\), where the AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p\\). If all the \\(p\\) complex roots of the AR polynomial satisfy \\(|z_i| &gt;1\\) for all \\(i=1,...,p\\), i.e., all the \\(p\\) roots are outside the unit disc in \\(\\mathbb{C}\\), then we showed that the AR(\\(p\\)) equation became \\[\ne_t = \\left(1-\\frac{B}{z_1}\\right) \\cdots \\left(1-\\frac{B}{z_p}\\right) Y_t .\n\\] Since each \\(|\\frac{1}{z_i}|&lt;1\\), we have \\(\\left(1-\\frac{B}{z_i}\\right) ^{-1} = \\sum_{j=0}^\\infty z_i^{-j} B^j\\). Then \\[\n\\begin{split}\nY_t &= \\left(1-\\frac{B}{z_1}\\right)^{-1} \\cdots \\left(1-\\frac{B}{z_p}\\right)^{-1} e_t \\\\\n&= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) e_t .\n\\end{split}\n\\] As we mentioned last time, this product can be written as a GLP. To see this, we first consider the simple case \\(p=2\\). Suppose \\(a_j = z_1^{-j}\\) and \\(b_j = z_2^{-j}\\). Then \\[\n\\begin{split}\n& \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\left(\\sum_{j=0}^\\infty z_2^{-j} B^j \\right) = \\left(\\sum_{j=0}^\\infty a_j B^j \\right) \\left(\\sum_{j=0}^\\infty b_j B^j \\right) \\\\\n&= \\left(a_0 + a_1 B^1 + a_2 B^2 + \\cdots \\right) \\left(b_0 + b_1 B^1 + b_2 B^2 + \\cdots \\right) \\\\\n&= a_0 b_0 + (a_0 b_1 + a_1 b_0) B^1 + (a_0 b_2 + a_1 b_1 + a_2 b_0) B^2 + (a_0 b_3 + a_1 b_2 + a_2 b_1 + a_3 b_0) B^3 + \\cdots \\\\\n&= \\sum_{n=0}^\\infty \\underbrace{\\left( \\sum_{j_1+j_2 = n} a_{j_1} b_{j_2} \\right)}_{c_n} B^n .\n\\end{split}\n\\] For general \\(p\\), we have the similar result \\[\n\\left(\\sum_{j=0}^\\infty a_{1,j} B^j \\right) \\left(\\sum_{j=0}^\\infty a_{2,j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty a_{p,j} B^j \\right) = \\sum_{n=0}^\\infty c_n B^n\n\\] where \\(c_n = \\sum_{j_1+j_2+\\cdots + j_p = n} a_{1,j_1} a_{2,j_2} \\cdots a_{p,j_p}\\). Note: this can be seen as a \\(p\\)-fold convolution. Using this result, \\[\n\\begin{split}\n& \\Phi(B)^{-1}\n= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) = \\sum_{n=0}^\\infty \\psi_n B^n ,\\\\\n& \\text{ where}\\quad \\psi_n = \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p} .\n\\end{split}\n\\] So the AR(\\(p\\)) can be written as \\[\n\\begin{split}\nY_t\n&= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) e_t = \\left(\\sum_{n=0}^\\infty \\psi_n B^n \\right) e_t = \\sum_{n=0}^\\infty \\psi_n e_{t-n}\n\\end{split}\n\\] which looks like a GLP, with the \\(\\{\\psi_n\\}\\) specified above. To ensure this is a GLP, we still need to verify \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\) (see the definition of GLP)."
  },
  {
    "objectID": "TSA-Lecture05.html#sketch-of-the-remaining-proof",
    "href": "TSA-Lecture05.html#sketch-of-the-remaining-proof",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "The sketch of the proof for \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\): \\[\n\\begin{split}\n\\sum_{n=0}^\\infty |\\psi_n| &= \\sum_{n=0}^\\infty \\left| \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p} \\right| \\le \\sum_{n=0}^\\infty \\sum_{j_1+j_2+\\cdots + j_p = n} |z_1|^{-j_i} |z_2|^{-j_2} \\cdots |z_p|^{-j_p} \\\\\n&\\le \\sum_{n=0}^\\infty \\sum_{j_1+j_2+\\cdots + j_p = n} |z^*|^{-j_1-j_2-\\cdots - j_p}\n\\le \\sum_{n=0}^\\infty c(p) n^p |z^*|^{-n} &lt; \\infty .\n\\end{split}\n\\] Notes on the missing details (without proof):\n\nLet \\(z^*\\) be the root among \\(\\{z_1,\\dots, z_p\\}\\) with the smallest modulus, so \\(1&lt;|z^*| \\le \\min\\{ |z_1|,\\dots, |z_p| \\}\\).\nThe number of terms in the summation \\(\\sum_{j_1+j_2+\\cdots + j_p = n}\\) can be upper bounded by \\(c(p) n^p\\) where \\(c(p)\\) is a constant that only depends on \\(p\\). (Since \\({n+p-1\\choose p-1} = \\frac{(n+p-1)(n+p-2) \\cdots (n+1)}{ (p-1)!}\\) is a polynomial of \\(n\\) with degree less than \\(p\\) and coefficients only depend on \\(p\\).)\nThe last step is because \\(\\sum_{n=0}^\\infty n^p |z^*|^{-n}\\) converges for \\(|z^*| &gt;1\\).\n\nSo we just showed \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\), which finishes the proof that \\(Y_t = \\Phi(B)^{-1} e_t\\) is a GLP (as long as all roots of \\(\\Phi(x)\\) are outside the unit disc in \\(\\mathbb{C}\\))."
  },
  {
    "objectID": "TSA-Lecture05.html#discussion-on-other-cases-without-proof",
    "href": "TSA-Lecture05.html#discussion-on-other-cases-without-proof",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "In summary, for AR(\\(p\\)) process, we have the following results. (Look similar to the three cases for AR(\\(1\\)) from last lecture.)\n\nIf all roots of the AR polynomial are outside the unit disc (\\(|z_i| &gt; 1\\) for all \\(i\\)), then \\((Y_t)\\) is causal and stationary.\nIf at least one of the roots is inside the unit disc (\\(|z_i| &lt; 1\\) for one \\(z_i\\)), and none of the roots is on the unit circle (\\(|z_i| \\neq 1\\) for all \\(i\\)), then \\((Y_t)\\) is non-causal (future-dependent) and stationary.\nIf at least one root is a unit root (i.e. \\(|z_i| = 1\\) for one \\(z_i\\)), then \\((Y_t)\\) is not stationary."
  },
  {
    "objectID": "TSA-Lecture05.html#example-ar2",
    "href": "TSA-Lecture05.html#example-ar2",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "We start with a concrete AR(\\(2\\)) example. Suppose the AR(\\(2\\)) equation is \\[\nY_t = Y_{t-1} + Y_{t-2} + e_t .\n\\] We can use the previous results to find whether this process is causal or stationary. The AR polynomial for this example is \\(\\Phi(x) = 1 - x - x^2\\). Solving \\(\\Phi(x)=0\\), we get two roots \\[\nz_{1,2} = \\frac{-1 \\pm \\sqrt{1 + 4}}{2} = \\frac{-1 \\pm \\sqrt{5}}{2} .\n\\] Since \\(\\left| \\frac{-1 - \\sqrt{5}}{2} \\right| &gt;1\\) and \\(\\left| \\frac{-1 + \\sqrt{5}}{2} \\right| &lt;1\\), this process is stationary but non-causal. Note: in this example \\(z_{1,2}\\) are both real, so the modulus \\(|z|\\) reduces to the absolute value of real number.\nIn general, for a generic AR(\\(2\\)) equation \\[\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t ,\n\\] the AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x - \\phi_2 x^2\\), which has two roots \\[\nz_{1,2} = \\frac{-\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{2\\phi_2} .\n\\] The causality condition can be explicitly characterized by \\[\n|z_{1,2}| &gt; 1 \\iff\n\\begin{cases}\n\\phi_1 + \\phi_2 &lt; 1 \\\\\n\\phi_2 - \\phi_1 &lt; 1 \\\\\n|\\phi_2| &lt; 1\n\\end{cases}\n\\]"
  },
  {
    "objectID": "TSA-Lecture05.html#method-1-using-convolution",
    "href": "TSA-Lecture05.html#method-1-using-convolution",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "3.1 Method 1: using convolution",
    "text": "3.1 Method 1: using convolution\nWe can use the formula \\(\\psi_n = \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p}\\) from the first part of this lecture.\nExample: consider the AR(\\(2\\)) equation \\(Y_t = \\frac{1}{6} Y_{t-1} + \\frac{1}{6} Y_{t-2} + e_t\\).\nExercise: verify this AR(\\(2\\)) process is causal, and the roots of the AR polynomial are \\(\\{-3, 2\\}\\).\nThen we can use the formula above to calculate \\(\\psi_n\\): \\[\n\\begin{split}\n\\psi_0 &= z_1^0 z_2^0 = 1 \\\\\n\\psi_1 &= z_1^0 z_2^{-1} + z_1^{-1} z_2^0 = 1 \\times \\frac{1}{2} + \\frac{1}{-3} \\times 1 = \\frac{1}{2} + \\left(-\\frac{1}{3}\\right) = \\frac{1}{6} \\\\\n\\psi_2 &= z_1^0 z_2^{-2} + z_1^{-1} z_2^{-1} + z_1^{-2} z_2^0 = \\frac{1}{4} + \\frac{1}{-3} \\cdot \\frac{1}{2} + \\frac{1}{9} = \\frac{7}{36} \\\\\n\\cdots\n\\end{split}\n\\]"
  },
  {
    "objectID": "TSA-Lecture05.html#method-2-using-arp-equation",
    "href": "TSA-Lecture05.html#method-2-using-arp-equation",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "3.2 Method 2: using AR(\\(p\\)) equation",
    "text": "3.2 Method 2: using AR(\\(p\\)) equation\nWe can use the AR(\\(p\\)) equation and directly solve a system for \\(\\{\\psi_n\\}\\). Consider the same example \\[\nY_t = \\frac{1}{6} Y_{t-1} + \\frac{1}{6} Y_{t-2} + e_t .\n\\] Since we want to get the GLP form \\(Y_t = \\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots\\), we can plug it into the equation above: \\[\n\\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots = \\frac{1}{6} (\\psi_0 e_{t-1} + \\psi_1 e_{t-2} + \\psi_2 e_{t-3} + \\cdots) + \\frac{1}{6} (\\psi_0 e_{t-2} + \\psi_1 e_{t-3} + \\psi_2 e_{t-4} + \\cdots) + e_t .\n\\] For this to hold, the coefficients for each \\(e_{t-n}\\) term (\\(n=0,1,\\dots\\)) on both sides should match. So we get the following system of equations \\[\n\\begin{split}\n\\psi_0 &= 1 \\\\\n\\psi_1 &= \\frac{1}{6} \\psi_0 \\\\\n\\psi_n &= \\frac{1}{6} \\psi_{n-1} + \\frac{1}{6} \\psi_{n-2} \\quad\\text{for } n\\ge 2\n\\end{split}\n\\] So \\(\\psi_0 = 1\\), \\(\\psi_1 = \\frac{1}{6} \\psi_0 = \\frac{1}{6}\\), \\(\\psi_2 = \\frac{1}{6} \\psi_1 + \\frac{1}{6} \\psi_0 = \\frac{1}{36} + \\frac{1}{6} = \\frac{7}{36}\\). These results are same as the earlier convolution method."
  },
  {
    "objectID": "TSA-Lecture05.html#a-property-of-the-recursion-part",
    "href": "TSA-Lecture05.html#a-property-of-the-recursion-part",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "4.1 A property of the recursion part",
    "text": "4.1 A property of the recursion part\nIf \\(k\\) is large, solving \\(\\gamma_k\\) recursively from \\((\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\) may be hard. We have the following useful property.\nClaim: If the roots of the AR polynomial \\(\\Phi(x)\\) are distinct, then there exist (complex) numbers \\(A_1, \\dots, A_p\\), such that the solution to \\[\n\\begin{cases}\n\\text{initial conditions for } (\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\\\\n\\text{recursion equation: } \\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2} + \\cdots + \\phi_p \\gamma_{k-p}, \\quad \\forall k\\ge p\n\\end{cases}\n\\] is given by \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}, \\quad \\forall k \\ge 0 .\n\\]\nThis gives another idea to calculate \\(\\gamma_k\\) in the final step of the YW method (useful when we are targeting for a large \\(k\\)): after getting the values of \\((\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\), we can solve \\((A_1,...,A_p)\\) such that \\(A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k} = \\gamma_k\\) hold for all \\(0\\le k\\le p-1\\). Then for this solution \\((A_1,...,A_p)\\), the formula \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) (for \\(k\\ge 0\\)) gives the ACVF we wanted."
  },
  {
    "objectID": "TSA-Lecture05.html#a-related-observation",
    "href": "TSA-Lecture05.html#a-related-observation",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "4.2 A related observation",
    "text": "4.2 A related observation\nFor causal AR(\\(p\\)) process, the ACVF has the following asymptotic rate (up to some constant factor) \\[\n\\gamma_k \\approx \\Bigl[ \\min \\left\\{ |z_1|, \\ldots, |z_p| \\right\\} \\Bigr]^{-k}, \\text{ as } k\\to \\infty.\n\\] It decays exponentially, and never stays at zero for a ``long time”.\nExample: if the AR polynomial for an AR(\\(3\\)) has roots \\(\\{z_1 = 2, z_2=3, z_3=10\\}\\), then \\(\\gamma_k \\approx 2^{-k}\\) for large \\(k\\).\nNote: should be \\(\\gamma_k \\approx c \\cdot \\left[ \\min \\left\\{ |z_1|, \\ldots, |z_p| \\right\\} \\right]^{-k}\\)."
  },
  {
    "objectID": "TSA-Lecture07.html",
    "href": "TSA-Lecture07.html",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "",
    "text": "Recall last time, for the ARMA(\\(1,1\\)) \\[\nY_t - \\phi Y_{t-1} = e_t - \\theta e_{t-1},\n\\] we derived a causal GLP (also an MA(\\(\\infty\\))) representation for \\((Y_t)\\) \\[\nY_t = e_t + \\sum_{k=1}^{\\infty} \\phi^{k-1} (\\phi - \\theta) e_{t-k}\n\\] whenever the ARMA(\\(1,1\\)) is causal (only if \\(|\\phi|&lt;1\\)). We will use the GLP representation above to find the ACVFs for \\((Y_t)\\) through the Yule-Walker method.\nFor \\(k\\ge 0\\), multiply \\(Y_{t-k}\\) on both sides and take the expectations. \\[\n\\begin{split}\nY_t - \\phi Y_{t-1} &= e_t - \\theta e_{t-1},\\\\\nY_t Y_{t-k} - \\phi Y_{t-1} Y_{t-k} &= e_t Y_{t-k} - \\theta e_{t-1} Y_{t-k}, \\\\\n\\mathbb{E}[Y_t Y_{t-k}] - \\phi \\mathbb{E}[Y_{t-1} Y_{t-k}] &= \\mathbb{E}[e_t Y_{t-k}] - \\theta \\mathbb{E}[e_{t-1} Y_{t-k}] .\n\\end{split}\n\\] As before, since \\((Y_n)\\) is mean zero and stationary, \\(\\mathbb{E}[Y_{t} Y_{t-k}] = \\gamma_{k}\\) and \\(\\mathbb{E}[Y_{t-1} Y_{t-k}] = \\gamma_{k-1}\\). So the \\(k\\)-th YW equation is \\[\n\\gamma_{k} - \\phi \\gamma_{k-1}= \\mathbb{E}[e_t Y_{t-k}] - \\theta \\mathbb{E}[e_{t-1} Y_{t-k}].\n\\] Assuming causality, we can use the previous causal GLP, then \\[\n\\gamma_{k} - \\phi \\gamma_{k-1}= \\mathbb{E}[e_t (e_{t-k}+ \\psi_1 e_{t-k-1 + \\cdots})] - \\theta \\mathbb{E}[e_{t-1} (e_{t-k}+ \\psi_1 e_{t-k-1 + \\cdots})].\n\\] Consider all possible \\(k\\ge 0\\): \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 - \\theta \\psi_1 \\sigma_e^2, &0\\text{th YW eq}\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = 0 - \\theta \\sigma_e^2, &1\\text{st YW eq}\\\\\n\\gamma_{k} - \\phi \\gamma_{k-1}= 0 - \\theta 0 = 0, &k\\text{-th YW eq, for } k\\ge 2\n\\end{cases}\n\\] Recall that \\(\\psi_1 = \\phi-\\theta\\), from the first 2 YW equations we have \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 - \\theta (\\phi-\\theta) \\sigma_e^2 \\quad &(\\text{YW}0)\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = - \\theta \\sigma_e^2 &(\\text{YW}1)\n\\end{cases}\n\\] \\((\\text{YW}0) + \\phi(\\text{YW}1)\\): \\[\n(1 - \\phi^2)\\gamma_0 = (1 - 2\\theta\\phi + \\theta^2)\\sigma_e^2\n\\implies \\gamma_0 = \\frac{1 - 2\\theta\\phi + \\theta^2}{1 - \\phi^2}\\sigma_e^2 .\n\\] Then plug it into \\((\\text{YW}1)\\): \\[\n\\begin{split}\n\\gamma_1 &= \\phi\\gamma_0 - \\theta\\sigma_e^2\n= \\sigma_e^2 \\left( \\frac{\\phi(1-2\\theta\\phi+\\theta^2)}{1-\\phi^2}\n  - \\frac{\\theta (1-\\phi^2)}{1-\\phi^2} \\right) \\\\\n&= \\sigma_e^2 \\left( \\frac{\\phi - 2\\theta\\phi^2 + \\phi\\theta^2 - \\theta + \\theta\\phi^2}\n       {1-\\phi^2} \\right) \\\\\n&= \\sigma_e^2 \\frac{\\phi - \\theta + \\theta\\phi(\\theta-\\phi)}{1-\\phi^2}\n= \\sigma_e^2 \\frac{(\\phi-\\theta)(1-\\theta\\phi)}{1-\\phi^2} .\n\\end{split}\n\\] Use the \\(k\\)-th YW equations (for \\(k\\ge 2\\)) recursively: \\[\n\\gamma_k = \\phi^{k-1} \\frac{(\\phi - \\theta)(1 - \\theta\\phi)}{1 - \\phi^2} \\sigma_e^2, \\quad \\forall k\\ge 1.\n\\] We can observe that, \\(\\gamma_k \\to 0\\) exponentially as \\(k\\to \\infty\\) (since \\(|\\phi|&lt;1\\) under causality condition).\n\n\n\nBy the invertiblity condition (see last lecture), we need all the roots of MA polynomial are outside the unit disk, which reduces to \\(|\\theta| &lt;1\\) for ARMA(\\(1,1\\)). When invertibility holds, we have the following invertible representation (also an AR(\\(\\infty\\))) for ARMA(\\(1,1\\)): \\[\ne_t = Y_t + \\sum_{j=1}^{\\infty} \\theta^{j-1} (\\theta - \\phi) Y_{t-j} .\n\\] Exercise: derive the formula above, and verify \\(\\sum_{j=0}^{\\infty} |\\pi_j| &lt; \\infty\\) in this invertible representation."
  },
  {
    "objectID": "TSA-Lecture07.html#acvf-of-arma11",
    "href": "TSA-Lecture07.html#acvf-of-arma11",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "",
    "text": "Recall last time, for the ARMA(\\(1,1\\)) \\[\nY_t - \\phi Y_{t-1} = e_t - \\theta e_{t-1},\n\\] we derived a causal GLP (also an MA(\\(\\infty\\))) representation for \\((Y_t)\\) \\[\nY_t = e_t + \\sum_{k=1}^{\\infty} \\phi^{k-1} (\\phi - \\theta) e_{t-k}\n\\] whenever the ARMA(\\(1,1\\)) is causal (only if \\(|\\phi|&lt;1\\)). We will use the GLP representation above to find the ACVFs for \\((Y_t)\\) through the Yule-Walker method.\nFor \\(k\\ge 0\\), multiply \\(Y_{t-k}\\) on both sides and take the expectations. \\[\n\\begin{split}\nY_t - \\phi Y_{t-1} &= e_t - \\theta e_{t-1},\\\\\nY_t Y_{t-k} - \\phi Y_{t-1} Y_{t-k} &= e_t Y_{t-k} - \\theta e_{t-1} Y_{t-k}, \\\\\n\\mathbb{E}[Y_t Y_{t-k}] - \\phi \\mathbb{E}[Y_{t-1} Y_{t-k}] &= \\mathbb{E}[e_t Y_{t-k}] - \\theta \\mathbb{E}[e_{t-1} Y_{t-k}] .\n\\end{split}\n\\] As before, since \\((Y_n)\\) is mean zero and stationary, \\(\\mathbb{E}[Y_{t} Y_{t-k}] = \\gamma_{k}\\) and \\(\\mathbb{E}[Y_{t-1} Y_{t-k}] = \\gamma_{k-1}\\). So the \\(k\\)-th YW equation is \\[\n\\gamma_{k} - \\phi \\gamma_{k-1}= \\mathbb{E}[e_t Y_{t-k}] - \\theta \\mathbb{E}[e_{t-1} Y_{t-k}].\n\\] Assuming causality, we can use the previous causal GLP, then \\[\n\\gamma_{k} - \\phi \\gamma_{k-1}= \\mathbb{E}[e_t (e_{t-k}+ \\psi_1 e_{t-k-1 + \\cdots})] - \\theta \\mathbb{E}[e_{t-1} (e_{t-k}+ \\psi_1 e_{t-k-1 + \\cdots})].\n\\] Consider all possible \\(k\\ge 0\\): \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 - \\theta \\psi_1 \\sigma_e^2, &0\\text{th YW eq}\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = 0 - \\theta \\sigma_e^2, &1\\text{st YW eq}\\\\\n\\gamma_{k} - \\phi \\gamma_{k-1}= 0 - \\theta 0 = 0, &k\\text{-th YW eq, for } k\\ge 2\n\\end{cases}\n\\] Recall that \\(\\psi_1 = \\phi-\\theta\\), from the first 2 YW equations we have \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 - \\theta (\\phi-\\theta) \\sigma_e^2 \\quad &(\\text{YW}0)\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = - \\theta \\sigma_e^2 &(\\text{YW}1)\n\\end{cases}\n\\] \\((\\text{YW}0) + \\phi(\\text{YW}1)\\): \\[\n(1 - \\phi^2)\\gamma_0 = (1 - 2\\theta\\phi + \\theta^2)\\sigma_e^2\n\\implies \\gamma_0 = \\frac{1 - 2\\theta\\phi + \\theta^2}{1 - \\phi^2}\\sigma_e^2 .\n\\] Then plug it into \\((\\text{YW}1)\\): \\[\n\\begin{split}\n\\gamma_1 &= \\phi\\gamma_0 - \\theta\\sigma_e^2\n= \\sigma_e^2 \\left( \\frac{\\phi(1-2\\theta\\phi+\\theta^2)}{1-\\phi^2}\n  - \\frac{\\theta (1-\\phi^2)}{1-\\phi^2} \\right) \\\\\n&= \\sigma_e^2 \\left( \\frac{\\phi - 2\\theta\\phi^2 + \\phi\\theta^2 - \\theta + \\theta\\phi^2}\n       {1-\\phi^2} \\right) \\\\\n&= \\sigma_e^2 \\frac{\\phi - \\theta + \\theta\\phi(\\theta-\\phi)}{1-\\phi^2}\n= \\sigma_e^2 \\frac{(\\phi-\\theta)(1-\\theta\\phi)}{1-\\phi^2} .\n\\end{split}\n\\] Use the \\(k\\)-th YW equations (for \\(k\\ge 2\\)) recursively: \\[\n\\gamma_k = \\phi^{k-1} \\frac{(\\phi - \\theta)(1 - \\theta\\phi)}{1 - \\phi^2} \\sigma_e^2, \\quad \\forall k\\ge 1.\n\\] We can observe that, \\(\\gamma_k \\to 0\\) exponentially as \\(k\\to \\infty\\) (since \\(|\\phi|&lt;1\\) under causality condition)."
  },
  {
    "objectID": "TSA-Lecture07.html#invertible-representation-for-arma11",
    "href": "TSA-Lecture07.html#invertible-representation-for-arma11",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "",
    "text": "By the invertiblity condition (see last lecture), we need all the roots of MA polynomial are outside the unit disk, which reduces to \\(|\\theta| &lt;1\\) for ARMA(\\(1,1\\)). When invertibility holds, we have the following invertible representation (also an AR(\\(\\infty\\))) for ARMA(\\(1,1\\)): \\[\ne_t = Y_t + \\sum_{j=1}^{\\infty} \\theta^{j-1} (\\theta - \\phi) Y_{t-j} .\n\\] Exercise: derive the formula above, and verify \\(\\sum_{j=0}^{\\infty} |\\pi_j| &lt; \\infty\\) in this invertible representation."
  },
  {
    "objectID": "TSA-Lecture07.html#find-the-causal-glp-representation",
    "href": "TSA-Lecture07.html#find-the-causal-glp-representation",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "2.1 Find the causal GLP representation",
    "text": "2.1 Find the causal GLP representation\nConsider the ARMA(\\(p,q\\)) \\[\nY_t - \\phi_1 Y_{t-1} - \\cdots - \\phi_p Y_{t-p} = e_t - \\theta_1 e_{t-1} - \\cdots - \\theta_q e_{t-q} .\n\\] Assume the causality condition holds, i.e., assume all the roots of the AR polynomial are outside of the unit disk. So there exists a causal GLP representation \\(Y_t = \\sum_{j=0}^{\\infty} \\psi_j e_{t-j}\\). Our goal is to find \\(\\{\\psi_j\\}\\).\nWe can simply plug it into the ARMA(\\(p,q\\)) equation \\[\n\\begin{split}\n&\\left( \\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots \\right)\n  - \\phi_1 \\left( \\psi_0 e_{t-1} + \\psi_1 e_{t-2} + \\psi_2 e_{t-3} + \\cdots \\right) - \\cdots - \\phi_p \\left( \\psi_0 e_{t-p} + \\psi_1 e_{t-p-1} + \\psi_2 e_{t-p-2} + \\cdots \\right) \\\\\n&= e_t - \\theta_1 e_{t-1} - \\theta_2 e_{t-2} - \\cdots - \\theta_q e_{t-q} .\n\\end{split}\n\\] Then compare the coefficients for \\(e_{t-k}\\) on both sides: \\[\n\\begin{cases}\n\\psi_0 = 1 \\\\\n\\psi_1 - \\phi_1 \\psi_0 = -\\theta_1 \\\\\n\\psi_2 - \\phi_1 \\psi_1 - \\phi_2 \\psi_0 = -\\theta_2 \\\\\n\\psi_3 - \\phi_1 \\psi_2 - \\phi_2 \\psi_1 - \\phi_3 \\psi_0 = -\\theta_3 \\\\\n\\quad \\cdots \\\\\n\\psi_j - \\phi_1 \\psi_{j-1} - \\phi_2 \\psi_{j-2} - \\cdots - \\phi_p \\psi_{j-p} = 0, \\text{ for large } j \\text{ such that } j\\ge p \\text{ and } j&gt;q\n\\end{cases}\n\\] So we get \\[\n\\begin{cases}\n\\psi_0 = 1 \\\\\n\\psi_1 = \\phi_1 - \\theta_1 \\\\\n\\psi_2 = \\phi_1 \\psi_1 + \\phi_2 - \\theta_2 \\\\\n\\psi_3 = \\phi_1 \\psi_2 + \\phi_2 \\psi_1 + \\phi_3 - \\theta_3 \\\\\n\\quad\\cdots \\\\\n\\psi_j = \\phi_1 \\psi_{j-1} + \\phi_2 \\psi_{j-2} + \\cdots + \\phi_p \\psi_{j-p}, \\text{ for large } j \\text{ such that } j\\ge p \\text{ and } j&gt;q\n\\end{cases}\n\\] which can be seen as recursive equations for \\(\\{\\psi_j\\}\\)."
  },
  {
    "objectID": "TSA-Lecture07.html#yw-approach-for-acvf",
    "href": "TSA-Lecture07.html#yw-approach-for-acvf",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "2.2 YW approach for ACVF",
    "text": "2.2 YW approach for ACVF\nThe basic idea of YW method is same as before, assume causality and stationarity. For any \\(k&gt;q\\), the \\(k\\)-th YW equations is \\[\n\\begin{split}\n\\mathbb{E}[Y_t Y_{t-k}] - \\phi_1 \\mathbb{E}[Y_{t-1} Y_{t-k}] - \\cdots - \\phi_p \\mathbb{E}[Y_{t-p} Y_{t-k}] &= \\mathbb{E}[e_t Y_{t-k}] - \\theta_1 \\mathbb{E}[e_{t-1} Y_{t-k}] - \\cdots - \\theta_q \\mathbb{E}[e_{t-q} Y_{t-k}] \\\\\n\\gamma_{k} - \\phi_1 \\gamma_{k-1} - \\cdots - \\phi_p \\gamma_{k-p} &= 0, \\quad \\forall k&gt;q\n\\end{split}\n\\] So the recursion part of YW equations have the structure as \\(AR(p)\\). Using the earlier results, suppose the \\(p\\) roots (assuming \\(\\phi_p \\ne 0\\)) of the AR polynomial are \\(z_1,...,z_p\\) and they are all distinct, then there exist \\(p\\) complex numbers \\(A_1,...,A_p\\) such that \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\n\\] hold for all \\(k \\ge q+1-p\\).\n\nIf \\(p\\ge q\\): we need to solve \\((\\gamma_0,...,\\gamma_p)\\) from the first \\(p+1\\) YW equations (YW eq\\(0\\), …, YW eq\\(p\\)), then we can use \\((\\gamma_{1},...,\\gamma_{p})\\) as the initial conditions to determine \\((A_1,...,A_p)\\).\nIf \\(q&gt; p\\): we need to solve \\((\\gamma_0,...,\\gamma_q)\\) from the first \\(q+1\\) YW equations (YW eq\\(0\\), …, YW eq\\(q\\)), then use \\((\\gamma_{q-p+1},...,\\gamma_{q})\\) as the initial conditions to determine \\((A_1,...,A_p)\\).\n\nRemark: dividing the YW equation by \\(\\gamma_0\\) gives \\(\\rho_{k} - \\phi_1 \\rho_{k-1} - \\cdots - \\phi_p \\rho_{k-p} = 0\\) for \\(k&gt;q\\). So the ACF has the same structure \\(\\rho_k = \\widetilde{A}_1 z_1^{-k} + \\widetilde{A}_2 z_2^{-k} + \\cdots + \\widetilde{A}_p z_p^{-k}\\).\nRemark: if the \\(p\\) roots are not distinct, ACVF/ACF have more complicated formula as we discussed before (see last lecture)."
  },
  {
    "objectID": "TSA-Lecture08.html",
    "href": "TSA-Lecture08.html",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "",
    "text": "So far, we considered models for stationary time series AR(\\(p\\)), MA(\\(q\\)), ARMA(\\(p,q\\)). These models can be used for the (stationary) stochastic component in observed time series.\n(Assume the underlying distribution of \\((Y_t)\\) is stationary. See Wold’s decomposition theorem from earlier lectures.) For an observed time series \\((Y_t)\\), it can be represented as a sum of two parts: \\[\n\\underbrace{Y_t}_{observed} = \\underbrace{\\mu_t}_{deterministic} + \\underbrace{X_t}_{stochastic} .\n\\]\n\n\\(\\{X_t\\}\\) is a stationary stochastic component. We may fit a model (like AR(\\(p\\)), MA(\\(q\\)), ARMA(\\(p,q\\))) for it.\n\\(\\{\\mu_t\\}\\) is a deterministic component. And it is often non-stationary (which reduces to non-constant, since it is deterministic). It may reflect the trend, or trend combined with seasonality.\n\nThe idea to deal with \\((Y_t)\\): first estimate \\(\\mu_t\\) with \\(\\widehat{\\mu}_t\\), then extract \\(X_t\\) by \\(X_t = Y_t - \\widehat{\\mu}_t\\)."
  },
  {
    "objectID": "TSA-Lecture08.html#example-ma1",
    "href": "TSA-Lecture08.html#example-ma1",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "2.1 Example: MA(\\(1\\))",
    "text": "2.1 Example: MA(\\(1\\))\nMA(\\(1\\)) time series are all \\(q\\)-dependent with \\(q=1\\). Consider an MA(\\(1\\)) with \\(\\theta= \\frac{1}{2}\\): \\[\nY_t = e_t - \\frac{1}{2} e_{t-1}.\n\\] We have \\[\n\\rho_1 = \\frac{-\\theta}{1 + \\theta^2}\n= \\frac{-\\frac{1}{2}}{1 + \\frac{1}{4}}\n= -\\frac{2}{5}, \\quad \\rho_k=0 \\text{ for } k\\ge 2.\n\\] Then \\[\n\\begin{split}\n\\operatorname{Var}(\\overline{Y}) &= \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\rho_k \\right] = \\frac{\\gamma_0}{n} \\left( 1+ 2\\left( 1-\\frac{1}{n}\\right)\\rho_1 \\right) \\\\\n&= \\frac{\\gamma_0}{n} \\left( 1+ 2\\left( 1-\\frac{1}{n}\\right)(-0.4) \\right) \\\\\n&\\approx \\frac{\\gamma_0}{n}(1-2\\cdot 0.4) = \\frac{\\gamma_0}{5n} \\quad (\\text{for large }n)\n\\end{split}\n\\] So for large \\(n\\), \\(\\operatorname{Var}(\\overline{Y})\\) is approximately \\(5\\) times smaller than the variance of the iid case.\n(Note: this is a mean-reverting time series.)\nSince \\((Y_t)\\) is \\(q\\)-dependent (with \\(q=1\\)), the \\(q\\)-dependent CLT works. So we know \\(\\overline{Y}\\) is approximately normally distributed for large \\(n\\). Then we can construct confidence interval for \\(\\mu\\): \\[\n\\mu\\in \\left[\\overline{Y} \\pm 2\\sqrt{\\operatorname{Var}(\\overline{Y})} \\right] = \\left[\\overline{Y} \\pm 2\\sqrt{\\frac{\\gamma_0}{5n}} \\right], \\text{ with prob } 95\\% .\n\\]"
  },
  {
    "objectID": "TSA-Lecture08.html#example-ar1",
    "href": "TSA-Lecture08.html#example-ar1",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "2.2 Example: AR(\\(1\\))",
    "text": "2.2 Example: AR(\\(1\\))\nConsider a causal AR(\\(1\\)) (with \\(|\\phi|&lt;1\\)): \\[\nY_t - \\phi Y_{t-1} = e_t .\n\\] For causal AR(\\(1\\)), we have \\(\\rho_k = \\phi^k\\) (for \\(k\\ge 0\\)). So \\[\n\\operatorname{Var}(\\overline{Y}) = \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\rho_k \\right] = \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\phi^k \\right].\n\\] For large \\(n\\), we have the following approximation \\[\n\\operatorname{Var}(\\overline{Y}) = \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\phi^k \\right] \\approx \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^\\infty \\phi^k \\right] = \\frac{\\gamma_0}{n} \\frac{1+\\phi}{1-\\phi}.\n\\] Exercise: verify the last step \\(1 + 2 \\sum_{k=1}^\\infty \\phi^k = \\frac{1+\\phi}{1-\\phi}\\).\nFor example, if \\(\\phi=0.9\\), then \\(\\frac{1+\\phi}{1-\\phi}=19 \\approx 20\\), so for large \\(n\\), \\(\\operatorname{Var}(\\overline{Y}) \\approx \\frac{20 \\gamma_0}{n}\\) which is approximately \\(20\\) times larger than the variance of the iid case.\nRemark: This example is a ``mean-avoiding” time series. We can compare it with the previous MA(\\(1\\)) with \\(\\theta=0.5\\) example which we remarked as a mean-reverting time series, and we got \\(\\operatorname{Var}(\\overline{Y}) \\approx 0.2\\cdot \\frac{\\gamma_0}{n}\\). If we look at the formula for \\(\\operatorname{Var}(\\overline{Y})\\), the main difference is from the ACFs. In last example, \\(\\rho_1= -0.4\\) is negative and \\(\\rho_k=0\\) for \\(k\\ge2\\), which made \\(\\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\phi^k \\right] \\approx 0.2\\). In this example, \\(\\rho_k =0.9^k\\) gives \\(\\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\phi^k \\right] \\approx 20\\)."
  },
  {
    "objectID": "TSA-Lecture08.html#example-random-walk",
    "href": "TSA-Lecture08.html#example-random-walk",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "2.3 Example: random walk",
    "text": "2.3 Example: random walk\nConsider the random walk model \\[\nY_1=e_1, \\quad Y_t = Y_{t-1} + e_t, \\quad e_t \\sim \\mathrm{iid}(0,\\sigma_e^2).\n\\] Suppose we still want to look at the sample mean \\(\\overline{Y}\\). Note that the random walk \\((Y_t)\\) is not stationary, the earlier formula for \\(\\operatorname{Var}(\\overline{Y})\\) cannot be applied here. We can directly calculate the variance. Note that \\(Y_1 = e_1\\), \\(Y_2 = e_1+e_2\\),… \\[\n\\begin{split}\n\\operatorname{Var}(\\overline{Y}) &= \\frac{1}{n^2} \\operatorname{Var}\\left( \\sum_{t=1}^n Y_t \\right)\n= \\frac{1}{n^2} \\operatorname{Var}\\left( \\sum_{t=1}^n \\sum_{j=1}^t e_j \\right) \\\\\n&= \\frac{1}{n^2} \\operatorname{Var} \\left( \\sum_{t=1}^n (n-t+1) e_t \\right) = \\frac{1}{n^2} \\sum_{t=1}^n (n-t+1)^2 \\sigma_e^2 \\\\\n&= \\frac{\\sigma_e^2}{n^2} \\sum_{j=1}^n j^2 = \\sigma_e^2\\cdot \\frac{n (n+1)(2n+1)}{6 n^2} \\to \\infty, \\text{ as } n\\to\\infty\n\\end{split}\n\\] So as the sample size increases, we are less and less certain about the mean of a random walk."
  },
  {
    "objectID": "TSA-Lecture08.html#linear-trend",
    "href": "TSA-Lecture08.html#linear-trend",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "4.1 Linear trend",
    "text": "4.1 Linear trend\nConsider the linear regression model. Assume \\[\n\\mu_t = \\beta_0 + \\beta_1 t.\n\\] \\(\\left( Y_t \\right)_{t=1}^n\\) are observed data. To estimate \\(\\mu_t\\) via linear regression, we can minimize the objective function \\[\nQ(\\beta_0, \\beta_1) = \\sum_{t=1}^n \\left( Y_t - \\beta_0 - \\beta_1 t \\right)^2.\n\\] Solve the equations \\[\n\\begin{cases}\n0= \\frac{\\partial Q}{\\partial \\beta_0}\n= -2 \\sum_{t=1}^n \\left( Y_t - \\beta_0 - \\beta_1 t \\right)  \\\\\n0=\\frac{\\partial Q}{\\partial \\beta_1}\n= -2 \\sum_{t=1}^n \\left( Y_t - \\beta_0 - \\beta_1 t \\right) t\n\\end{cases}\n\\] we get the minimizer \\((\\widehat{\\beta}_0,\\, \\widehat{\\beta}_1) = \\underset{\\beta_0,\\beta_1}{\\arg\\min} Q(\\beta_0, \\beta_1)\\) is \\[\n\\widehat{\\beta}_1 = \\frac{\\sum_{t=1}^n (Y_t - \\overline{Y}) (t - \\overline{t})}{\\sum_{t=1}^n (t - \\overline{t})^2},\n\\quad\n\\widehat{\\beta}_0 = \\overline{Y} - \\widehat{\\beta}_1 \\overline{t}.\n\\] Then the estimated linear trend is \\[\n\\widehat{\\mu}_t = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 t.\n\\]"
  },
  {
    "objectID": "TSA-Lecture08.html#some-other-regression-models",
    "href": "TSA-Lecture08.html#some-other-regression-models",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "4.2 Some other regression models",
    "text": "4.2 Some other regression models\n\nQuadratic trend: Assume\n\n\\[\n\\mu_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2\n= \\begin{bmatrix} 1 & t & t^2 \\end{bmatrix}\n  \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\n\\] This is a special case of polynomial trends. Any polynomial trend can be estimated via linear regression.\n\nCosine trend: Assume\n\n\\[\n\\mu_t = \\beta_0 + \\beta_1 \\cos \\left( \\frac{2\\pi}{f} t \\right)\n+ \\beta_2 \\sin \\left( \\frac{2\\pi}{f} t \\right)\n\\] where \\(f\\) is frequency.\n\nSeasonal/Cyclical trend: Assume \\(\\mu_t\\) is periodic. For example, suppose \\(t\\) denotes the month, and we assume ``the means are the same for the same months”, i.e., \\[\n\\begin{split}\n&\\mu_1 = \\mu_{13} = \\mu_{25} = \\mu_{1+12k} \\quad (\\text{Jan}) \\\\\n&\\mu_2 = \\mu_{14} = \\mu_{2+12k} \\quad (\\text{Feb}) \\\\\n&\\cdots \\\\\n&\\mu_{12} = \\mu_{24}= \\mu_{12k} \\quad (\\text{Dec})\n\\end{split}\n\\] To estimate \\((\\mu_1,...,\\mu_{12})\\), we can fit the following linear regression using the observed \\((Y_t)\\) \\[\nY_t = \\beta_1 X_{\\mathrm{Jan}} + \\beta_2 X_{\\mathrm{Feb}} + \\cdots + \\beta_{12} X_{\\mathrm{Dec}} + \\varepsilon_t\n\\] where the indicator/dummy variables \\(X_{\\mathrm{month}}\\) are defined as \\[\nX_{\\mathrm{Jan}} =\n\\begin{cases}\n1,& \\text{if } t \\text{ is January}\\\\\n0,& \\text{otherwise}\n\\end{cases}\n\\] After solving this linear regression, we get estimates \\((\\widehat{\\beta}_1,...,\\widehat{\\beta}_{12})\\). Then we let \\((\\widehat{\\mu}_1,...,\\widehat{\\mu}_{12}) = (\\widehat{\\beta}_1,...,\\widehat{\\beta}_{12})\\)."
  }
]