[
  {
    "objectID": "TSA-Lecture23.html",
    "href": "TSA-Lecture23.html",
    "title": "25 Spring 439/639 TSA: Lecture 23",
    "section": "",
    "text": "ARCH/GARCH stands for Autoregressive Conditional Heteroskedastic / Generalized Autoregressive Conditional Heteroskedastic models. These models are models for conditional heteroskedasticity, which means that they allow the variance of the error terms to change over time. This is particularly useful in financial time series data, where volatility (i.e. conditional variance) tends to cluster over time.\nIn contrast, ARMA models, are models of conditional mean, while ARCH/GARCH models are used to model conditional variance. Specifically, for AR(1):\n\\[\n\\begin{aligned}\nY_t &= \\phi Y_{t-1} + e_t \\\\\ne_t &\\sim iid(0, \\sigma^2)\n\\end{aligned}\n\\]\nHere, unconditional mean is constant over time, \\(EY_t = 0\\). But conditional mean is given by:\n\\[\nE[Y_t | Y_{t-1}] = \\phi Y_{t-1} =: \\mu_{t|t-1}.\n\\]\nSo AR(1) model could be written as:\n\\[\nY_t = \\underbrace{\\mu_{t|t-1}}_{\\text{conditional mean}} + \\underbrace{\\sigma_0}_{\\text{conditional sd}} e_t,\\qquad e_t\\sim iid(0,1).\n\\]\n\n\nARCH(1) model, on the other hand, models conditional variance:\n\\[\n\\begin{cases}\nY_t = \\mu_{0} + \\sigma_{t|t-1} e_t, \\quad e_t\\sim iid(0,1) \\\\\n\\sigma_{t|t-1}^2 = \\alpha_0 + \\alpha_1 Y_{t-1}^2\n\\end{cases}\n\\]\nThis is conditionally heteroskedastic model, as the variance of the error term \\(e_t\\) is not constant but depends on the past values of the series. Note, we need \\(\\alpha_0 &gt; 0\\) and \\(\\alpha_1 &gt; 0\\) for the model to be valid. These models were developed by Robert Engle (ARCH, 1982) and Tim Bollerslev (GARCH, 1986). ARCH models are often used in financial time series analysis, particularly for modeling return volatility.\n\n\nIt can be shown that ARCH(1) process is stationary if \\(0&lt;\\alpha_1 &lt; 1\\). It may seem contradictory that a stationary process can have non-constant conditional variance. But, recall that weakly stationary processes has constant unconditional variance.\n\n\n\nConsider \\(X_t\\) the price of a stock/asset. The return can be defined as:\n\\[\nY_t = \\nabla \\log(X_t)\\approx \\frac{X_t - X_{t-1}}{X_{t-1}} = \\frac{X_t}{X_{t-1}} - 1\n\\] Standard deviation of \\(Y_t\\) is known as volatility.\nFor simplicity assume \\(\\mu_0=0\\), then ARCH(1) process becomes:\n\\[\n\\begin{cases}\nY_t = \\sigma_{t} e_t, \\quad e_t\\sim iid(0,1) \\\\\n\\sigma_{t}^2 = \\alpha_0 + \\alpha_1 Y_{t-1}^2\n\\end{cases}\n\\]\nIf the previous time series value \\(Y_{t-1}\\) is large, then volatility \\(\\sigma_{t}\\) will also be large, indicating a period of high uncertainty or risk in the asset’s return: \\(Y_t\\) may potentially be quite large as well.\n\n\n\n\nExpectation \\[\nEY_t=E[\\sigma_{t} e_t]=E[\\sigma_{t}]E[e_t]=0.\n\\]\nVariance \\[\nVar(Y_t) = E(Y_t^2) = E[\\sigma_{t}^2e_t^2] = E[\\sigma_{t}^2]E[e_t^2] = E[\\sigma_{t}^2]\n\\] \\[\n=\\alpha_0 + \\alpha_1 E[Y_{t-1}^2]=\\alpha_0 + \\alpha_1 Var(Y_t).\n\\]\n\nIn the last step we used the fact that ARCH(1) is stationary, so \\(E[Y_t^2] = Var(Y_t)\\). Solving for \\(Var(Y_t)\\) gives:\n\\[\nVar(Y_t) = \\frac{\\alpha_0}{1 - \\alpha_1}.\n\\]\nThus it is necessary to have \\(\\alpha_1 &lt; 1\\) for the model to be stable and for the variance to be finite.\nNote, the conditional variance will not be constant:\n\\[\nVar(Y_t|Y_{t-1}) = \\alpha_0 + \\alpha_1 Y_{t-1}^2.\n\\]\n\nDependence\n\n\\(Y_t\\) and \\(Y_{t-h}\\) are uncorrelated for \\(h &gt; 0\\), but dependent.\n\\[\ncov(Y_t,Y_{t-1}) = E[Y_t Y_{t-1}] - E[Y_t]E[Y_{t-1}] = E[Y_t Y_{t-1}]=E[\\sigma_{t} e_t \\sigma_{t-1} e_{t-1}] = E[\\sigma_{t} \\sigma_{t-1} e_{t-1}] E[e_t] = 0.\n\\] Above we used the fact that \\(e_t\\) is independent of the past, i.e. of \\(\\sigma_{t}, \\sigma_{t-1}, e_{t-1}\\).\n\n\\(Y_t\\) is leptokurtic, i.e. has heavier tails than normal.\n\nRecall, kurtosis is defined as \\(K=\\frac{\\mu_4}{\\sigma_t^2}=\\frac{EY_t^4}{(Var Y_t)^2}\\).\n\\[\n\\mu_4=E[Y_t^4]=E[\\sigma_t^4 e_t^4]=E[\\sigma_t^4]E[e_t^4]= 3E[\\sigma_t^4].\n\\]\n\\[\n=3E[(\\alpha_0+\\alpha_1 Y_{t-1}^2)^2]=3(\\alpha_0^2+\\alpha_1^2 E[Y_{t-1}^4]+2\\alpha_0\\alpha_1 E[Y_{t-1}^2]).\n\\]\nNote that \\(E[Y_{t-1}^2] = Var(Y_t)\\) and \\(E[Y_{t-1}^4] = \\mu_4\\) by stationarity, so \\[\n\\begin{aligned}\n\\mu_4(1-3\\alpha_1^2) &= 3\\alpha_0^2+6\\alpha_0\\alpha_1 \\frac{\\alpha_0}{1-\\alpha_1} \\\\\n&= \\frac{3\\alpha_0^2-3\\alpha_0^2\\alpha_1+6\\alpha_0^2\\alpha_1}{1-\\alpha_1} \\\\\n&= 3\\alpha_0^2\\frac{1+\\alpha_1}{1-\\alpha_1}\n\\end{aligned}\n\\]\nThus, we have:\n\\[\n\\mu_4 = 3\\frac{\\alpha_0^2}{(1-3\\alpha_1^2)}\\frac{1+\\alpha_1}{1-\\alpha_1}\n\\]\nSo kurtosis is given by:\n\\[\nK = \\frac{\\mu_4}{(\\sigma_t^2)^2} = \\frac{3\\frac{\\alpha_0^2}{(1-3\\alpha_1^2)}\\frac{1+\\alpha_1}{1-\\alpha_1}}{\\frac{\\alpha_0^2}{(1 - \\alpha_1)^2}} = 3\\frac{1-\\alpha_1^2}{1-3\\alpha_1^2}\n\\]\nExcess kurtosis is \\[\nK-K_{normal}=K-3=\\frac{6\\alpha_1^2}{1-3\\alpha_1^2}&gt;0.\n\\] The ARCH(1) model exhibits excess kurtosis, indicating that it has heavier tails than a normal distribution.\nThis is an important property for financial time series, as it suggests that extreme events (e.g., market crashes) are more likely than what a normal distribution would predict.\nSo in summary for \\(Y_t\\sim ARCH(1)\\), we have:\n\nExpectation: \\(EY_t = 0\\).\nVariance: \\(Var(Y_t) = \\frac{\\alpha_0}{1 - \\alpha_1}\\).\nACF \\(\\rho_k=0\\) for \\(k&gt;0\\).\n\\(Y_t\\) is leptokurtic, i.e. has heavier tails than normal.\n\\(Y_t\\) is symmetric (question: why?).\n\nIn other words, \\(Y_t\\) is a symmetric heavy-tailed white noise process! But it is not an independent process!\n\n\n\nWe can think of \\(Y_t^2\\) as approximately following AR(1) in the following sense.\nLet’s write the following equations and take their difference: \\[\n\\begin{aligned}\nY_t^2 &= \\sigma_t^2 e_t^2\\\\\n\\alpha_0 + \\alpha_1 Y_{t-1}^2 &= \\sigma_{t}^2.\n\\end{aligned}\n\\]\nWe get\n\\[\nY_t^2 - \\alpha_0 - \\alpha_1 Y_{t-1}^2 = \\sigma_t^2(e_t^2-1)=:\\eta_t\n\\]\nNote, that on the RHS we have \\(e_t^2-1\\), a centered \\(\\chi^2_1\\) noise term, scaled by \\(\\sigma_t^2\\). Thus we have\n\\[\nY_t^2 = \\alpha_0 + \\alpha_1 Y_{t-1}^2 + \\eta_t\n\\]\n\\(Y_t^2\\) follows AR(1) process with the non-normal, mean zero innovation process \\(\\eta_t\\), which is uncorrelated but dependant.\n\n\n\nIf \\(Y_t\\sim WN\\) and \\(Y_t^2\\sim AR(1)\\), then \\(Y_t\\sim ARCH(1)\\). We will refer to this behavior (i.e. uncorrelated but dependent) as ARCH effects.\nWe will talk about fitting the ARCH/GARCH models below.\n\n\n\n\nWe can generalize the ARCH(1) model to ARCH(p) model as follows:\n\\[\nY_t = \\sigma_t e_t, \\quad \\sigma_t^2 = \\alpha_0 + \\sum_{i=1}^{p} \\alpha_i Y_{t-i}^2.\n\\]\nLarger values of \\(Y_t\\) in the last \\(p\\) lags will lead to larger values of \\(\\sigma_t^2\\), thus potentially large \\(Y_t\\).\nOne can show that ARCH(p) is stationary if the sum of the coefficients is less than 1: \\[\n\\sum_{i=0}^{p} \\alpha_i &lt; 1.\n\\]\n\n\n\nGeneralized Autoregressive Conditional Heteroskedasticity (GARCH) models extend ARCH models by including lagged values of the conditional variance itself. The GARCH(p,q) model is defined as:\n\\[\nY_t = \\sigma_t e_t, \\quad \\sigma_t^2 = \\alpha_0 + \\sum_{i=1}^{p} \\alpha_i Y_{t-i}^2 + \\sum_{j=1}^{q} \\beta_j \\sigma_{t-j}^2.\n\\]\nLarger values of \\(Y_t\\) in the last \\(p\\) lags and/or larger values of \\(\\sigma_t^2\\) in the last \\(q\\) lags will lead to larger values of \\(\\sigma_t^2\\), thus potentially large \\(Y_t\\).\n\n\nIt can be shown that GARCH(p,q) process is stationary if \\[\n\\sum_{i=1}^{p} \\alpha_i + \\sum_{j=1}^{q} \\beta_j &lt; 1.\n\\] Note, that all \\(\\alpha_i, \\beta_j &gt; 0\\).\nSimilarly to the previous analysis, one can show that \\(Y_t\\sim GARCH(p,q)\\) is white noise, mean zero, and \\(Y_t^2\\sim ARMA(p,q)\\) with non-normal errors:\n\\[\nY_t^2=\\alpha_0 + \\sum_{i=1}^{max(p,q)} (\\alpha_i+\\beta_i) Y_{t-i}^2 +\\eta_t- \\sum_{j=1}^{q} \\beta_j \\eta_{t-j}.\n\\]",
    "crumbs": [
      "Lecture 23"
    ]
  },
  {
    "objectID": "TSA-Lecture23.html#arch1",
    "href": "TSA-Lecture23.html#arch1",
    "title": "25 Spring 439/639 TSA: Lecture 23",
    "section": "",
    "text": "ARCH(1) model, on the other hand, models conditional variance:\n\\[\n\\begin{cases}\nY_t = \\mu_{0} + \\sigma_{t|t-1} e_t, \\quad e_t\\sim iid(0,1) \\\\\n\\sigma_{t|t-1}^2 = \\alpha_0 + \\alpha_1 Y_{t-1}^2\n\\end{cases}\n\\]\nThis is conditionally heteroskedastic model, as the variance of the error term \\(e_t\\) is not constant but depends on the past values of the series. Note, we need \\(\\alpha_0 &gt; 0\\) and \\(\\alpha_1 &gt; 0\\) for the model to be valid. These models were developed by Robert Engle (ARCH, 1982) and Tim Bollerslev (GARCH, 1986). ARCH models are often used in financial time series analysis, particularly for modeling return volatility.\n\n\nIt can be shown that ARCH(1) process is stationary if \\(0&lt;\\alpha_1 &lt; 1\\). It may seem contradictory that a stationary process can have non-constant conditional variance. But, recall that weakly stationary processes has constant unconditional variance.\n\n\n\nConsider \\(X_t\\) the price of a stock/asset. The return can be defined as:\n\\[\nY_t = \\nabla \\log(X_t)\\approx \\frac{X_t - X_{t-1}}{X_{t-1}} = \\frac{X_t}{X_{t-1}} - 1\n\\] Standard deviation of \\(Y_t\\) is known as volatility.\nFor simplicity assume \\(\\mu_0=0\\), then ARCH(1) process becomes:\n\\[\n\\begin{cases}\nY_t = \\sigma_{t} e_t, \\quad e_t\\sim iid(0,1) \\\\\n\\sigma_{t}^2 = \\alpha_0 + \\alpha_1 Y_{t-1}^2\n\\end{cases}\n\\]\nIf the previous time series value \\(Y_{t-1}\\) is large, then volatility \\(\\sigma_{t}\\) will also be large, indicating a period of high uncertainty or risk in the asset’s return: \\(Y_t\\) may potentially be quite large as well.\n\n\n\n\nExpectation \\[\nEY_t=E[\\sigma_{t} e_t]=E[\\sigma_{t}]E[e_t]=0.\n\\]\nVariance \\[\nVar(Y_t) = E(Y_t^2) = E[\\sigma_{t}^2e_t^2] = E[\\sigma_{t}^2]E[e_t^2] = E[\\sigma_{t}^2]\n\\] \\[\n=\\alpha_0 + \\alpha_1 E[Y_{t-1}^2]=\\alpha_0 + \\alpha_1 Var(Y_t).\n\\]\n\nIn the last step we used the fact that ARCH(1) is stationary, so \\(E[Y_t^2] = Var(Y_t)\\). Solving for \\(Var(Y_t)\\) gives:\n\\[\nVar(Y_t) = \\frac{\\alpha_0}{1 - \\alpha_1}.\n\\]\nThus it is necessary to have \\(\\alpha_1 &lt; 1\\) for the model to be stable and for the variance to be finite.\nNote, the conditional variance will not be constant:\n\\[\nVar(Y_t|Y_{t-1}) = \\alpha_0 + \\alpha_1 Y_{t-1}^2.\n\\]\n\nDependence\n\n\\(Y_t\\) and \\(Y_{t-h}\\) are uncorrelated for \\(h &gt; 0\\), but dependent.\n\\[\ncov(Y_t,Y_{t-1}) = E[Y_t Y_{t-1}] - E[Y_t]E[Y_{t-1}] = E[Y_t Y_{t-1}]=E[\\sigma_{t} e_t \\sigma_{t-1} e_{t-1}] = E[\\sigma_{t} \\sigma_{t-1} e_{t-1}] E[e_t] = 0.\n\\] Above we used the fact that \\(e_t\\) is independent of the past, i.e. of \\(\\sigma_{t}, \\sigma_{t-1}, e_{t-1}\\).\n\n\\(Y_t\\) is leptokurtic, i.e. has heavier tails than normal.\n\nRecall, kurtosis is defined as \\(K=\\frac{\\mu_4}{\\sigma_t^2}=\\frac{EY_t^4}{(Var Y_t)^2}\\).\n\\[\n\\mu_4=E[Y_t^4]=E[\\sigma_t^4 e_t^4]=E[\\sigma_t^4]E[e_t^4]= 3E[\\sigma_t^4].\n\\]\n\\[\n=3E[(\\alpha_0+\\alpha_1 Y_{t-1}^2)^2]=3(\\alpha_0^2+\\alpha_1^2 E[Y_{t-1}^4]+2\\alpha_0\\alpha_1 E[Y_{t-1}^2]).\n\\]\nNote that \\(E[Y_{t-1}^2] = Var(Y_t)\\) and \\(E[Y_{t-1}^4] = \\mu_4\\) by stationarity, so \\[\n\\begin{aligned}\n\\mu_4(1-3\\alpha_1^2) &= 3\\alpha_0^2+6\\alpha_0\\alpha_1 \\frac{\\alpha_0}{1-\\alpha_1} \\\\\n&= \\frac{3\\alpha_0^2-3\\alpha_0^2\\alpha_1+6\\alpha_0^2\\alpha_1}{1-\\alpha_1} \\\\\n&= 3\\alpha_0^2\\frac{1+\\alpha_1}{1-\\alpha_1}\n\\end{aligned}\n\\]\nThus, we have:\n\\[\n\\mu_4 = 3\\frac{\\alpha_0^2}{(1-3\\alpha_1^2)}\\frac{1+\\alpha_1}{1-\\alpha_1}\n\\]\nSo kurtosis is given by:\n\\[\nK = \\frac{\\mu_4}{(\\sigma_t^2)^2} = \\frac{3\\frac{\\alpha_0^2}{(1-3\\alpha_1^2)}\\frac{1+\\alpha_1}{1-\\alpha_1}}{\\frac{\\alpha_0^2}{(1 - \\alpha_1)^2}} = 3\\frac{1-\\alpha_1^2}{1-3\\alpha_1^2}\n\\]\nExcess kurtosis is \\[\nK-K_{normal}=K-3=\\frac{6\\alpha_1^2}{1-3\\alpha_1^2}&gt;0.\n\\] The ARCH(1) model exhibits excess kurtosis, indicating that it has heavier tails than a normal distribution.\nThis is an important property for financial time series, as it suggests that extreme events (e.g., market crashes) are more likely than what a normal distribution would predict.\nSo in summary for \\(Y_t\\sim ARCH(1)\\), we have:\n\nExpectation: \\(EY_t = 0\\).\nVariance: \\(Var(Y_t) = \\frac{\\alpha_0}{1 - \\alpha_1}\\).\nACF \\(\\rho_k=0\\) for \\(k&gt;0\\).\n\\(Y_t\\) is leptokurtic, i.e. has heavier tails than normal.\n\\(Y_t\\) is symmetric (question: why?).\n\nIn other words, \\(Y_t\\) is a symmetric heavy-tailed white noise process! But it is not an independent process!\n\n\n\nWe can think of \\(Y_t^2\\) as approximately following AR(1) in the following sense.\nLet’s write the following equations and take their difference: \\[\n\\begin{aligned}\nY_t^2 &= \\sigma_t^2 e_t^2\\\\\n\\alpha_0 + \\alpha_1 Y_{t-1}^2 &= \\sigma_{t}^2.\n\\end{aligned}\n\\]\nWe get\n\\[\nY_t^2 - \\alpha_0 - \\alpha_1 Y_{t-1}^2 = \\sigma_t^2(e_t^2-1)=:\\eta_t\n\\]\nNote, that on the RHS we have \\(e_t^2-1\\), a centered \\(\\chi^2_1\\) noise term, scaled by \\(\\sigma_t^2\\). Thus we have\n\\[\nY_t^2 = \\alpha_0 + \\alpha_1 Y_{t-1}^2 + \\eta_t\n\\]\n\\(Y_t^2\\) follows AR(1) process with the non-normal, mean zero innovation process \\(\\eta_t\\), which is uncorrelated but dependant.\n\n\n\nIf \\(Y_t\\sim WN\\) and \\(Y_t^2\\sim AR(1)\\), then \\(Y_t\\sim ARCH(1)\\). We will refer to this behavior (i.e. uncorrelated but dependent) as ARCH effects.\nWe will talk about fitting the ARCH/GARCH models below.",
    "crumbs": [
      "Lecture 23"
    ]
  },
  {
    "objectID": "TSA-Lecture23.html#archp",
    "href": "TSA-Lecture23.html#archp",
    "title": "25 Spring 439/639 TSA: Lecture 23",
    "section": "",
    "text": "We can generalize the ARCH(1) model to ARCH(p) model as follows:\n\\[\nY_t = \\sigma_t e_t, \\quad \\sigma_t^2 = \\alpha_0 + \\sum_{i=1}^{p} \\alpha_i Y_{t-i}^2.\n\\]\nLarger values of \\(Y_t\\) in the last \\(p\\) lags will lead to larger values of \\(\\sigma_t^2\\), thus potentially large \\(Y_t\\).\nOne can show that ARCH(p) is stationary if the sum of the coefficients is less than 1: \\[\n\\sum_{i=0}^{p} \\alpha_i &lt; 1.\n\\]",
    "crumbs": [
      "Lecture 23"
    ]
  },
  {
    "objectID": "TSA-Lecture23.html#garchpq",
    "href": "TSA-Lecture23.html#garchpq",
    "title": "25 Spring 439/639 TSA: Lecture 23",
    "section": "",
    "text": "Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models extend ARCH models by including lagged values of the conditional variance itself. The GARCH(p,q) model is defined as:\n\\[\nY_t = \\sigma_t e_t, \\quad \\sigma_t^2 = \\alpha_0 + \\sum_{i=1}^{p} \\alpha_i Y_{t-i}^2 + \\sum_{j=1}^{q} \\beta_j \\sigma_{t-j}^2.\n\\]\nLarger values of \\(Y_t\\) in the last \\(p\\) lags and/or larger values of \\(\\sigma_t^2\\) in the last \\(q\\) lags will lead to larger values of \\(\\sigma_t^2\\), thus potentially large \\(Y_t\\).\n\n\nIt can be shown that GARCH(p,q) process is stationary if \\[\n\\sum_{i=1}^{p} \\alpha_i + \\sum_{j=1}^{q} \\beta_j &lt; 1.\n\\] Note, that all \\(\\alpha_i, \\beta_j &gt; 0\\).\nSimilarly to the previous analysis, one can show that \\(Y_t\\sim GARCH(p,q)\\) is white noise, mean zero, and \\(Y_t^2\\sim ARMA(p,q)\\) with non-normal errors:\n\\[\nY_t^2=\\alpha_0 + \\sum_{i=1}^{max(p,q)} (\\alpha_i+\\beta_i) Y_{t-i}^2 +\\eta_t- \\sum_{j=1}^{q} \\beta_j \\eta_{t-j}.\n\\]",
    "crumbs": [
      "Lecture 23"
    ]
  },
  {
    "objectID": "TSA-Lecture23.html#caveats",
    "href": "TSA-Lecture23.html#caveats",
    "title": "25 Spring 439/639 TSA: Lecture 23",
    "section": "2.1 Caveats",
    "text": "2.1 Caveats\n\nIn general, \\(p_0,q_0\\leq 2\\).\n\\(Y_t^2\\) rarely follows a simple ARMA structure.\nSometimes \\(|Y_t|\\) exhibits a better behavior, close to the predicted ARMA structure.\nIn reality one would fit a low dimensional GARCH model to \\(Y_t\\) and then would check the residuals for GARCH effects.",
    "crumbs": [
      "Lecture 23"
    ]
  },
  {
    "objectID": "TSA-Lecture20-sarima.html",
    "href": "TSA-Lecture20-sarima.html",
    "title": "439/639: SARIMA = Seasonal ARIMA",
    "section": "",
    "text": "This notebook is based on Chapter 10 of Cryer and Chan, “Time Series Analysis: With Applications in R”.",
    "crumbs": [
      "R notebook: SARIMA"
    ]
  },
  {
    "objectID": "TSA-Lecture20-sarima.html#model-specification",
    "href": "TSA-Lecture20-sarima.html#model-specification",
    "title": "439/639: SARIMA = Seasonal ARIMA",
    "section": "1.1 Model specification",
    "text": "1.1 Model specification\nThe figure below displays the monthly carbon dioxide levels at Alert, NWT, Canada. The data is available in the TSA package in R. The data is plotted from January 1994 to December 2004. The points are colored according to the month of the year.\nSeasonality is more noticable in the second plot, where we colored and labeled the points according to the month of the year.\n\n\nCode\nlibrary(TSA)\ndata(co2)\nplot(co2,ylab='CO2', \n    main='Monthly Carbon Dioxide Levels at Alert, NWT, Canada')\n\n\n\n\n\n\n\n\n\nCode\n# The points are colored according to the month of the year\n\n# Define colors for each month (12 distinct colors)\nmonth_colors &lt;- c(\"red\", \"orange\", \"yellow\", \"green\", \"cyan\", \n                  \"blue\", \"purple\", \"magenta\", \"brown\", \"gray\", \n                  \"black\", \"pink\")\nplot(window(co2,start=c(2000,1)),\n    ylab='CO2', \n    main ='Monthly Carbon Dioxide Levels at Alert, NWT, Canada')\nMonth=c('J','F','M','A','M','J','J','A','S','O','N','D')\npoints(window(co2,start=c(2000,1)),\n    pch=Month, col = month_colors[cycle(co2)])\n\n\n\n\n\n\n\n\n\nCode\nacf(as.vector(co2),lag.max=36, \n    main = 'Sample ACF of Yt',\n    xaxp=c(0,36,18))\n\n\n\n\n\n\n\n\n\nThe sample autocorrelation shows clear signs of seasonality.\nWe see a noticeable linear trend in the \\(Y_t\\) plot, indicating that the data is not stationary. We difference once, \\(\\nabla Y_t\\), and plot the differenced series as well as the corresponding sample ACF.\n\n\nCode\nplot(diff(co2),\n     ylab='First Difference of CO2',\n     xlab='Time',\n     main = expression(nabla * Y[t]))\n\n\n\n\n\n\n\n\n\nCode\nacf(as.vector(diff(co2)),\n    lag.max=36,\n    ci.type='ma',\n    xaxp=c(0,36,18),\n    main = expression(\"Sample ACF of \" * nabla * Y[t]))\n\n\n\n\n\n\n\n\n\nThe general upward trend disappears after the first difference. The ACF of the first difference shows a clear seasonal pattern, with peaks at lags 12, 24, and 36. Perhaps seasonal differencing will bring us to a series that may be modeled parsimoniously. We difference it again at lag 12.\nNow we are plotting \\(\\nabla_{12}\\nabla Y_t\\) and the corresponding ACF.\n\n\nCode\nplot(diff(diff(co2),lag=12), xlab='Time',\n    ylab='First and Seasonal Difference of CO2', \n    main = expression( nabla[12] * nabla * Y[t]))\n\n\n\n\n\n\n\n\n\nCode\nacf(as.vector(diff(diff(co2),lag=12)),lag.max=36,\n    ci.type='ma',xaxp=c(0,36,18), \n    main = expression(\"Sample ACF of \" * nabla[12] * nabla * Y[t]))\n\n\n\n\n\n\n\n\n\nCode\npacf(as.vector(diff(diff(co2),lag=12)),lag.max=36,\n    xaxp=c(0,36,18), \n    main = expression(\"Sample PACF of \" * nabla[12] * nabla * Y[t]))\n\n\n\n\n\n\n\n\n\nWe see significant spikes at lags 1, 12 and 13. We can model this parsimoniously as \\(MA(1)\\times MA(1)_{12}\\). So the original series follows ARIMA\\((0,1,1)\\times (0,1,1)_{12}\\), i.e.\n\\[\n\\nabla_{12}\\nabla Y_t = e_t-\\theta e_{t-1} - \\Theta e_{t-12} + \\theta\\Theta e_{t-13}.\n\\]",
    "crumbs": [
      "R notebook: SARIMA"
    ]
  },
  {
    "objectID": "TSA-Lecture20-sarima.html#fitting-and-diagnosing-the-model",
    "href": "TSA-Lecture20-sarima.html#fitting-and-diagnosing-the-model",
    "title": "439/639: SARIMA = Seasonal ARIMA",
    "section": "1.2 Fitting and Diagnosing the Model",
    "text": "1.2 Fitting and Diagnosing the Model\nFitting the model we get the following estimates of the parameters:\n\n\nCode\nm1.co2 &lt;- arima(co2,order=c(0,1,1),\n                seasonal=list(order=c(0,1,1),\n                period=12))\nm1.co2\n\n\n\nCall:\narima(x = co2, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 12))\n\nCoefficients:\n          ma1     sma1\n      -0.5792  -0.8206\ns.e.   0.0791   0.1137\n\nsigma^2 estimated as 0.5446:  log likelihood = -139.54,  aic = 283.08\n\n\nThe coefficients are highly significant. We check the residuals of the model. Other than some strange behavior in the middle of the series, this plot does not suggest any major irregularities with the model\n\n\nCode\nplot(window(rstandard(m1.co2),start=c(1995,2)),\n    ylab='Standardized Residuals',\n    type='o', \n         main = expression(\"Residuals from the ARIMA\" ~\n                       \"(\" * 0 * \",\" * 1 * \",\" * 1 * \")\" ~\n                       \"\\u00d7\" ~\n                       \"(\" * 0 * \",\" * 1 * \",\" * 1 * \")\"[12] ~\n                       \"Model\"))\nabline(h=0)\n\n\n\n\n\n\n\n\n\nTo diagnose further, we plot the sample ACF of the residuals. The only ``statistically significant’’ correlation is at lag 22, and this correlation has a value of only −0.17, a very small correlation. Furthermore, we can’t think of any reasonable interpretation for dependence at lag 22. Finally, we should not be surprised that one autocorrelation out of the 36 displayed is statistically significant. This could easily happen by chance alone. Except for marginal significance at lag 22, the model seems to have captured the essence of the dependence in the series. The Ljing-Box test for the residuals confirms that the residuals are uncorrelated.\n\n\nCode\nacf(as.vector(window(rstandard(m1.co2),start=c(1995,2))),\nlag.max=36, main = 'Sample ACF of Residuals',\nxaxp=c(0,36,18))\n\n\n\n\n\n\n\n\n\nCode\nBox.test(window(rstandard(m1.co2),start=c(1995,2)),lag=24,type='Ljung-Box')\n\n\n\n    Box-Ljung test\n\ndata:  window(rstandard(m1.co2), start = c(1995, 2))\nX-squared = 25.587, df = 24, p-value = 0.3745\n\n\nThe histogram, the QQ-plot and the Shapiro-Wilk test for normality of the residuals show that the residuals are nornally distributed.\n\n\nCode\nhist(window(rstandard(m1.co2),start=c(1995,2)),\nxlab='Standardized Residuals', main = 'Histogram of Residuals')\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(window(rstandard(m1.co2),start=c(1995,2)), main = 'QQ-Plot of Residuals')\nqqline(window(rstandard(m1.co2),start=c(1995,2)))\n\n\n\n\n\n\n\n\n\nCode\nshapiro.test(window(rstandard(m1.co2),start=c(1995,2)))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  window(rstandard(m1.co2), start = c(1995, 2))\nW = 0.98205, p-value = 0.1134\n\n\nAs one additional check, we overfit the model with ARIMA\\((0,1,2)\\times(0,1,1)_{12}\\).\n\n\nCode\nm2.co2 &lt;- arima(co2,order=c(0,1,2),seasonal=list(order=c(0,1,1),\nperiod=12))\nm2.co2\n\n\n\nCall:\narima(x = co2, order = c(0, 1, 2), seasonal = list(order = c(0, 1, 1), period = 12))\n\nCoefficients:\n          ma1      ma2     sma1\n      -0.5714  -0.0165  -0.8274\ns.e.   0.0897   0.0948   0.1224\n\nsigma^2 estimated as 0.5427:  log likelihood = -139.52,  aic = 285.05\n\n\nThe AIC increased and the new estimated term \\(\\theta_2\\) is not significant, while the other estimated terms have not changed much.\nThe ARIMA\\((0,1,1)\\times(0,1,1)_{12}\\) model was popularized in the first edition of the seminal book of Box and Jenkins (1976) when it was found to characterize the logarithms of a monthly airline passenger time series. This model has come to be known as the airline model.",
    "crumbs": [
      "R notebook: SARIMA"
    ]
  },
  {
    "objectID": "TSA-Lecture20-sarima.html#forecasting",
    "href": "TSA-Lecture20-sarima.html#forecasting",
    "title": "439/639: SARIMA = Seasonal ARIMA",
    "section": "1.3 Forecasting",
    "text": "1.3 Forecasting\nThe first plot shows the forecasts and 95% forecast limits for a lead time of two years for the ARIMA\\((0,1,1)\\times (0,1,1)_{12}\\) model that we fit. The last two years of observed data are also shown. The forecasts mimic the stochastic periodicity in the data quite well, and the forecast limits give a good feeling for the precision.\nThe second plot displays the last year of observed data and forecasts out four years. At this lead time, it is easy to see that the forecast limits are getting wider, as there is more uncertainty in the forecasts.\n\n\nCode\nplot(m1.co2,n1=c(2003,1),n.ahead=24,\n    xlab='Year',type='o',ylab='CO2 Levels', \n    main = 'Forecasts from the ARIMA(0,1,1)×(0,1,1)12 Model, Lead Time=2 Years')\n\n\n\n\n\n\n\n\n\nCode\nplot(m1.co2,n1=c(2004,1),n.ahead=48,\n    xlab='Year',type='b', ylab='CO2 Levels', \n    main = 'Forecasts from the ARIMA(0,1,1)×(0,1,1)12 Model, Lead Time=4 Years')",
    "crumbs": [
      "R notebook: SARIMA"
    ]
  },
  {
    "objectID": "TSA-Lecture20-sarima.html#summary",
    "href": "TSA-Lecture20-sarima.html#summary",
    "title": "439/639: SARIMA = Seasonal ARIMA",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nMultiplicative seasonal ARIMA models provide an efficient way to model time series whose seasonal tendencies are not as regular as we would have with a deterministic seasonal trend model which we covered earlier. Fortunately, these models are simply special ARIMA models so that no new theory is needed to investigate their properties. We illustrated the special nature of these models with a thorough modeling of an actual time series of CO2 data.",
    "crumbs": [
      "R notebook: SARIMA"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html",
    "href": "TSA-Lecture19-forecasting.html",
    "title": "Rnotebook: Forecasting",
    "section": "",
    "text": "Step 1: Model Specification\nStep 2: Parameter Estimation\nStep 3: Model Checking\nStep 4: Forecasting",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#steps-in-time-series-analysis",
    "href": "TSA-Lecture19-forecasting.html#steps-in-time-series-analysis",
    "title": "Rnotebook: Forecasting",
    "section": "",
    "text": "Step 1: Model Specification\nStep 2: Parameter Estimation\nStep 3: Model Checking\nStep 4: Forecasting",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#forecasting-ar1",
    "href": "TSA-Lecture19-forecasting.html#forecasting-ar1",
    "title": "Rnotebook: Forecasting",
    "section": "1.1 Forecasting AR(1)",
    "text": "1.1 Forecasting AR(1)\n\n\nCode\n# Example of Forecasting AR(1) Model\ndata(ar1.s)\nm1.ar1=arima(ar1.s,order=c(1,0,0))\nplot(m1.ar1,n.ahead=12,type='b',\n        xlab='Time',ylab='AR(1)', \n        main = 'Forecasting AR(1) with 12 steps ahead, phi=0.9')\n# add the horizontal line at the estimated mean (\"intercept\")\nabline(h=coef(m1.ar1)[names(coef(m1.ar1))=='intercept'])",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#forecasting-ar2",
    "href": "TSA-Lecture19-forecasting.html#forecasting-ar2",
    "title": "Rnotebook: Forecasting",
    "section": "1.2 Forecasting AR(2)",
    "text": "1.2 Forecasting AR(2)\n\n\nCode\n# Example of Forecasting AR(2) Model\ndata(ar2.s)\nm1.ar2=arima(ar2.s,order=c(2,0,0))\nplot(m1.ar2,n.ahead=12,type='b',\n        xlab='Time',ylab='AR(2)', \n        main = 'Forecasting AR(2) with h=12 , phi1=1.5, phi2=-0.75')\n# add the horizontal line at the estimated mean (\"intercept\")\nabline(h=coef(m1.ar2)[names(coef(m1.ar2))=='intercept'])",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#forecasting-ma2",
    "href": "TSA-Lecture19-forecasting.html#forecasting-ma2",
    "title": "Rnotebook: Forecasting",
    "section": "1.3 Forecasting MA(2)",
    "text": "1.3 Forecasting MA(2)\n\n\nCode\n# Example of Forecasting MA(2) Model\ndata(ma2.s)\nm1.ma2=arima(ma2.s,order=c(0,0,2))\nplot(m1.ma2,n.ahead=12,type='b',\n        xlab='Time',ylab='MA(2)', \n        main = 'Forecasting MA(2), h=12, theta1=1, theta2=-0.6')\n# add the horizontal line at the estimated mean (\"intercept\")\nabline(h=coef(m1.ma2)[names(coef(m1.ma2))=='intercept'])",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#forecasting-arma11",
    "href": "TSA-Lecture19-forecasting.html#forecasting-arma11",
    "title": "Rnotebook: Forecasting",
    "section": "1.4 Forecasting ARMA(1,1)",
    "text": "1.4 Forecasting ARMA(1,1)\n\n\nCode\n# Example of Forecasting ARMA(1,1) Model\ndata(arma11.s)\nm1.arma11=arima(arma11.s,order=c(1,0,1))\nplot(m1.arma11,n.ahead=12,type='b',\n        xlab='Time',ylab='ARMA(1,1)',\n        main = 'Forecasting ARMA(1,1), h=12, phi=0.6, theta=-0.3')\nabline(h=coef(m1.arma11)[names(coef(m1.arma11))=='intercept'])",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#forecasting-random-walk-with-drift",
    "href": "TSA-Lecture19-forecasting.html#forecasting-random-walk-with-drift",
    "title": "Rnotebook: Forecasting",
    "section": "1.5 Forecasting Random Walk with Drift",
    "text": "1.5 Forecasting Random Walk with Drift\nRecall, random walk with a drift is defined as \\[Y_t = Y_{t-1} + \\delta + e_t\\], where \\(e_t\\) is a white noise error term and \\(\\delta\\) is the drift parameter. The forecast for \\(\\hat{Y}_{t}(h)\\) is given by \\(\\hat{Y}_t(h) = Y_t + h\\,\\delta\\).",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#temperature-data",
    "href": "TSA-Lecture19-forecasting.html#temperature-data",
    "title": "Rnotebook: Forecasting",
    "section": "2.1 Temperature Data",
    "text": "2.1 Temperature Data\n\n\nCode\n# Exhibit 9.2\n# append 2 years of missing values to the tempdub data as we want to forecast\n# the temperature for two years.\ndata(tempdub)\n\ntempdub1=ts(c(tempdub,rep(NA,24)),\n                start=start(tempdub),\n                freq=frequency(tempdub)) \n\n# creates the first pair of harmonic functions and then fit the model\nhar.=harmonic(tempdub,1)\nm5.tempdub=arima(tempdub,order=c(0,0,0),xreg=har.)\nm5.tempdub\n\n\n\nCall:\narima(x = tempdub, order = c(0, 0, 0), xreg = har.)\n\nCoefficients:\n      intercept  cos(2*pi*t)  sin(2*pi*t)\n        46.2660     -26.7079      -2.1697\ns.e.     0.3056       0.4322       0.4322\n\nsigma^2 estimated as 13.45:  log likelihood = -391.44,  aic = 788.88\n\n\nCode\n# create the harmonic functions over the period of forecast.\nnewhar.=harmonic(ts(rep(1,24), start=c(1976,1),freq=12),1)\n# Compute and plot the forecasts.\nplot(m5.tempdub,n.ahead=24,\n        n1=c(1972,1),newxreg=newhar.,\n        type='b',ylab='Temperature',xlab='Year')",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#color-property-data",
    "href": "TSA-Lecture19-forecasting.html#color-property-data",
    "title": "Rnotebook: Forecasting",
    "section": "3.1 Color Property Data",
    "text": "3.1 Color Property Data\nAR(1) model was fitted to the color property data. The forecast is based on the fitted AR(1) model and can be used to predict future values of the time series.\n\n\nCode\ndata(color)\nm1.color=arima(color,order=c(1,0,0))\n\n# Exhibit 9.3 \ndata(color)\nm1.color=arima(color,order=c(1,0,0))\nplot(m1.color,n.ahead=12,type='b', xlab='Time', ylab='Color Property')\n# add the horizontal line at the estimated mean (\"intercept\") \nabline(h=coef(m1.color)[names(coef(m1.color))=='intercept'])",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture19-forecasting.html#hare-data",
    "href": "TSA-Lecture19-forecasting.html#hare-data",
    "title": "Rnotebook: Forecasting",
    "section": "3.2 Hare Data",
    "text": "3.2 Hare Data\nThe Canadian hare abundance series was fitted by working with the square root of the abundance numbers and then fitting an AR(3) model. Notice how the forecasts mimic the approximate cycle in the actual series even when we forecast with a lead time out to 25 years.\n\n\nCode\n# Exhibit 9.4\ndata(hare)\n# fixed the AR(2) coefficient to be 0 via the fixed argument.\nm1.hare=arima(sqrt(hare),order=c(3,0,0), fixed=c(NA,0,NA,NA)) \nplot(m1.hare, n.ahead=25,type='b',xlab='Year',ylab='Sqrt(hare)')\nabline(h=coef(m1.hare)[names(coef(m1.hare))=='intercept'])",
    "crumbs": [
      "R notebook: Forecasting"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html",
    "href": "TSA-Lecture14-model-specification.html",
    "title": "R notebook: Model Specification",
    "section": "",
    "text": "Step 1: Model Specification\nStep 2: Parameter Estimation\nStep 3: Model Checking\nStep 4: Forecasting",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#steps-in-time-series-analysis",
    "href": "TSA-Lecture14-model-specification.html#steps-in-time-series-analysis",
    "title": "R notebook: Model Specification",
    "section": "",
    "text": "Step 1: Model Specification\nStep 2: Parameter Estimation\nStep 3: Model Checking\nStep 4: Forecasting",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#specification-of-simulated-maq-processes",
    "href": "TSA-Lecture14-model-specification.html#specification-of-simulated-maq-processes",
    "title": "R notebook: Model Specification",
    "section": "1.1 Specification of simulated MA(q) processes",
    "text": "1.1 Specification of simulated MA(q) processes\n\n\nCode\n# Load the data\ndata(ma1.1.s)\n# ACF of the series\nacf(ma1.1.s,\n    xaxp=c(0,20,10),\n    main = \"ACF of MA(1) with theta=0.9\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nacf(ma1.1.s,\n    ci.type='ma',\n    xaxp=c(0,20,10),\n    main = \"Alternative bounds for the ACF of MA(1) with theta=0.9\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata(ma1.2.s)\nacf(ma1.2.s,\n    xaxp=c(0,20,10),\n    main = \"ACF of MA(1) with theta=-0.9\")\n\n\n\n\n\n\n\n\n\nCode\ndata(ma2.s)\nacf(ma2.s,\n    xaxp=c(0,20,10),\n    main = \"ACF of MA(2) with theta1=1, theta2=-0.6\")\n\n\n\n\n\n\n\n\n\nCode\nacf(ma2.s,\n    ci.type='ma',\n    xaxp=c(0,20,10),\n    main = \"Alternative bounds for the ACF of MA(2) with theta1=1, theta2=-0.6\")",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#specification-of-the-simulated-arp-processes",
    "href": "TSA-Lecture14-model-specification.html#specification-of-the-simulated-arp-processes",
    "title": "R notebook: Model Specification",
    "section": "1.2 Specification of the simulated AR(p) processes",
    "text": "1.2 Specification of the simulated AR(p) processes\n\n\nCode\ndata(ar1.s) \nacf(ar1.s,\n    xaxp=c(0,20,10),\n    main = \"ACF of AR(1) with phi=0.9\") \n\n\n\n\n\n\n\n\n\nCode\npacf(ar1.s,xaxp=c(0,20,10),\n      main = \"PACF of AR(1) with phi=0.9\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata(ar2.s)\nacf(ar2.s,\n    xaxp=c(0,20,10),\n    main = \"ACF of AR(2) with phi1=1.5, phi2=-0.75\")\n\n\n\n\n\n\n\n\n\nCode\npacf(ar2.s,\n     xaxp=c(0,20,10),\n     main = \"PACF of AR(2) with phi1=1.5, phi2=-0.75\")",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#specification-of-the-simulated-armapq-processes",
    "href": "TSA-Lecture14-model-specification.html#specification-of-the-simulated-armapq-processes",
    "title": "R notebook: Model Specification",
    "section": "1.3 Specification of the simulated ARMA(p,q) processes",
    "text": "1.3 Specification of the simulated ARMA(p,q) processes\n\n\nCode\ndata(arma11.s)\nplot(arma11.s, type='o',ylab=expression(Y[t]),\n    main = \"ARMA(1,1) with phi=0.6, theta=-0.3\")\n\n\n\n\n\n\n\n\n\nCode\nacf(arma11.s,xaxp=c(0,20,10),\n    main = \"ACF of ARMA(1,1) with phi=0.6, theta=-0.3\")\n\n\n\n\n\n\n\n\n\nCode\npacf(arma11.s,xaxp=c(0,20,10),\n     main = \"PACF of ARMA(1,1) with phi=0.6, theta=-0.3\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# In an R Notebook, simply run this code chunk:\n\n# 1) Create an empty matrix filled with \"X\":\nn_ar &lt;- 8   # AR orders from 0 to 7\nn_ma &lt;- 14  # MA orders from 0 to 13\neacf_mat &lt;- matrix(\"X\", nrow = n_ar, ncol = n_ma)\n\n# 2) Fill with \"0\" below the main diagonal:\n#    For AR&gt;0, columns at or past that AR index become \"0\".\nfor (i in 2:n_ar) {                  # i=2 means AR=1, i=3 means AR=2, etc.\n  for (j in i:n_ma) {                # j runs from \"i\" through 14\n    eacf_mat[i, j] &lt;- \"0\"\n  }\n}\n\n# 3) Put the special \"0*\" mark (the star at AR=1, MA=0):\neacf_mat[2, 2] &lt;- \"0*\"\n\n# 4) Label rows and columns:\nrownames(eacf_mat) &lt;- paste0(\"AR=\", 0:(n_ar-1))\ncolnames(eacf_mat) &lt;- paste0(\"MA=\", 0:(n_ma-1))\n\n# 5) Display the table nicely (using knitr::kable or similar):\nlibrary(knitr)\nkable(eacf_mat, align = \"c\", caption = \"Theoretical EACF for ARMA(1,1)\")\n\n\n\nTheoretical EACF for ARMA(1,1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMA=0\nMA=1\nMA=2\nMA=3\nMA=4\nMA=5\nMA=6\nMA=7\nMA=8\nMA=9\nMA=10\nMA=11\nMA=12\nMA=13\n\n\n\n\nAR=0\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\n\n\nAR=1\nX\n0*\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nAR=2\nX\nX\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nAR=3\nX\nX\nX\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nAR=4\nX\nX\nX\nX\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nAR=5\nX\nX\nX\nX\nX\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nAR=6\nX\nX\nX\nX\nX\nX\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nAR=7\nX\nX\nX\nX\nX\nX\nX\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nCode\n# Compute EACF\neacf(arma11.s)\n\n\nAR/MA\n  0 1 2 3 4 5 6 7 8 9 10 11 12 13\n0 x x x x o o o o o o o  o  o  o \n1 x o o o o o o o o o o  o  o  o \n2 x o o o o o o o o o o  o  o  o \n3 x x o o o o o o o o o  o  o  o \n4 x o x o o o o o o o o  o  o  o \n5 x o o o o o o o o o o  o  o  o \n6 x o o o x o o o o o o  o  o  o \n7 x o o o x o o o o o o  o  o  o",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#nonstationarity",
    "href": "TSA-Lecture14-model-specification.html#nonstationarity",
    "title": "R notebook: Model Specification",
    "section": "1.4 Nonstationarity",
    "text": "1.4 Nonstationarity",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#overdifferencing",
    "href": "TSA-Lecture14-model-specification.html#overdifferencing",
    "title": "R notebook: Model Specification",
    "section": "1.5 Overdifferencing",
    "text": "1.5 Overdifferencing\n\n\nCode\n# Differencing the random walk twice\nz &lt;- rnorm(60)\nrw &lt;- ts(cumsum(z))\nplot(diff(diff(rw)))\n\n\n\n\n\n\n\n\n\nCode\n# ACF and PACF of the twice-differenced series\nacf(diff(diff(rw)), \n    xaxp=c(0,22,11),\n    ci.type='ma',\n    main=\"Sample ACF of the overdifferenced random walk\")\n\n\n\n\n\n\n\n\n\nCode\nacf(diff(rw), \n    xaxp=c(0,22,11),\n    main=\"Sample ACF of the correctly differenced random walk\")",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#detecting-nonstationarity-augmented-dickey-fuller-test",
    "href": "TSA-Lecture14-model-specification.html#detecting-nonstationarity-augmented-dickey-fuller-test",
    "title": "R notebook: Model Specification",
    "section": "1.6 Detecting nonstationarity: Augmented Dickey-Fuller test",
    "text": "1.6 Detecting nonstationarity: Augmented Dickey-Fuller test\n\n\nCode\n# Augmented Dickey-Fuller test\n# Unit root test of the random walk and its first difference\nlibrary(tseries)\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nCode\nadf.test(rw)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  rw\nDickey-Fuller = -1.5988, Lag order = 3, p-value = 0.737\nalternative hypothesis: stationary\n\n\nCode\nadf.test(diff(rw))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(rw)\nDickey-Fuller = -3.4097, Lag order = 3, p-value = 0.06298\nalternative hypothesis: stationary",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#model-selection-arma-subsets-based-on-bic",
    "href": "TSA-Lecture14-model-specification.html#model-selection-arma-subsets-based-on-bic",
    "title": "R notebook: Model Specification",
    "section": "1.7 Model Selection: ARMA Subsets based on BIC",
    "text": "1.7 Model Selection: ARMA Subsets based on BIC\nSchwarz Bayesian Information Criterion (BIC) \\[\nBIC = – 2\\log(maximum likelihood) + k\\log(n)\n\\] where \\(k\\) is the number of parameters in the model and \\(n\\) is the number of observations.\nMinimizing the BIC is equivalent to maximizing the likelihood of the model while penalizing for the number of parameters.\n\n\nCode\n# ARMA Subsets\nset.seed(92397)\nyt &lt;- arima.sim(model=list(ar=c(rep(0,11),0.8),\n                            ma=c(rep(0,11),0.7)),n=120)\nres &lt;- armasubsets(y=yt,nar=14,nma=14,\n                    y.name='yt',ar.method='ols')\nplot(res)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load the oil price data\ndata(oil.price)\n# Plot the oil price series\nacf(as.vector(diff(log(oil.price))),xaxp=c(0,22,11))\n\n\n\n\n\n\n\n\n\nCode\npacf(as.vector(diff(log(oil.price))),xaxp=c(0,22,11))",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#random-walk",
    "href": "TSA-Lecture14-model-specification.html#random-walk",
    "title": "R notebook: Model Specification",
    "section": "1.8 Random Walk",
    "text": "1.8 Random Walk\n\n\nCode\n# Generate a random walk\nset.seed(439)\nz &lt;- rnorm(60)\nrw &lt;- ts(cumsum(z))\n# Plot the random walk\nplot(rw, main=\"Random Walk\")\n\n\n\n\n\n\n\n\n\nCode\n# Unit root test of the random walk and its first difference\nadf.test(rw)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  rw\nDickey-Fuller = -1.5526, Lag order = 3, p-value = 0.7556\nalternative hypothesis: stationary\n\n\nCode\nadf.test(diff(rw))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(rw)\nDickey-Fuller = -3.3082, Lag order = 3, p-value = 0.079\nalternative hypothesis: stationary\n\n\nCode\n# ACF, PACF of the Random Walk\nacf(rw, main=\"ACF of Random Walk\")\n\n\n\n\n\n\n\n\n\nCode\npacf(rw, main=\"PACF of Random Walk\")\n\n\n\n\n\n\n\n\n\nAfter differencing \\[\nW_t=\\nabla Y_t = Y_t - Y_{t-1}.\n\\]\n\n\nCode\n# ACF, PACF of the differenced Random Walk\nacf(diff(rw), main=\"ACF of differenced Random Walk\")\n\n\n\n\n\n\n\n\n\nCode\npacf(diff(rw), main=\"PACF of differenced Random Walk\")",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#the-los-angeles-annual-rainfall-series",
    "href": "TSA-Lecture14-model-specification.html#the-los-angeles-annual-rainfall-series",
    "title": "R notebook: Model Specification",
    "section": "1.9 The Los Angeles Annual Rainfall Series",
    "text": "1.9 The Los Angeles Annual Rainfall Series\n\n\nCode\n# Load the data\ndata(larain)\nplot(larain)\n\n\n\n\n\n\n\n\n\nCode\n# Stabilize the variance\nqqnorm(log(larain),\n       main = \"Normal Q-Q Plot of log(larain)\") \nqqline(log(larain))\n\n\n\n\n\n\n\n\n\nDisplaying sample autocorrelations\n\n\nCode\nacf(log(larain),\n    xaxp=c(0,20,10),\n    main = \"ACF of log(larain)\")\n\n\n\n\n\n\n\n\n\nCode\nmean(log(larain))\n\n\n[1] 2.593385\n\n\nCode\nsd(log(larain))\n\n\n[1] 0.4774679",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#the-chemical-process-color-property-series",
    "href": "TSA-Lecture14-model-specification.html#the-chemical-process-color-property-series",
    "title": "R notebook: Model Specification",
    "section": "1.10 The Chemical Process Color Property Series",
    "text": "1.10 The Chemical Process Color Property Series\n\n\nCode\n# Load the data\ndata(color)\nplot(color)\n\n\n\n\n\n\n\n\n\nCode\n# Plot ACF and PACF\nacf(color,ci.type='ma', lag.max=20, main=\"ACF of color\")\n\n\n\n\n\n\n\n\n\nCode\npacf(color, lag.max=20, main=\"PACF of color\")",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#the-annual-abundance-of-canadian-hare-series",
    "href": "TSA-Lecture14-model-specification.html#the-annual-abundance-of-canadian-hare-series",
    "title": "R notebook: Model Specification",
    "section": "1.11 The Annual Abundance of Canadian Hare Series",
    "text": "1.11 The Annual Abundance of Canadian Hare Series\n\n\nCode\n# Load the data\ndata(hare)\nplot(hare)\n\n\n\n\n\n\n\n\n\nCode\n# Box-Cox transformation\nBoxCox.ar(hare)\n\n\n\n\n\n\n\n\n\nCode\n# Plot ACF and PACF of the transformed series\nacf(hare^.5, main=\"ACF of hare^.5\")\n\n\n\n\n\n\n\n\n\nCode\npacf(hare^.5, main=\"PACF of hare^.5\")",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture14-model-specification.html#the-oil-price-series",
    "href": "TSA-Lecture14-model-specification.html#the-oil-price-series",
    "title": "R notebook: Model Specification",
    "section": "1.12 The Oil Price Series",
    "text": "1.12 The Oil Price Series\n\n\nCode\nlibrary(tseries)\n# Load the data\ndata(\"oil.price\")\nplot(oil.price)\n\n\n\n\n\n\n\n\n\nCode\n# Plot ACF and PACF\nacf(as.vector(oil.price),\n    xaxp=c(0,24,12),\n    main=\"ACF of oil.price\")\n\n\n\n\n\n\n\n\n\nCode\npacf(as.vector(oil.price), \n    xaxp=c(0,24,12),\n    main=\"PACF of oil.price\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Log transformation\nplot(log(oil.price))\n\n\n\n\n\n\n\n\n\nCode\n# Differencing the log-transformed series\nplot(diff(log(oil.price)))\n\n\n\n\n\n\n\n\n\nCode\nacf(diff(as.vector(log(oil.price))),xaxp=c(0,24,12))\n\n\n\n\n\n\n\n\n\nCode\npacf(diff(as.vector(log(oil.price))),xaxp=c(0,24,12))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ADF test: to test for stationarity of log-transformed series\nadf.test(log(oil.price))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(oil.price)\nDickey-Fuller = -1.1119, Lag order = 6, p-value = 0.9189\nalternative hypothesis: stationary\n\n\nCode\nadf.test(diff(log(oil.price)))\n\n\nWarning in adf.test(diff(log(oil.price))): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(log(oil.price))\nDickey-Fuller = -6.6505, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nCode\nres &lt;- armasubsets(y=diff(log(oil.price)),nar=7,nma=7,y.name='test', ar.method='ols')\nplot(res, main = \"ARMA Subsets\")",
    "crumbs": [
      "R notebook: Model Specification"
    ]
  },
  {
    "objectID": "TSA-Lecture12.html",
    "href": "TSA-Lecture12.html",
    "title": "25 Spring 439/639 TSA: Lecture 12",
    "section": "",
    "text": "Last time, we introduced one definition of the partial autocorrelation function (partial ACF, PACF) \\(\\phi_{kk}\\).\nDefinition 1: \\[\n\\phi_{kk} = \\operatorname{corr} \\left(\n    Y_t,\\ Y_{t-k} \\ \\middle\\vert \\ Y_{t-1},\\, Y_{t-2},\\, \\dots,\\, Y_{t-k+1} \\right).\n\\] From this definition, it’s easy to show that: For AR(\\(1\\)) process, \\(Y_t = \\phi Y_{t-1}+ e_t\\), \\(\\phi_{11} = \\rho_1\\), and \\(\\phi_{kk}=0\\) for any lag \\(k\\ge 2\\).\n\n\n\nDefinition 2: \\[\n\\phi_{kk} = \\mathrm{corr}(\\operatorname{Res}_t, \\operatorname{Res}_{t-k}),\n\\] where \\(\\operatorname{Res}_t\\) = residual from (linear) regressing \\(Y_t\\) on \\(Y_{t-1}, \\dots, Y_{t-k+1}\\), and \\(\\operatorname{Res}_{t-k}\\) = residual from (linear) regressing \\(Y_{t-k}\\) on \\(Y_{t-1}, \\dots, Y_{t-k+1}\\). So \\(\\operatorname{Res}_t\\) and \\(\\operatorname{Res}_{t-k}\\) are the unexplained variation in \\(Y_t\\), \\(Y_{t-k}\\) after “partialling out” the effect of \\(Y_{t-1}, \\dots, Y_{t-k+1}\\) (i.e., “controlling for” \\(Y_{t-1}, \\dots, Y_{t-k+1}\\)).\nLet’s look at the AR(\\(1\\)) example \\(Y_t = \\phi Y_{t-1}+ e_t\\) again. To find \\(\\phi_{22}\\), we need to regress \\(Y_t\\) on \\(Y_{t-1}\\), and regress \\(Y_{t-2}\\) on \\(Y_{t-1}\\).\nSo we need to find \\(a, b\\) by minimizing \\[\n\\min_{a} \\mathbb{E}\\left( Y_t - a Y_{t-1} \\right)^2,\\quad\n\\min_{b} \\mathbb{E}\\left( Y_{t-2} - b Y_{t-1} \\right)^2,\n\\] then \\(\\operatorname{Res}_t = Y_t - \\widehat{Y}_t\\) and \\(\\operatorname{Res}_{t-k}= Y_{t-2} -\\widehat{Y}_{t-2}\\), where \\(\\widehat{Y}_t = a Y_{t-1}\\), \\(\\widehat{Y}_{t-2} = b Y_{t-1}\\).\nTo solve \\(\\min_{a} \\mathbb{E}\\left( Y_t - a Y_{t-1} \\right)^2\\), we take the derivative of the objective function: \\[\n\\frac{\\partial}{\\partial a} \\, \\mathbb{E}\\left[ \\left( Y_t - a Y_{t-1} \\right)^2 \\right]\n= -2 \\mathbb{E}\\left[ \\left( Y_t - a Y_{t-1} \\right) Y_{t-1} \\right]\n\\] Set the derivative equal to \\(0\\) gives \\[\n\\mathbb{E}\\left[ \\left( Y_t - a Y_{t-1} \\right) Y_{t-1} \\right] = 0 \\implies \\gamma_1 - a \\gamma_0 = 0\n\\implies a = \\frac{\\gamma_1}{\\gamma_0} = \\rho_1 = \\phi.\n\\] To solve \\(\\min_{b} \\mathbb{E}\\left( Y_{t-2} - b Y_{t-1} \\right)^2\\), we take the derivative of the objective function: \\[\n\\frac{\\partial}{\\partial b} \\mathbb{E}\\left( Y_{t-2} - b Y_{t-1} \\right)^2 = -2 \\mathbb{E}\\left[ \\left( Y_{t-2} - b Y_{t-1} \\right) Y_{t-1} \\right]\n\\] Set the derivative equal to \\(0\\) gives \\[\n\\mathbb{E}\\left[ \\left( Y_{t-2} - b Y_{t-1} \\right) Y_{t-1} \\right] = 0\n\\implies \\gamma_1 - b \\gamma_0 = 0 \\implies b = \\frac{\\gamma_1}{\\gamma_0} = \\rho_1 = \\phi.\n\\] Then we get \\[\n\\phi_{22} = \\mathrm{corr}(\\operatorname{Res}_t, \\operatorname{Res}_{t-2}) = \\mathrm{corr}(Y_t - \\phi Y_{t-1}, Y_{t-2} -\\phi Y_{t-1}) = \\mathrm{corr}(e_t, Y_{t-2} -\\phi Y_{t-1}) =0.\n\\] In general, for AR(\\(p\\)), the PACF at lags \\(1\\) through \\(p\\) can be nonzero, and PACF at lags \\(k\\ge p+1\\) are all zero. And we also have the following results.\n\n\n\n\nMA(\\(q\\))\nAR(\\(p\\))\nARMA(\\(p,q\\))\n\n\n\n\nACF\ncuts off after \\(q\\)\nexponential decay\nexponential decay\n\n\nPACF\nexponential decay\ncuts off after \\(p\\)\nexponential decay\n\n\n\n\n\n\nWe also have an alternative, “computational” definition of \\(\\phi_{kk}\\). The idea is to fit an AR(\\(k\\)) model to the stationary time series \\((Y_t)\\) of our interest, i.e, fit the linear regression to \\((Y_t)\\): \\[\nY_t = \\phi_{k1} Y_{t-1} + \\phi_{k2} Y_{t-2} + \\cdots + \\phi_{kk} Y_{t-k} + \\epsilon,\n\\] or in other words, find \\(\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk}\\) to minimize \\[\n\\min_{\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk}} \\mathbb{E}\\left( Y_t - \\phi_{k1} Y_{t-1} - \\phi_{k2} Y_{t-2} - \\cdots - \\phi_{kk} Y_{t-k} \\right)^2 .\n\\]\nDefinition 3 (We claim without proof that:) For stationary \\((Y_t)\\), its PACF at lag \\(k\\), i.e. \\(\\phi_{kk}\\), is same as the fitted value of \\(\\phi_{kk}\\) in the \\((\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk})\\) from the regression (namely, AR(\\(k\\)) fitting) above.\nIn other words, \\(\\phi_{kk}\\) is the last \\(\\phi_{kj}\\) term in the AR(\\(k\\)) approximation to \\((Y_t)\\).\nFrom this definition, we can show the following fact: the solution \\((\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk})\\) to the regression above must satisfy \\[\n\\Gamma_k \\ \\vec\\phi_k = \\vec\\gamma_k,\n\\] \\[\n\\text{where}\\quad\n\\Gamma_k = \\begin{bmatrix}\n\\gamma_0 & \\gamma_1 & \\cdots & \\gamma_{k-1} \\\\\n\\gamma_1 & \\gamma_0 & \\cdots & \\gamma_{k-2} \\\\\n\\vdots & \\vdots &        & \\vdots \\\\\n\\gamma_{k-1} & \\gamma_{k-2} & \\cdots & \\gamma_0\n\\end{bmatrix},\\quad\n\\vec\\phi_k = \\begin{bmatrix}\n\\phi_{k1} \\\\\n\\phi_{k2} \\\\\n\\vdots \\\\\n\\phi_{kk}\n\\end{bmatrix},\\quad\n\\vec\\gamma_k = \\begin{bmatrix}\n\\gamma_1 \\\\\n\\gamma_2 \\\\\n\\vdots \\\\\n\\gamma_k\n\\end{bmatrix}.\n\\] Note: the derivation for \\(\\Gamma_k \\ \\vec\\phi_k = \\vec\\gamma_k\\) is very similar to the AR(\\(1\\)) example after Definition 2.\nSo now we know the PACF \\(\\phi_{kk}\\) is the last entry of \\(\\Gamma_k^{-1}\\ \\vec\\gamma_k\\). Inverting a \\(k\\times k\\) matrix \\(\\Gamma_k\\) can be computationally expensive. Instead, we can use Durbin–Levinson Recursion (we will get to it soon) to directly calculate the entries of \\(\\Gamma_k^{-1}\\ \\vec\\gamma_k\\) without computing matrix inverse.\nFrom this Definition 3 and the fact above, we can also intuitively see the PACF behavior for AR(\\(p\\)) that we mentioned earlier. Suppose \\((Y_n)\\) is an AR(\\(p\\)), \\(Y_t = \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots + \\phi_{p} Y_{t-p} + e_t\\) where \\(\\phi_1,...,\\phi_p\\) are the true parameters of the AR(\\(p\\)). To get PACF \\(\\phi_{kk}\\), we need to fit an AR(\\(k\\))-like linear regression for \\(Y_t\\). If \\(k\\ge p\\), then intuitively, \\[\n\\begin{split}\n\\text{fitting a regression model}\\quad & Y_t = \\phi_{k1} Y_{t-1} + \\phi_{k2} Y_{t-2} + \\cdots + \\phi_{kk} Y_{t-k} + \\epsilon, \\\\\n\\text{for an }\\operatorname{AR}(p)\\quad & Y_t = \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots + \\phi_{p} Y_{t-p} + e_t,\n\\end{split}\n\\] should give us the fitted value \\((\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk}) = (\\phi_1,...,\\phi_p,0,...,0)\\). So we have the following results for AR(\\(p\\)).\n\nIf \\(k=p\\), then \\(\\phi_{pp} = \\phi_{p}\\).\nIf \\(k\\ge p+1\\), i.e. \\(k&gt;p\\), then \\(\\phi_{kk} = 0\\).\n\nRemark: For AR(\\(p\\)) and \\(k\\ge p\\), the statement \\((\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk}) = (\\phi_1,...,\\phi_p,0,...,0)\\) can be verified by showing that it is indeed a solution to \\(\\Gamma_k \\ \\vec\\phi_k = \\vec\\gamma_k\\). The derivation reduces to YW equations for AR(\\(p\\)).",
    "crumbs": [
      "Lecture 12"
    ]
  },
  {
    "objectID": "TSA-Lecture12.html#definition-1",
    "href": "TSA-Lecture12.html#definition-1",
    "title": "25 Spring 439/639 TSA: Lecture 12",
    "section": "",
    "text": "Last time, we introduced one definition of the partial autocorrelation function (partial ACF, PACF) \\(\\phi_{kk}\\).\nDefinition 1: \\[\n\\phi_{kk} = \\operatorname{corr} \\left(\n    Y_t,\\ Y_{t-k} \\ \\middle\\vert \\ Y_{t-1},\\, Y_{t-2},\\, \\dots,\\, Y_{t-k+1} \\right).\n\\] From this definition, it’s easy to show that: For AR(\\(1\\)) process, \\(Y_t = \\phi Y_{t-1}+ e_t\\), \\(\\phi_{11} = \\rho_1\\), and \\(\\phi_{kk}=0\\) for any lag \\(k\\ge 2\\).",
    "crumbs": [
      "Lecture 12"
    ]
  },
  {
    "objectID": "TSA-Lecture12.html#definition-2",
    "href": "TSA-Lecture12.html#definition-2",
    "title": "25 Spring 439/639 TSA: Lecture 12",
    "section": "",
    "text": "Definition 2: \\[\n\\phi_{kk} = \\mathrm{corr}(\\operatorname{Res}_t, \\operatorname{Res}_{t-k}),\n\\] where \\(\\operatorname{Res}_t\\) = residual from (linear) regressing \\(Y_t\\) on \\(Y_{t-1}, \\dots, Y_{t-k+1}\\), and \\(\\operatorname{Res}_{t-k}\\) = residual from (linear) regressing \\(Y_{t-k}\\) on \\(Y_{t-1}, \\dots, Y_{t-k+1}\\). So \\(\\operatorname{Res}_t\\) and \\(\\operatorname{Res}_{t-k}\\) are the unexplained variation in \\(Y_t\\), \\(Y_{t-k}\\) after “partialling out” the effect of \\(Y_{t-1}, \\dots, Y_{t-k+1}\\) (i.e., “controlling for” \\(Y_{t-1}, \\dots, Y_{t-k+1}\\)).\nLet’s look at the AR(\\(1\\)) example \\(Y_t = \\phi Y_{t-1}+ e_t\\) again. To find \\(\\phi_{22}\\), we need to regress \\(Y_t\\) on \\(Y_{t-1}\\), and regress \\(Y_{t-2}\\) on \\(Y_{t-1}\\).\nSo we need to find \\(a, b\\) by minimizing \\[\n\\min_{a} \\mathbb{E}\\left( Y_t - a Y_{t-1} \\right)^2,\\quad\n\\min_{b} \\mathbb{E}\\left( Y_{t-2} - b Y_{t-1} \\right)^2,\n\\] then \\(\\operatorname{Res}_t = Y_t - \\widehat{Y}_t\\) and \\(\\operatorname{Res}_{t-k}= Y_{t-2} -\\widehat{Y}_{t-2}\\), where \\(\\widehat{Y}_t = a Y_{t-1}\\), \\(\\widehat{Y}_{t-2} = b Y_{t-1}\\).\nTo solve \\(\\min_{a} \\mathbb{E}\\left( Y_t - a Y_{t-1} \\right)^2\\), we take the derivative of the objective function: \\[\n\\frac{\\partial}{\\partial a} \\, \\mathbb{E}\\left[ \\left( Y_t - a Y_{t-1} \\right)^2 \\right]\n= -2 \\mathbb{E}\\left[ \\left( Y_t - a Y_{t-1} \\right) Y_{t-1} \\right]\n\\] Set the derivative equal to \\(0\\) gives \\[\n\\mathbb{E}\\left[ \\left( Y_t - a Y_{t-1} \\right) Y_{t-1} \\right] = 0 \\implies \\gamma_1 - a \\gamma_0 = 0\n\\implies a = \\frac{\\gamma_1}{\\gamma_0} = \\rho_1 = \\phi.\n\\] To solve \\(\\min_{b} \\mathbb{E}\\left( Y_{t-2} - b Y_{t-1} \\right)^2\\), we take the derivative of the objective function: \\[\n\\frac{\\partial}{\\partial b} \\mathbb{E}\\left( Y_{t-2} - b Y_{t-1} \\right)^2 = -2 \\mathbb{E}\\left[ \\left( Y_{t-2} - b Y_{t-1} \\right) Y_{t-1} \\right]\n\\] Set the derivative equal to \\(0\\) gives \\[\n\\mathbb{E}\\left[ \\left( Y_{t-2} - b Y_{t-1} \\right) Y_{t-1} \\right] = 0\n\\implies \\gamma_1 - b \\gamma_0 = 0 \\implies b = \\frac{\\gamma_1}{\\gamma_0} = \\rho_1 = \\phi.\n\\] Then we get \\[\n\\phi_{22} = \\mathrm{corr}(\\operatorname{Res}_t, \\operatorname{Res}_{t-2}) = \\mathrm{corr}(Y_t - \\phi Y_{t-1}, Y_{t-2} -\\phi Y_{t-1}) = \\mathrm{corr}(e_t, Y_{t-2} -\\phi Y_{t-1}) =0.\n\\] In general, for AR(\\(p\\)), the PACF at lags \\(1\\) through \\(p\\) can be nonzero, and PACF at lags \\(k\\ge p+1\\) are all zero. And we also have the following results.\n\n\n\n\nMA(\\(q\\))\nAR(\\(p\\))\nARMA(\\(p,q\\))\n\n\n\n\nACF\ncuts off after \\(q\\)\nexponential decay\nexponential decay\n\n\nPACF\nexponential decay\ncuts off after \\(p\\)\nexponential decay",
    "crumbs": [
      "Lecture 12"
    ]
  },
  {
    "objectID": "TSA-Lecture12.html#definition-3",
    "href": "TSA-Lecture12.html#definition-3",
    "title": "25 Spring 439/639 TSA: Lecture 12",
    "section": "",
    "text": "We also have an alternative, “computational” definition of \\(\\phi_{kk}\\). The idea is to fit an AR(\\(k\\)) model to the stationary time series \\((Y_t)\\) of our interest, i.e, fit the linear regression to \\((Y_t)\\): \\[\nY_t = \\phi_{k1} Y_{t-1} + \\phi_{k2} Y_{t-2} + \\cdots + \\phi_{kk} Y_{t-k} + \\epsilon,\n\\] or in other words, find \\(\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk}\\) to minimize \\[\n\\min_{\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk}} \\mathbb{E}\\left( Y_t - \\phi_{k1} Y_{t-1} - \\phi_{k2} Y_{t-2} - \\cdots - \\phi_{kk} Y_{t-k} \\right)^2 .\n\\]\nDefinition 3 (We claim without proof that:) For stationary \\((Y_t)\\), its PACF at lag \\(k\\), i.e. \\(\\phi_{kk}\\), is same as the fitted value of \\(\\phi_{kk}\\) in the \\((\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk})\\) from the regression (namely, AR(\\(k\\)) fitting) above.\nIn other words, \\(\\phi_{kk}\\) is the last \\(\\phi_{kj}\\) term in the AR(\\(k\\)) approximation to \\((Y_t)\\).\nFrom this definition, we can show the following fact: the solution \\((\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk})\\) to the regression above must satisfy \\[\n\\Gamma_k \\ \\vec\\phi_k = \\vec\\gamma_k,\n\\] \\[\n\\text{where}\\quad\n\\Gamma_k = \\begin{bmatrix}\n\\gamma_0 & \\gamma_1 & \\cdots & \\gamma_{k-1} \\\\\n\\gamma_1 & \\gamma_0 & \\cdots & \\gamma_{k-2} \\\\\n\\vdots & \\vdots &        & \\vdots \\\\\n\\gamma_{k-1} & \\gamma_{k-2} & \\cdots & \\gamma_0\n\\end{bmatrix},\\quad\n\\vec\\phi_k = \\begin{bmatrix}\n\\phi_{k1} \\\\\n\\phi_{k2} \\\\\n\\vdots \\\\\n\\phi_{kk}\n\\end{bmatrix},\\quad\n\\vec\\gamma_k = \\begin{bmatrix}\n\\gamma_1 \\\\\n\\gamma_2 \\\\\n\\vdots \\\\\n\\gamma_k\n\\end{bmatrix}.\n\\] Note: the derivation for \\(\\Gamma_k \\ \\vec\\phi_k = \\vec\\gamma_k\\) is very similar to the AR(\\(1\\)) example after Definition 2.\nSo now we know the PACF \\(\\phi_{kk}\\) is the last entry of \\(\\Gamma_k^{-1}\\ \\vec\\gamma_k\\). Inverting a \\(k\\times k\\) matrix \\(\\Gamma_k\\) can be computationally expensive. Instead, we can use Durbin–Levinson Recursion (we will get to it soon) to directly calculate the entries of \\(\\Gamma_k^{-1}\\ \\vec\\gamma_k\\) without computing matrix inverse.\nFrom this Definition 3 and the fact above, we can also intuitively see the PACF behavior for AR(\\(p\\)) that we mentioned earlier. Suppose \\((Y_n)\\) is an AR(\\(p\\)), \\(Y_t = \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots + \\phi_{p} Y_{t-p} + e_t\\) where \\(\\phi_1,...,\\phi_p\\) are the true parameters of the AR(\\(p\\)). To get PACF \\(\\phi_{kk}\\), we need to fit an AR(\\(k\\))-like linear regression for \\(Y_t\\). If \\(k\\ge p\\), then intuitively, \\[\n\\begin{split}\n\\text{fitting a regression model}\\quad & Y_t = \\phi_{k1} Y_{t-1} + \\phi_{k2} Y_{t-2} + \\cdots + \\phi_{kk} Y_{t-k} + \\epsilon, \\\\\n\\text{for an }\\operatorname{AR}(p)\\quad & Y_t = \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots + \\phi_{p} Y_{t-p} + e_t,\n\\end{split}\n\\] should give us the fitted value \\((\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk}) = (\\phi_1,...,\\phi_p,0,...,0)\\). So we have the following results for AR(\\(p\\)).\n\nIf \\(k=p\\), then \\(\\phi_{pp} = \\phi_{p}\\).\nIf \\(k\\ge p+1\\), i.e. \\(k&gt;p\\), then \\(\\phi_{kk} = 0\\).\n\nRemark: For AR(\\(p\\)) and \\(k\\ge p\\), the statement \\((\\phi_{k1},\\phi_{k2},\\dots,\\phi_{kk}) = (\\phi_1,...,\\phi_p,0,...,0)\\) can be verified by showing that it is indeed a solution to \\(\\Gamma_k \\ \\vec\\phi_k = \\vec\\gamma_k\\). The derivation reduces to YW equations for AR(\\(p\\)).",
    "crumbs": [
      "Lecture 12"
    ]
  },
  {
    "objectID": "TSA-Lecture10.html",
    "href": "TSA-Lecture10.html",
    "title": "25 Spring 439/639 TSA: Lecture 10",
    "section": "",
    "text": "Last time, we introduced ARIMA(\\(p,d,q\\)), a model for non-stationary time series. If \\(Y_t \\sim \\operatorname{ARIMA}(p, d, q)\\), taking difference \\(d\\) times gives a stationary time series \\(W_t = \\nabla^d Y_t \\sim \\operatorname{ARMA}(p, q)\\).\n\n\nLet’s take another look at the random walk + noise example from last lecture. \\[\nY_t = X_t + \\eta_t = \\sum_{j=1}^t e_j + \\eta_t, \\quad \\eta_t \\sim \\mathrm{iid}(0,\\sigma_\\eta^2),\\quad e_t \\sim \\mathrm{iid}(0,\\sigma_e^2)\n\\] where \\((X_t)\\) is a random walk, \\((\\eta_t)\\) is a sequence of noise, and \\((\\eta_t)\\) is independent of \\((e_t)\\). Taking the difference gives \\[\n\\begin{split}\nW_t = \\nabla Y_t\n    = e_t + \\eta_t - \\eta_{t-1}.\n\\end{split}\n\\] We can verify \\((W_t)\\) is stationary: \\[\n\\mathbb{E} W_t = \\mathbb{E} \\left[ e_t + \\eta_t - \\eta_{t-1} \\right] = 0 ,\n\\] \\[\n\\operatorname{Var} (W_t) = \\operatorname{Var} \\left( e_t + \\eta_t - \\eta_{t-1} \\right) = \\sigma_e^2 + 2 \\sigma_\\eta^2\n\\qquad \\text{(by independence)},\n\\] \\[\n\\gamma_1 = \\operatorname{Cov}(W_t, W_{t-1})\n= \\operatorname{Cov}\\left( e_t + \\eta_t - \\eta_{t-1},\\, e_{t-1} + \\eta_{t-1} - \\eta_{t-2} \\right)\n= -\\operatorname{Cov}(\\eta_{t-1}, \\eta_{t-1}) = -\\sigma_\\eta^2 ,\n\\] \\[\n\\gamma_k = \\operatorname{Cov}(W_t, W_{t-k})\n= \\operatorname{Cov}\\left( e_t + \\eta_t - \\eta_{t-1},\\, e_{t-k} + \\eta_{t-k} - \\eta_{t-k-1} \\right) = 0, \\quad \\text{for all } k \\geq 2.\n\\] So \\((W_t)\\) is stationary. Then by the reasoning from last time, there exist an uncorrelated stationary process \\((\\widetilde{\\epsilon}_t)\\) (think of \\(\\left( \\widetilde{e}_t \\right) \\sim \\text{iid}\\left(0, \\widetilde{\\sigma}_e^2\\right)\\)) and a constant \\(\\widetilde{\\theta}\\) such that \\[\nW_t = \\widetilde{e}_t - \\widetilde{\\theta}\\,\\widetilde{e}_{t-1}\n\\sim \\text{MA}(1) \\implies Y_t \\sim \\mathrm{IMA}(1,1) = \\mathrm{ARIMA}(0,1,1).\n\\]\n\n\n\nIf \\(Y_t \\sim \\operatorname{ARIMA}(p, d, q)\\), then \\(W_t = \\nabla^d Y_t \\sim \\operatorname{ARMA}(p, q)\\). So it can be characterized by the AR polynomial \\(\\Phi(x)\\) and MA polynomial \\(\\Theta(x)\\): \\[\n\\Phi(B) W_t = \\Theta(B) e_t .\n\\] Note that \\(W_t = (1-B)^d Y_t\\), we have \\[\n\\Phi(B)\\, (1-B)^d\\, Y_t = \\Theta(B) \\, e_t .\n\\] So \\(\\Phi^*(x) = \\Phi(x)\\ (1-x)^d\\) can be seen as an AR polynomial for \\(Y_t\\). Assume \\((W_t)\\) is causal, then \\(\\Phi^*(x)\\) has \\(p+d\\) roots, with \\(z=1\\) repeated \\(d\\) times and the other \\(p\\) roots (i.e. the roots of \\(\\Phi(x)\\)) are all outside the unit disk.\n\n\n\nIn reality, usually \\(d=1\\) or \\(d=2\\). If \\(d\\) is too large, this is called overdifferencing, and it has the following issues:\n\nLeads to more complicated than necessary models.\nLeads to non-invertible models.\n\nFor example, consider the random walk \\(Y_t = \\sum_{i=1}^t e_i\\). \\((Y_t)\\) is non-stationary. Take the difference: \\[\nW_t = \\nabla Y_t = Y_t - Y_{t-1}\n= \\sum_{i=1}^{t} e_i - \\sum_{i=1}^{t-1} e_i = e_t .\n\\] So \\(W_t\\) can by modeled by an MA(\\(0\\)), which is stationary (and invertible). If we take the difference one more time: \\[\nZ_t = \\nabla^2 Y_t = W_t - W_{t-1}= e_t - e_{t-1} \\sim \\mathrm{MA}(1)\n\\] Although it can still be modeled by an ARMA model MA(\\(1\\)), but it is more complicated than MA(\\(0\\)), and this MA(\\(1\\)) above is not invertible (since \\(|\\theta|=1\\)).",
    "crumbs": [
      "Lecture 10"
    ]
  },
  {
    "objectID": "TSA-Lecture10.html#review-the-previous-example",
    "href": "TSA-Lecture10.html#review-the-previous-example",
    "title": "25 Spring 439/639 TSA: Lecture 10",
    "section": "",
    "text": "Let’s take another look at the random walk + noise example from last lecture. \\[\nY_t = X_t + \\eta_t = \\sum_{j=1}^t e_j + \\eta_t, \\quad \\eta_t \\sim \\mathrm{iid}(0,\\sigma_\\eta^2),\\quad e_t \\sim \\mathrm{iid}(0,\\sigma_e^2)\n\\] where \\((X_t)\\) is a random walk, \\((\\eta_t)\\) is a sequence of noise, and \\((\\eta_t)\\) is independent of \\((e_t)\\). Taking the difference gives \\[\n\\begin{split}\nW_t = \\nabla Y_t\n    = e_t + \\eta_t - \\eta_{t-1}.\n\\end{split}\n\\] We can verify \\((W_t)\\) is stationary: \\[\n\\mathbb{E} W_t = \\mathbb{E} \\left[ e_t + \\eta_t - \\eta_{t-1} \\right] = 0 ,\n\\] \\[\n\\operatorname{Var} (W_t) = \\operatorname{Var} \\left( e_t + \\eta_t - \\eta_{t-1} \\right) = \\sigma_e^2 + 2 \\sigma_\\eta^2\n\\qquad \\text{(by independence)},\n\\] \\[\n\\gamma_1 = \\operatorname{Cov}(W_t, W_{t-1})\n= \\operatorname{Cov}\\left( e_t + \\eta_t - \\eta_{t-1},\\, e_{t-1} + \\eta_{t-1} - \\eta_{t-2} \\right)\n= -\\operatorname{Cov}(\\eta_{t-1}, \\eta_{t-1}) = -\\sigma_\\eta^2 ,\n\\] \\[\n\\gamma_k = \\operatorname{Cov}(W_t, W_{t-k})\n= \\operatorname{Cov}\\left( e_t + \\eta_t - \\eta_{t-1},\\, e_{t-k} + \\eta_{t-k} - \\eta_{t-k-1} \\right) = 0, \\quad \\text{for all } k \\geq 2.\n\\] So \\((W_t)\\) is stationary. Then by the reasoning from last time, there exist an uncorrelated stationary process \\((\\widetilde{\\epsilon}_t)\\) (think of \\(\\left( \\widetilde{e}_t \\right) \\sim \\text{iid}\\left(0, \\widetilde{\\sigma}_e^2\\right)\\)) and a constant \\(\\widetilde{\\theta}\\) such that \\[\nW_t = \\widetilde{e}_t - \\widetilde{\\theta}\\,\\widetilde{e}_{t-1}\n\\sim \\text{MA}(1) \\implies Y_t \\sim \\mathrm{IMA}(1,1) = \\mathrm{ARIMA}(0,1,1).\n\\]",
    "crumbs": [
      "Lecture 10"
    ]
  },
  {
    "objectID": "TSA-Lecture10.html#ar-and-ma-polynomial-for-arimapdq",
    "href": "TSA-Lecture10.html#ar-and-ma-polynomial-for-arimapdq",
    "title": "25 Spring 439/639 TSA: Lecture 10",
    "section": "",
    "text": "If \\(Y_t \\sim \\operatorname{ARIMA}(p, d, q)\\), then \\(W_t = \\nabla^d Y_t \\sim \\operatorname{ARMA}(p, q)\\). So it can be characterized by the AR polynomial \\(\\Phi(x)\\) and MA polynomial \\(\\Theta(x)\\): \\[\n\\Phi(B) W_t = \\Theta(B) e_t .\n\\] Note that \\(W_t = (1-B)^d Y_t\\), we have \\[\n\\Phi(B)\\, (1-B)^d\\, Y_t = \\Theta(B) \\, e_t .\n\\] So \\(\\Phi^*(x) = \\Phi(x)\\ (1-x)^d\\) can be seen as an AR polynomial for \\(Y_t\\). Assume \\((W_t)\\) is causal, then \\(\\Phi^*(x)\\) has \\(p+d\\) roots, with \\(z=1\\) repeated \\(d\\) times and the other \\(p\\) roots (i.e. the roots of \\(\\Phi(x)\\)) are all outside the unit disk.",
    "crumbs": [
      "Lecture 10"
    ]
  },
  {
    "objectID": "TSA-Lecture10.html#overdifferencing",
    "href": "TSA-Lecture10.html#overdifferencing",
    "title": "25 Spring 439/639 TSA: Lecture 10",
    "section": "",
    "text": "In reality, usually \\(d=1\\) or \\(d=2\\). If \\(d\\) is too large, this is called overdifferencing, and it has the following issues:\n\nLeads to more complicated than necessary models.\nLeads to non-invertible models.\n\nFor example, consider the random walk \\(Y_t = \\sum_{i=1}^t e_i\\). \\((Y_t)\\) is non-stationary. Take the difference: \\[\nW_t = \\nabla Y_t = Y_t - Y_{t-1}\n= \\sum_{i=1}^{t} e_i - \\sum_{i=1}^{t-1} e_i = e_t .\n\\] So \\(W_t\\) can by modeled by an MA(\\(0\\)), which is stationary (and invertible). If we take the difference one more time: \\[\nZ_t = \\nabla^2 Y_t = W_t - W_{t-1}= e_t - e_{t-1} \\sim \\mathrm{MA}(1)\n\\] Although it can still be modeled by an ARMA model MA(\\(1\\)), but it is more complicated than MA(\\(0\\)), and this MA(\\(1\\)) above is not invertible (since \\(|\\theta|=1\\)).",
    "crumbs": [
      "Lecture 10"
    ]
  },
  {
    "objectID": "TSA-Lecture10.html#the-corresponding-non-stationary-armapdq-for-arimapdq",
    "href": "TSA-Lecture10.html#the-corresponding-non-stationary-armapdq-for-arimapdq",
    "title": "25 Spring 439/639 TSA: Lecture 10",
    "section": "3.1 The corresponding non-stationary ARMA(\\(p+d,q\\)) for ARIMA(\\(p,d,q\\))",
    "text": "3.1 The corresponding non-stationary ARMA(\\(p+d,q\\)) for ARIMA(\\(p,d,q\\))\nExample: Suppose \\(Y_t \\sim \\operatorname{ARIMA}(p, 1, q)\\). Let \\(W_t = \\nabla Y_t\\), then \\(W_t \\sim \\operatorname{ARMA}(p, q)\\), so \\[\nW_t - \\phi_1 W_{t-1} - \\cdots - \\phi_p W_{t-p} = e_t - \\theta_1 e_{t-1} - \\cdots - \\theta_q e_{t-q}.\n\\] \\[\n(Y_t - Y_{t-1}) - \\phi_1 (Y_{t-1} - Y_{t-2}) - \\phi_2 (Y_{t-2} - Y_{t-3}) - \\cdots\n- \\phi_p (Y_{t-p} - Y_{t-p-1}) = e_t - \\theta_1 e_{t-1} - \\cdots - \\theta_q e_{t-q}.\n\\] \\[\nY_t\n- (1 + \\phi_1) Y_{t-1}\n- (\\phi_2 - \\phi_1) Y_{t-2}\n- \\cdots\n- (\\phi_p - \\phi_{p-1}) Y_{t-p}\n+ \\phi_p Y_{t-p-1}\n= e_t - \\theta_1 e_{t-1} - \\cdots - \\theta_q e_{t-q}.\n\\] As we already know, the last equation above is an ARMA(\\(p+1,q\\)), but not stationary. Indeed, its AR polynomial is \\[\n\\begin{split}\n\\Phi^*(x) &= 1 - (1 + \\phi_1)x - (\\phi_2 - \\phi_1)x^2 - \\cdots - (\\phi_p - \\phi_{p-1})x^p + \\phi_p x^{p+1} = (1 - x)\\left(1 - \\phi_1 x - \\phi_2 x^2 - \\cdots - \\phi_p x^p \\right)\n\\end{split}\n\\] which has a root \\(z=1\\).",
    "crumbs": [
      "Lecture 10"
    ]
  },
  {
    "objectID": "TSA-Lecture10.html#glp-representation-of-arima011",
    "href": "TSA-Lecture10.html#glp-representation-of-arima011",
    "title": "25 Spring 439/639 TSA: Lecture 10",
    "section": "3.2 “GLP” representation of ARIMA(\\(0,1,1\\))",
    "text": "3.2 “GLP” representation of ARIMA(\\(0,1,1\\))\nAs we mentioned before, we cannot really derive a GLP representation for a non-stationary process. Some steps in the following analysis are not rigorous. Keep in mind that the big idea is to get a sense of the ACF behavior through an analogous way of GLP.\nSuppose \\(Y_t \\sim \\operatorname{ARIMA}(0, 1, 1)\\), with \\(\\nabla Y_t = e_t - \\theta e_{t-1}\\). Then \\(Y_t - Y_{t-1} = e_t - \\theta\\, e_{t-1}\\). So \\[\n\\begin{split}\nY_t &= Y_{t-1} + e_t - \\theta\\, e_{t-1} = Y_{t-2} + e_{t-1} - \\theta\\, e_{t-2} + e_t - \\theta\\, e_{t-1} \\\\\n& = Y_{t-2} + e_t + (1-\\theta)\\, e_{t-1} - \\theta\\, e_{t-2} \\\\\n&= \\cdots = Y_{t-m} + e_t + (1-\\theta)\\, e_{t-1} + \\ldots + (1-\\theta)\\, e_{t-m+1} - \\theta\\, e_{t-m} \\\\\n& \\approx e_t + \\sum_{j=1}^{\\infty} (1-\\theta) e_{t-j}\n\\end{split}\n\\] where the last step is not rigorous, but can be thought as: \\(Y_{t-m} \\to 0\\) as \\(m \\to \\infty\\) (assuming the process started at zero).\nThe last line above \\(e_t + \\sum_{j=1}^{\\infty} (1-\\theta) e_{t-j}\\) looks like a GLP, but it is not a GLP because \\(\\sum_{j=1}^{\\infty} |1-\\theta|\\) diverges (assuming \\(\\theta\\neq 1\\)), the condition \\(\\sum |\\psi_j| &lt; \\infty\\) fails.\nOne can also show that : \\[\n\\operatorname{Var}(Y_t) =\n\\left[1 + \\theta^2 + (1-\\theta)^2 (t+m)\\right]\\sigma_e^2\n\\qquad (\\text{which grows linearly in } t),\n\\] and the following results (not rigorous) for moderate \\(k\\) and large \\(t\\) \\[\n\\rho_{t, t-k} =\n\\frac{[1 - \\theta + \\theta^2 + (1-\\theta)^2 (t + m - k)] \\sigma_e^2}\n     {\\sqrt{ \\operatorname{Var}(Y_t) \\operatorname{Var}(Y_{t-k}) } }\n\\approx\n\\frac{(1-\\theta)^2 (t+m)\\, \\sigma_e^2}\n     {\\sqrt{ (1-\\theta)^2 (t+m) \\cdot (1-\\theta)^2 (t+m-k) }\\, \\sigma_e^2}\n\\approx 1 .\n\\] So for an IMA(\\(1,1\\)) process, the ACF \\(\\rho_{t, t-k} \\approx 1\\) for moderate \\(k\\) and large \\(t\\), which behaves similar to a random walk (for large \\(t\\)). If we plot the time series, it will exhibit wandering behavior (RW-like).",
    "crumbs": [
      "Lecture 10"
    ]
  },
  {
    "objectID": "TSA-Lecture10.html#glp-representation-of-arima110",
    "href": "TSA-Lecture10.html#glp-representation-of-arima110",
    "title": "25 Spring 439/639 TSA: Lecture 10",
    "section": "3.3 “GLP” representation of ARIMA(\\(1,1,0\\))",
    "text": "3.3 “GLP” representation of ARIMA(\\(1,1,0\\))\nSuppose \\(Y_t \\sim \\operatorname{ARIMA}(1, 1, 0)\\), with \\(Y_{t} - Y_{t-1} - \\phi ( Y_{t-1} - Y_{t-2} ) = e_t\\). So \\[\nY_{t} = (1 + \\phi) Y_{t-1} - \\phi Y_{t-2} + e_t\n\\] which is a non-stationary AR(\\(2\\)).\nAs before, we suppose there is a “GLP” representation \\(Y_t = \\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots\\). Plug into the non-stationary AR(\\(2\\)) above: \\[\n\\left( \\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots \\right) =\n(1+\\phi)\\left( \\psi_0 e_{t-1} + \\psi_1 e_{t-2} + \\psi_2 e_{t-3} + \\cdots \\right)\n- \\phi \\left( \\psi_0 e_{t-2} + \\psi_1 e_{t-3} + \\psi_2 e_{t-4} + \\cdots \\right)\n+ e_t\n\\] Comparing the coefficients of \\(e_{t-k}\\): \\[\n\\begin{cases}\n\\psi_0 = 1\\\\\n\\psi_1 = (1+\\phi)\\psi_0 \\\\\n\\psi_k = (1+\\phi)\\psi_{k-1} - \\phi\\psi_{k-2} \\quad \\text{for } k\\ge 2\n\\end{cases}\n\\] which gives \\[\n\\psi_k = 1 + \\phi + \\cdots + \\phi^k = \\frac{1-\\phi^{k+1}}{1-\\phi},\\quad \\text{for any } k\\ge 0.\n\\] Exercise: verify this result.\nAs we expected, this “GLP” is not a GLP, since the condition \\(\\sum |\\psi_j| &lt; \\infty\\) fails.",
    "crumbs": [
      "Lecture 10"
    ]
  },
  {
    "objectID": "TSA-Lecture08.html",
    "href": "TSA-Lecture08.html",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "",
    "text": "So far, we considered models for stationary time series AR(\\(p\\)), MA(\\(q\\)), ARMA(\\(p,q\\)). These models can be used for the (stationary) stochastic component in observed time series.\n(Assume the underlying distribution of \\((Y_t)\\) is stationary. See Wold’s decomposition theorem from earlier lectures.) For an observed time series \\((Y_t)\\), it can be represented as a sum of two parts: \\[\n\\underbrace{Y_t}_{observed} = \\underbrace{\\mu_t}_{deterministic} + \\underbrace{X_t}_{stochastic} .\n\\]\n\n\\(\\{X_t\\}\\) is a stationary stochastic component. We may fit a model (like AR(\\(p\\)), MA(\\(q\\)), ARMA(\\(p,q\\))) for it.\n\\(\\{\\mu_t\\}\\) is a deterministic component. And it is often non-stationary (which reduces to non-constant, since it is deterministic). It may reflect the trend, or trend combined with seasonality.\n\nThe idea to deal with \\((Y_t)\\):\n\nfirst estimate \\(\\mu_t\\) with \\(\\widehat{\\mu}_t\\),\nestimate \\(X_t\\) by \\(\\widehat{X}_t = Y_t - \\widehat{\\mu}_t\\).\n\nThen we can fit any of the stationary time series models (like AR(\\(p\\)), MA(\\(q\\)), ARMA(\\(p,q\\))) to the estimated residuals \\(\\widehat{X}_t\\).",
    "crumbs": [
      "Lecture 8"
    ]
  },
  {
    "objectID": "TSA-Lecture08.html#example-ma1",
    "href": "TSA-Lecture08.html#example-ma1",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "2.1 Example: MA(\\(1\\))",
    "text": "2.1 Example: MA(\\(1\\))\nMA(\\(1\\)) time series are all \\(q\\)-dependent with \\(q=1\\). Consider an MA(\\(1\\)) with \\(\\theta= \\frac{1}{2}\\): \\[\nY_t = e_t - \\frac{1}{2} e_{t-1}.\n\\] We have \\[\n\\rho_1 = \\frac{-\\theta}{1 + \\theta^2}\n= \\frac{-\\frac{1}{2}}{1 + \\frac{1}{4}}\n= -\\frac{2}{5}, \\quad \\rho_k=0 \\text{ for } k\\ge 2.\n\\] Then \\[\n\\begin{split}\n\\operatorname{Var}(\\overline{Y}) &= \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\rho_k \\right] = \\frac{\\gamma_0}{n} \\left( 1+ 2\\left( 1-\\frac{1}{n}\\right)\\rho_1 \\right) \\\\\n&= \\frac{\\gamma_0}{n} \\left( 1+ 2\\left( 1-\\frac{1}{n}\\right)(-0.4) \\right) \\\\\n&\\approx \\frac{\\gamma_0}{n}(1-2\\cdot 0.4) = \\frac{\\gamma_0}{5n} \\quad (\\text{for large }n)\n\\end{split}\n\\] So for large \\(n\\), \\(\\operatorname{Var}(\\overline{Y})\\) is approximately \\(5\\) times smaller than the variance of the iid case.\n(Note: this is a mean-reverting time series.)\nSince \\((Y_t)\\) is \\(q\\)-dependent (with \\(q=1\\)), the \\(q\\)-dependent CLT works. So we know \\(\\overline{Y}\\) is approximately normally distributed for large \\(n\\). Then we can construct confidence interval for \\(\\mu\\): \\[\n\\mu\\in \\left[\\overline{Y} \\pm 2\\sqrt{\\operatorname{Var}(\\overline{Y})} \\right] = \\left[\\overline{Y} \\pm 2\\sqrt{\\frac{\\gamma_0}{5n}} \\right], \\text{ with prob } 95\\% .\n\\]",
    "crumbs": [
      "Lecture 8"
    ]
  },
  {
    "objectID": "TSA-Lecture08.html#example-ar1",
    "href": "TSA-Lecture08.html#example-ar1",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "2.2 Example: AR(\\(1\\))",
    "text": "2.2 Example: AR(\\(1\\))\nConsider a causal AR(\\(1\\)) (with \\(|\\phi|&lt;1\\)): \\[\nY_t - \\phi Y_{t-1} = e_t .\n\\] For causal AR(\\(1\\)), we have \\(\\rho_k = \\phi^k\\) (for \\(k\\ge 0\\)). So \\[\n\\operatorname{Var}(\\overline{Y}) = \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\rho_k \\right] = \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\phi^k \\right].\n\\] For large \\(n\\), we have the following approximation \\[\n\\operatorname{Var}(\\overline{Y}) = \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\phi^k \\right] \\approx \\frac{\\gamma_0}{n} \\left[ 1 + 2 \\sum_{k=1}^\\infty \\phi^k \\right] = \\frac{\\gamma_0}{n} \\frac{1+\\phi}{1-\\phi}.\n\\] Exercise: verify the last step \\(1 + 2 \\sum_{k=1}^\\infty \\phi^k = \\frac{1+\\phi}{1-\\phi}\\).\nFor example, if \\(\\phi=0.9\\), then \\(\\frac{1+\\phi}{1-\\phi}=19 \\approx 20\\), so for large \\(n\\), \\(\\operatorname{Var}(\\overline{Y}) \\approx \\frac{20 \\gamma_0}{n}\\) which is approximately \\(20\\) times larger than the variance of the iid case.\nRemark: This example is a ``mean-avoiding” time series. We can compare it with the previous MA(\\(1\\)) with \\(\\theta=0.5\\) example which we remarked as a mean-reverting time series, and we got \\(\\operatorname{Var}(\\overline{Y}) \\approx 0.2\\cdot \\frac{\\gamma_0}{n}\\). If we look at the formula for \\(\\operatorname{Var}(\\overline{Y})\\), the main difference is from the ACFs. In last example, \\(\\rho_1= -0.4\\) is negative and \\(\\rho_k=0\\) for \\(k\\ge2\\), which made \\(\\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\phi^k \\right] \\approx 0.2\\). In this example, \\(\\rho_k =0.9^k\\) gives \\(\\left[ 1 + 2 \\sum_{k=1}^n \\left( 1 - \\frac{k}{n} \\right) \\phi^k \\right] \\approx 20\\).",
    "crumbs": [
      "Lecture 8"
    ]
  },
  {
    "objectID": "TSA-Lecture08.html#example-random-walk",
    "href": "TSA-Lecture08.html#example-random-walk",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "2.3 Example: random walk",
    "text": "2.3 Example: random walk\nConsider the random walk model \\[\nY_1=e_1, \\quad Y_t = Y_{t-1} + e_t, \\quad e_t \\sim \\mathrm{iid}(0,\\sigma_e^2).\n\\] Suppose we still want to look at the sample mean \\(\\overline{Y}\\). Note that the random walk \\((Y_t)\\) is not stationary, the earlier formula for \\(\\operatorname{Var}(\\overline{Y})\\) cannot be applied here. We can directly calculate the variance. Note that \\(Y_1 = e_1\\), \\(Y_2 = e_1+e_2\\),… \\[\n\\begin{split}\n\\operatorname{Var}(\\overline{Y}) &= \\frac{1}{n^2} \\operatorname{Var}\\left( \\sum_{t=1}^n Y_t \\right)\n= \\frac{1}{n^2} \\operatorname{Var}\\left( \\sum_{t=1}^n \\sum_{j=1}^t e_j \\right) \\\\\n&= \\frac{1}{n^2} \\operatorname{Var} \\left( \\sum_{t=1}^n (n-t+1) e_t \\right) = \\frac{1}{n^2} \\sum_{t=1}^n (n-t+1)^2 \\sigma_e^2 \\\\\n&= \\frac{\\sigma_e^2}{n^2} \\sum_{j=1}^n j^2 = \\sigma_e^2\\cdot \\frac{n (n+1)(2n+1)}{6 n^2} \\to \\infty, \\text{ as } n\\to\\infty\n\\end{split}\n\\] So as the sample size increases, we are less and less certain about the mean of a random walk.",
    "crumbs": [
      "Lecture 8"
    ]
  },
  {
    "objectID": "TSA-Lecture08.html#linear-trend",
    "href": "TSA-Lecture08.html#linear-trend",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "4.1 Linear trend",
    "text": "4.1 Linear trend\nConsider the linear regression model. Assume \\[\n\\mu_t = \\beta_0 + \\beta_1 t.\n\\] \\(\\left( Y_t \\right)_{t=1}^n\\) are observed data. To estimate \\(\\mu_t\\) via linear regression, we can minimize the objective function \\[\nQ(\\beta_0, \\beta_1) = \\sum_{t=1}^n \\left( Y_t - \\beta_0 - \\beta_1 t \\right)^2.\n\\] Solve the equations \\[\n\\begin{cases}\n0= \\frac{\\partial Q}{\\partial \\beta_0}\n= -2 \\sum_{t=1}^n \\left( Y_t - \\beta_0 - \\beta_1 t \\right)  \\\\\n0=\\frac{\\partial Q}{\\partial \\beta_1}\n= -2 \\sum_{t=1}^n \\left( Y_t - \\beta_0 - \\beta_1 t \\right) t\n\\end{cases}\n\\] we get the minimizer \\((\\widehat{\\beta}_0,\\, \\widehat{\\beta}_1) = \\underset{\\beta_0,\\beta_1}{\\arg\\min} Q(\\beta_0, \\beta_1)\\) is \\[\n\\widehat{\\beta}_1 = \\frac{\\sum_{t=1}^n (Y_t - \\overline{Y}) (t - \\overline{t})}{\\sum_{t=1}^n (t - \\overline{t})^2},\n\\quad\n\\widehat{\\beta}_0 = \\overline{Y} - \\widehat{\\beta}_1 \\overline{t}.\n\\] Then the estimated linear trend is \\[\n\\widehat{\\mu}_t = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 t.\n\\]",
    "crumbs": [
      "Lecture 8"
    ]
  },
  {
    "objectID": "TSA-Lecture08.html#some-other-regression-models",
    "href": "TSA-Lecture08.html#some-other-regression-models",
    "title": "25 Spring 439/639 TSA: Lecture 8",
    "section": "4.2 Some other regression models",
    "text": "4.2 Some other regression models\n\nQuadratic trend: Assume\n\n\\[\n\\mu_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2\n= \\begin{bmatrix} 1 & t & t^2 \\end{bmatrix}\n  \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\n\\] This is a special case of polynomial trends. Any polynomial trend can be estimated via linear regression.\n\nCosine trend: Assume\n\n\\[\n\\mu_t = \\beta_0 + \\beta_1 \\cos \\left( \\frac{2\\pi}{f} t \\right)\n+ \\beta_2 \\sin \\left( \\frac{2\\pi}{f} t \\right)\n\\] where \\(f\\) is frequency.\n\nSeasonal/Cyclical trend: Assume \\(\\mu_t\\) is periodic. For example, suppose \\(t\\) denotes the month, and we assume ``the means are the same for the same months”, i.e., \\[\n\\begin{split}\n&\\mu_1 = \\mu_{13} = \\mu_{25} = \\mu_{1+12k} \\quad (\\text{Jan}) \\\\\n&\\mu_2 = \\mu_{14} = \\mu_{2+12k} \\quad (\\text{Feb}) \\\\\n&\\cdots \\\\\n&\\mu_{12} = \\mu_{24}= \\mu_{12k} \\quad (\\text{Dec})\n\\end{split}\n\\] To estimate \\((\\mu_1,...,\\mu_{12})\\), we can fit the following linear regression using the observed \\((Y_t)\\) \\[\nY_t = \\beta_1 X_{\\mathrm{Jan}} + \\beta_2 X_{\\mathrm{Feb}} + \\cdots + \\beta_{12} X_{\\mathrm{Dec}} + \\varepsilon_t\n\\] where the indicator/dummy variables \\(X_{\\mathrm{month}}\\) are defined as \\[\nX_{\\mathrm{Jan}} =\n\\begin{cases}\n1,& \\text{if } t \\text{ is January}\\\\\n0,& \\text{otherwise}\n\\end{cases}\n\\] After solving this linear regression, we get estimates \\((\\widehat{\\beta}_1,...,\\widehat{\\beta}_{12})\\). Then we let \\((\\widehat{\\mu}_1,...,\\widehat{\\mu}_{12}) = (\\widehat{\\beta}_1,...,\\widehat{\\beta}_{12})\\).",
    "crumbs": [
      "Lecture 8"
    ]
  },
  {
    "objectID": "TSA-Lecture07.html",
    "href": "TSA-Lecture07.html",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "",
    "text": "Recall last time, for the ARMA(\\(1,1\\)) \\[\nY_t - \\phi Y_{t-1} = e_t - \\theta e_{t-1},\n\\] we derived a causal GLP (also an MA(\\(\\infty\\))) representation for \\((Y_t)\\) \\[\nY_t = e_t + \\sum_{k=1}^{\\infty} \\phi^{k-1} (\\phi - \\theta) e_{t-k}\n\\] whenever the ARMA(\\(1,1\\)) is causal (only if \\(|\\phi|&lt;1\\)). We will use the GLP representation above to find the ACVFs for \\((Y_t)\\) through the Yule-Walker method.\nFor \\(k\\ge 0\\), multiply \\(Y_{t-k}\\) on both sides and take the expectations. \\[\n\\begin{split}\nY_t - \\phi Y_{t-1} &= e_t - \\theta e_{t-1},\\\\\nY_t Y_{t-k} - \\phi Y_{t-1} Y_{t-k} &= e_t Y_{t-k} - \\theta e_{t-1} Y_{t-k}, \\\\\n\\mathbb{E}[Y_t Y_{t-k}] - \\phi \\mathbb{E}[Y_{t-1} Y_{t-k}] &= \\mathbb{E}[e_t Y_{t-k}] - \\theta \\mathbb{E}[e_{t-1} Y_{t-k}] .\n\\end{split}\n\\] As before, since \\((Y_n)\\) is mean zero and stationary, \\(\\mathbb{E}[Y_{t} Y_{t-k}] = \\gamma_{k}\\) and \\(\\mathbb{E}[Y_{t-1} Y_{t-k}] = \\gamma_{k-1}\\). So the \\(k\\)-th YW equation is \\[\n\\gamma_{k} - \\phi \\gamma_{k-1}= \\mathbb{E}[e_t Y_{t-k}] - \\theta \\mathbb{E}[e_{t-1} Y_{t-k}].\n\\] Assuming causality, we can use the previous causal GLP, then \\[\n\\gamma_{k} - \\phi \\gamma_{k-1}= \\mathbb{E}[e_t (e_{t-k}+ \\psi_1 e_{t-k-1 + \\cdots})] - \\theta \\mathbb{E}[e_{t-1} (e_{t-k}+ \\psi_1 e_{t-k-1 + \\cdots})].\n\\] Consider all possible \\(k\\ge 0\\): \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 - \\theta \\psi_1 \\sigma_e^2, &0\\text{th YW eq}\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = 0 - \\theta \\sigma_e^2, &1\\text{st YW eq}\\\\\n\\gamma_{k} - \\phi \\gamma_{k-1}= 0 - \\theta 0 = 0, &k\\text{-th YW eq, for } k\\ge 2\n\\end{cases}\n\\] Recall that \\(\\psi_1 = \\phi-\\theta\\), from the first 2 YW equations we have \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 - \\theta (\\phi-\\theta) \\sigma_e^2 \\quad &(\\text{YW}0)\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = - \\theta \\sigma_e^2 &(\\text{YW}1)\n\\end{cases}\n\\] \\((\\text{YW}0) + \\phi(\\text{YW}1)\\): \\[\n(1 - \\phi^2)\\gamma_0 = (1 - 2\\theta\\phi + \\theta^2)\\sigma_e^2\n\\implies \\gamma_0 = \\frac{1 - 2\\theta\\phi + \\theta^2}{1 - \\phi^2}\\sigma_e^2 .\n\\] Then plug it into \\((\\text{YW}1)\\): \\[\n\\begin{split}\n\\gamma_1 &= \\phi\\gamma_0 - \\theta\\sigma_e^2\n= \\sigma_e^2 \\left( \\frac{\\phi(1-2\\theta\\phi+\\theta^2)}{1-\\phi^2}\n  - \\frac{\\theta (1-\\phi^2)}{1-\\phi^2} \\right) \\\\\n&= \\sigma_e^2 \\left( \\frac{\\phi - 2\\theta\\phi^2 + \\phi\\theta^2 - \\theta + \\theta\\phi^2}\n       {1-\\phi^2} \\right) \\\\\n&= \\sigma_e^2 \\frac{\\phi - \\theta + \\theta\\phi(\\theta-\\phi)}{1-\\phi^2}\n= \\sigma_e^2 \\frac{(\\phi-\\theta)(1-\\theta\\phi)}{1-\\phi^2} .\n\\end{split}\n\\] Use the \\(k\\)-th YW equations (for \\(k\\ge 2\\)) recursively: \\[\n\\gamma_k = \\phi^{k-1} \\frac{(\\phi - \\theta)(1 - \\theta\\phi)}{1 - \\phi^2} \\sigma_e^2, \\quad \\forall k\\ge 1.\n\\] We can observe that, \\(\\gamma_k \\to 0\\) exponentially as \\(k\\to \\infty\\) (since \\(|\\phi|&lt;1\\) under causality condition).\n\n\n\nBy the invertiblity condition (see last lecture), we need all the roots of MA polynomial are outside the unit disk, which reduces to \\(|\\theta| &lt;1\\) for ARMA(\\(1,1\\)). When invertibility holds, we have the following invertible representation (also an AR(\\(\\infty\\))) for ARMA(\\(1,1\\)): \\[\ne_t = Y_t + \\sum_{j=1}^{\\infty} \\theta^{j-1} (\\theta - \\phi) Y_{t-j} .\n\\] Exercise: derive the formula above, and verify \\(\\sum_{j=0}^{\\infty} |\\pi_j| &lt; \\infty\\) in this invertible representation.",
    "crumbs": [
      "Lecture 7"
    ]
  },
  {
    "objectID": "TSA-Lecture07.html#acvf-of-arma11",
    "href": "TSA-Lecture07.html#acvf-of-arma11",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "",
    "text": "Recall last time, for the ARMA(\\(1,1\\)) \\[\nY_t - \\phi Y_{t-1} = e_t - \\theta e_{t-1},\n\\] we derived a causal GLP (also an MA(\\(\\infty\\))) representation for \\((Y_t)\\) \\[\nY_t = e_t + \\sum_{k=1}^{\\infty} \\phi^{k-1} (\\phi - \\theta) e_{t-k}\n\\] whenever the ARMA(\\(1,1\\)) is causal (only if \\(|\\phi|&lt;1\\)). We will use the GLP representation above to find the ACVFs for \\((Y_t)\\) through the Yule-Walker method.\nFor \\(k\\ge 0\\), multiply \\(Y_{t-k}\\) on both sides and take the expectations. \\[\n\\begin{split}\nY_t - \\phi Y_{t-1} &= e_t - \\theta e_{t-1},\\\\\nY_t Y_{t-k} - \\phi Y_{t-1} Y_{t-k} &= e_t Y_{t-k} - \\theta e_{t-1} Y_{t-k}, \\\\\n\\mathbb{E}[Y_t Y_{t-k}] - \\phi \\mathbb{E}[Y_{t-1} Y_{t-k}] &= \\mathbb{E}[e_t Y_{t-k}] - \\theta \\mathbb{E}[e_{t-1} Y_{t-k}] .\n\\end{split}\n\\] As before, since \\((Y_n)\\) is mean zero and stationary, \\(\\mathbb{E}[Y_{t} Y_{t-k}] = \\gamma_{k}\\) and \\(\\mathbb{E}[Y_{t-1} Y_{t-k}] = \\gamma_{k-1}\\). So the \\(k\\)-th YW equation is \\[\n\\gamma_{k} - \\phi \\gamma_{k-1}= \\mathbb{E}[e_t Y_{t-k}] - \\theta \\mathbb{E}[e_{t-1} Y_{t-k}].\n\\] Assuming causality, we can use the previous causal GLP, then \\[\n\\gamma_{k} - \\phi \\gamma_{k-1}= \\mathbb{E}[e_t (e_{t-k}+ \\psi_1 e_{t-k-1 + \\cdots})] - \\theta \\mathbb{E}[e_{t-1} (e_{t-k}+ \\psi_1 e_{t-k-1 + \\cdots})].\n\\] Consider all possible \\(k\\ge 0\\): \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 - \\theta \\psi_1 \\sigma_e^2, &0\\text{th YW eq}\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = 0 - \\theta \\sigma_e^2, &1\\text{st YW eq}\\\\\n\\gamma_{k} - \\phi \\gamma_{k-1}= 0 - \\theta 0 = 0, &k\\text{-th YW eq, for } k\\ge 2\n\\end{cases}\n\\] Recall that \\(\\psi_1 = \\phi-\\theta\\), from the first 2 YW equations we have \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 - \\theta (\\phi-\\theta) \\sigma_e^2 \\quad &(\\text{YW}0)\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = - \\theta \\sigma_e^2 &(\\text{YW}1)\n\\end{cases}\n\\] \\((\\text{YW}0) + \\phi(\\text{YW}1)\\): \\[\n(1 - \\phi^2)\\gamma_0 = (1 - 2\\theta\\phi + \\theta^2)\\sigma_e^2\n\\implies \\gamma_0 = \\frac{1 - 2\\theta\\phi + \\theta^2}{1 - \\phi^2}\\sigma_e^2 .\n\\] Then plug it into \\((\\text{YW}1)\\): \\[\n\\begin{split}\n\\gamma_1 &= \\phi\\gamma_0 - \\theta\\sigma_e^2\n= \\sigma_e^2 \\left( \\frac{\\phi(1-2\\theta\\phi+\\theta^2)}{1-\\phi^2}\n  - \\frac{\\theta (1-\\phi^2)}{1-\\phi^2} \\right) \\\\\n&= \\sigma_e^2 \\left( \\frac{\\phi - 2\\theta\\phi^2 + \\phi\\theta^2 - \\theta + \\theta\\phi^2}\n       {1-\\phi^2} \\right) \\\\\n&= \\sigma_e^2 \\frac{\\phi - \\theta + \\theta\\phi(\\theta-\\phi)}{1-\\phi^2}\n= \\sigma_e^2 \\frac{(\\phi-\\theta)(1-\\theta\\phi)}{1-\\phi^2} .\n\\end{split}\n\\] Use the \\(k\\)-th YW equations (for \\(k\\ge 2\\)) recursively: \\[\n\\gamma_k = \\phi^{k-1} \\frac{(\\phi - \\theta)(1 - \\theta\\phi)}{1 - \\phi^2} \\sigma_e^2, \\quad \\forall k\\ge 1.\n\\] We can observe that, \\(\\gamma_k \\to 0\\) exponentially as \\(k\\to \\infty\\) (since \\(|\\phi|&lt;1\\) under causality condition).",
    "crumbs": [
      "Lecture 7"
    ]
  },
  {
    "objectID": "TSA-Lecture07.html#invertible-representation-for-arma11",
    "href": "TSA-Lecture07.html#invertible-representation-for-arma11",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "",
    "text": "By the invertiblity condition (see last lecture), we need all the roots of MA polynomial are outside the unit disk, which reduces to \\(|\\theta| &lt;1\\) for ARMA(\\(1,1\\)). When invertibility holds, we have the following invertible representation (also an AR(\\(\\infty\\))) for ARMA(\\(1,1\\)): \\[\ne_t = Y_t + \\sum_{j=1}^{\\infty} \\theta^{j-1} (\\theta - \\phi) Y_{t-j} .\n\\] Exercise: derive the formula above, and verify \\(\\sum_{j=0}^{\\infty} |\\pi_j| &lt; \\infty\\) in this invertible representation.",
    "crumbs": [
      "Lecture 7"
    ]
  },
  {
    "objectID": "TSA-Lecture07.html#find-the-causal-glp-representation",
    "href": "TSA-Lecture07.html#find-the-causal-glp-representation",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "2.1 Find the causal GLP representation",
    "text": "2.1 Find the causal GLP representation\nConsider the ARMA(\\(p,q\\)) \\[\nY_t - \\phi_1 Y_{t-1} - \\cdots - \\phi_p Y_{t-p} = e_t - \\theta_1 e_{t-1} - \\cdots - \\theta_q e_{t-q} .\n\\] Assume the causality condition holds, i.e., assume all the roots of the AR polynomial are outside of the unit disk. So there exists a causal GLP representation \\(Y_t = \\sum_{j=0}^{\\infty} \\psi_j e_{t-j}\\). Our goal is to find \\(\\{\\psi_j\\}\\).\nWe can simply plug it into the ARMA(\\(p,q\\)) equation \\[\n\\begin{split}\n&\\left( \\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots \\right)\n  - \\phi_1 \\left( \\psi_0 e_{t-1} + \\psi_1 e_{t-2} + \\psi_2 e_{t-3} + \\cdots \\right) - \\cdots - \\phi_p \\left( \\psi_0 e_{t-p} + \\psi_1 e_{t-p-1} + \\psi_2 e_{t-p-2} + \\cdots \\right) \\\\\n&= e_t - \\theta_1 e_{t-1} - \\theta_2 e_{t-2} - \\cdots - \\theta_q e_{t-q} .\n\\end{split}\n\\] Then compare the coefficients for \\(e_{t-k}\\) on both sides: \\[\n\\begin{cases}\n\\psi_0 = 1 \\\\\n\\psi_1 - \\phi_1 \\psi_0 = -\\theta_1 \\\\\n\\psi_2 - \\phi_1 \\psi_1 - \\phi_2 \\psi_0 = -\\theta_2 \\\\\n\\psi_3 - \\phi_1 \\psi_2 - \\phi_2 \\psi_1 - \\phi_3 \\psi_0 = -\\theta_3 \\\\\n\\quad \\cdots \\\\\n\\psi_j - \\phi_1 \\psi_{j-1} - \\phi_2 \\psi_{j-2} - \\cdots - \\phi_p \\psi_{j-p} = 0, \\text{ for large } j \\text{ such that } j\\ge p \\text{ and } j&gt;q\n\\end{cases}\n\\] So we get \\[\n\\begin{cases}\n\\psi_0 = 1 \\\\\n\\psi_1 = \\phi_1 - \\theta_1 \\\\\n\\psi_2 = \\phi_1 \\psi_1 + \\phi_2 - \\theta_2 \\\\\n\\psi_3 = \\phi_1 \\psi_2 + \\phi_2 \\psi_1 + \\phi_3 - \\theta_3 \\\\\n\\quad\\cdots \\\\\n\\psi_j = \\phi_1 \\psi_{j-1} + \\phi_2 \\psi_{j-2} + \\cdots + \\phi_p \\psi_{j-p}, \\text{ for large } j \\text{ such that } j\\ge p \\text{ and } j&gt;q\n\\end{cases}\n\\] which can be seen as recursive equations for \\(\\{\\psi_j\\}\\).",
    "crumbs": [
      "Lecture 7"
    ]
  },
  {
    "objectID": "TSA-Lecture07.html#yw-approach-for-acvf",
    "href": "TSA-Lecture07.html#yw-approach-for-acvf",
    "title": "25 Spring 439/639 TSA: Lecture 7",
    "section": "2.2 YW approach for ACVF",
    "text": "2.2 YW approach for ACVF\nThe basic idea of YW method is same as before, assume causality and stationarity. For any \\(k&gt;q\\), the \\(k\\)-th YW equations is \\[\n\\begin{split}\n\\mathbb{E}[Y_t Y_{t-k}] - \\phi_1 \\mathbb{E}[Y_{t-1} Y_{t-k}] - \\cdots - \\phi_p \\mathbb{E}[Y_{t-p} Y_{t-k}] &= \\mathbb{E}[e_t Y_{t-k}] - \\theta_1 \\mathbb{E}[e_{t-1} Y_{t-k}] - \\cdots - \\theta_q \\mathbb{E}[e_{t-q} Y_{t-k}] \\\\\n\\gamma_{k} - \\phi_1 \\gamma_{k-1} - \\cdots - \\phi_p \\gamma_{k-p} &= 0, \\quad \\forall k&gt;q\n\\end{split}\n\\] So the recursion part of YW equations have the structure as \\(AR(p)\\). Using the earlier results, suppose the \\(p\\) roots (assuming \\(\\phi_p \\ne 0\\)) of the AR polynomial are \\(z_1,...,z_p\\) and they are all distinct, then there exist \\(p\\) complex numbers \\(A_1,...,A_p\\) such that \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\n\\] hold for all \\(k \\ge q+1-p\\).\n\nIf \\(p\\ge q\\): we need to solve \\((\\gamma_0,...,\\gamma_p)\\) from the first \\(p+1\\) YW equations (YW eq\\(0\\), …, YW eq\\(p\\)), then we can use \\((\\gamma_{1},...,\\gamma_{p})\\) as the initial conditions to determine \\((A_1,...,A_p)\\).\nIf \\(q&gt; p\\): we need to solve \\((\\gamma_0,...,\\gamma_q)\\) from the first \\(q+1\\) YW equations (YW eq\\(0\\), …, YW eq\\(q\\)), then use \\((\\gamma_{q-p+1},...,\\gamma_{q})\\) as the initial conditions to determine \\((A_1,...,A_p)\\).\n\nRemark: dividing the YW equation by \\(\\gamma_0\\) gives \\(\\rho_{k} - \\phi_1 \\rho_{k-1} - \\cdots - \\phi_p \\rho_{k-p} = 0\\) for \\(k&gt;q\\). So the ACF has the same structure \\(\\rho_k = \\widetilde{A}_1 z_1^{-k} + \\widetilde{A}_2 z_2^{-k} + \\cdots + \\widetilde{A}_p z_p^{-k}\\).\nRemark: if the \\(p\\) roots are not distinct, ACVF/ACF have more complicated formula as we discussed before (see last lecture).",
    "crumbs": [
      "Lecture 7"
    ]
  },
  {
    "objectID": "TSA-Lecture06.html",
    "href": "TSA-Lecture06.html",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "",
    "text": "Last time we claimed that: Suppose \\(z_1,...,z_p\\) are roots of the AR polynomial \\(\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p\\), and assume these roots are distinct. Then there exist complex numbers \\(A_1,...,A_p\\), such that the solution to \\[\n\\begin{cases}\n\\text{initial conditions for } (\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\\\\n\\text{recursion equation: } \\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2} + \\cdots + \\phi_p \\gamma_{k-p}, \\quad \\forall k\\ge p\n\\end{cases}\n\\] is given by \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}, \\quad \\forall k \\ge 0 .\n\\]\nHere we prove part of this claim. We can verify \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) (\\(\\forall k \\ge 0\\)) indeed satisfy the recursion, by plugging it into the recursion equations. Suppose \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) for some \\(A_1,...,A_p\\). Then for any \\(k\\ge p\\), \\[\n\\begin{split}\n& \\gamma_k - \\phi_1 \\gamma_{k-1} - \\phi_2 \\gamma_{k-2} - \\cdots - \\phi_p \\gamma_{k-p} \\\\\n=& \\left(A_1 z_1^{-k} + \\cdots + A_p z_p^{-k} \\right) - \\phi_1 \\left(A_1 z_1^{-k+1} + \\cdots + A_p z_p^{-k+1} \\right) - \\phi_2 \\left(A_1 z_1^{-k+2} + \\cdots + A_p z_p^{-k+2} \\right) \\\\\n&- \\cdots - \\phi_p \\left(A_1 z_1^{-k+p} + \\cdots + A_p z_p^{-k+p} \\right) \\\\\n=& A_1 z_1^{-k} (1 - \\phi_1 z_1^1 - \\phi_2 z_1^2 - \\cdots - \\phi_p z_1^p) + \\cdots + A_p z_p^{-k} (1 - \\phi_1 z_p^1 - \\phi_2 z_p^2 - \\cdots - \\phi_p z_p^p) \\\\\n=& A_1 z_1^{-k} \\Phi(z_1) + \\cdots + A_p z_p^{-k} \\Phi(z_p) = 0.\n\\end{split}\n\\] So the constructed \\(\\{\\gamma_k\\}\\) satisfy the recursion equations.\nTo determine the \\((A_1,...,A_p)\\), we use the initial conditions for \\((\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\) (solved from the first \\(p+1\\) YW equations) to solve \\((A_1,...,A_p)\\).\n\n\n\nAssume \\(\\phi_1,\\phi_2 \\in \\mathbb{R}\\) and \\(\\phi_2 \\neq 0\\). Consider the AR(\\(2\\)) equation: \\[\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t .\n\\] The AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x - \\phi_2 x^2\\), which has two roots \\[\nz_{1,2} = \\frac{-\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{2\\phi_2} .\n\\] There are 3 cases for the roots \\(z_1,z_2\\):\n\n\ntwo distinct real roots.\n\n\ntwo repeated real roots. (Note: we can show that if \\(z_1=z_2\\), then they must be real.)\n\n\ntwo distinct (non-real) complex roots.\n\n\nCase (a): \\(z_1, z_2\\in \\mathbb{R}\\) and \\(z_1 \\neq z_2\\). By the claim above, \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k}, \\quad \\forall k \\ge 0 .\n\\] We can use \\(\\gamma_0,\\gamma_1\\) to determine \\((A_1,A_2)\\).\nCase (b): \\(z_1= z_2\\in \\mathbb{R}\\). We claim without proof that, in this case, there exist complex numbers \\((A_1,A_2)\\) such that \\[\n\\gamma_k = (A_1 + A_2 k) z_1^{-k} , \\quad \\forall k \\ge 0 .\n\\] Exercise: verify that \\(\\gamma_k = (A_1 + A_2 k) z_1^{-k}\\) satisfy the recursion equation \\(\\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2}\\).\nIn this case, we still have \\(\\gamma_k \\to 0\\) as \\(k\\to \\infty\\) (assuming \\(|z_1|&gt;1\\), i.e., the causality condition holds) and it decays exponentially.\nWe can also derive the following result: \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\left(1+ \\frac{1+\\phi_2}{1-\\phi_2} k \\right) \\left(\\frac{\\phi_1}{2} \\right)^k,\n\\] and we also have \\(\\rho_k \\to 0\\) exponentially as \\(k\\to \\infty\\) under the causality condition.\nCase (c): \\(z_1 \\neq z_2\\) and \\(z_1,z_2 \\notin \\mathbb{R}\\). In this case, \\(z_1,z_2 \\in \\mathbb{C}\\), and \\(z_2 = \\overline{z_1}\\) since \\(\\phi_1,\\phi_2 \\in \\mathbb{R}\\). By the earlier claim, there exist \\(A_1,A_2 \\in \\mathbb{C}\\) such that \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k}, \\quad \\forall k \\ge 0 .\n\\] We can show that \\(A_2 = \\overline{A_1}\\) using the fact that all the ACVFs \\(\\gamma_k\\) are real.\nWe can also derive the following result: \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = R^k \\cdot \\frac{\\sin(k\\Theta + \\Phi)}{\\sin(\\Phi)} ,\n\\] where the amplitude term \\(R = \\sqrt{-\\phi_2}\\), the frequency term \\(\\Theta\\) satisfies \\(\\cos \\Theta = \\frac{\\phi_1}{2}/ \\sqrt{-\\phi_2}\\), and the phase term \\(\\Phi\\) satisfies \\(\\tan \\Phi = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2}}{\\phi_1} \\cdot \\frac{1-\\phi_2}{1+\\phi_2}\\).\nFrom this result, we can see that under the causality condition, \\(\\{\\gamma_k\\}\\) and \\(\\{\\rho_k\\}\\) both converge to \\(0\\) exponentially as \\(k\\to \\infty\\), since \\(|R|&lt;1\\) (see the exercise below) and \\(\\sin(k\\Theta + \\Phi)\\) is bounded.\nNote: A small issue here is \\(\\Theta\\) and \\(\\Phi\\) are not uniquely determined (modulo \\(2\\pi\\)) by \\(\\cos \\Theta\\) and \\(\\tan \\Phi\\). To make them well defined, they should also satisfy \\(\\sin \\Theta = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\sqrt{-\\phi_2}\\) (or \\(\\tan \\Theta = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\frac{\\phi_1}{2}\\)) and \\(\\sin \\Phi = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\sqrt{\\frac{\\phi_2 (\\phi_1^2 - (1-\\phi_2)^2)}{(1-\\phi_2)^2}}\\) (or \\(\\cos \\Phi = \\frac{\\phi_1(1+\\phi_2)}{2(1-\\phi_2)} \\Big/ \\sqrt{\\frac{\\phi_2 (\\phi_1^2 - (1-\\phi_2)^2)}{(1-\\phi_2)^2}}\\)).\nNote: Our results are similar to the textbook (page 73 of Cryan and Chan) with slight difference in \\(\\tan \\Phi\\).\nExercise: Why \\(-\\phi_2 &gt;0\\)? Why \\(|R|&lt;1\\) under the causality condition? (A harder exercise: try to derive the results above.)\n\n\n\n\nFor AR(\\(p\\)), if the AR polynomial has one root \\(z_1\\) with multiplicity \\(r\\) (i.e. this root repeated \\(r\\) times, \\(z_1 = \\cdots = z_r\\)) and all the other roots are distinct (\\(z_1, z_{r+1}, z_{r+2},\\dots,z_p\\) are distinct), then the ACVFs are in the following form: \\[\n\\gamma_k = (A_1 + A_2 k + \\cdots + A_r k^{r-1}) z_1^{-k} + A_{r+1} z_{r+1}^{-k} + \\cdots + A_{p} z_{p}^{-k}, \\quad \\forall k \\ge 0 .\n\\] which is analogous to a combination of the repeated roots case (see case (b)) and the distinct roots case (the claim at the beginning, or cases (a)(c)).\nFor a generic AR(\\(p\\)), the AR polynomial may have different roots with various multiplicities (the previous case is a special example where the multiplicities of all distinct roots are \\((r,1,\\dots,1)\\)). Then the form of ACVF is a linear combination of cases (a)(b)(c) in a more general way.\nFor MA(\\(q\\)) process, we always have \\[\n\\gamma_k = 0, \\text{ for all } k\\ge q+1.\n\\]",
    "crumbs": [
      "Lecture 6"
    ]
  },
  {
    "objectID": "TSA-Lecture06.html#acvf-for-arp",
    "href": "TSA-Lecture06.html#acvf-for-arp",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "",
    "text": "Last time we claimed that: Suppose \\(z_1,...,z_p\\) are roots of the AR polynomial \\(\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p\\), and assume these roots are distinct. Then there exist complex numbers \\(A_1,...,A_p\\), such that the solution to \\[\n\\begin{cases}\n\\text{initial conditions for } (\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\\\\n\\text{recursion equation: } \\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2} + \\cdots + \\phi_p \\gamma_{k-p}, \\quad \\forall k\\ge p\n\\end{cases}\n\\] is given by \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}, \\quad \\forall k \\ge 0 .\n\\]\nHere we prove part of this claim. We can verify \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) (\\(\\forall k \\ge 0\\)) indeed satisfy the recursion, by plugging it into the recursion equations. Suppose \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) for some \\(A_1,...,A_p\\). Then for any \\(k\\ge p\\), \\[\n\\begin{split}\n& \\gamma_k - \\phi_1 \\gamma_{k-1} - \\phi_2 \\gamma_{k-2} - \\cdots - \\phi_p \\gamma_{k-p} \\\\\n=& \\left(A_1 z_1^{-k} + \\cdots + A_p z_p^{-k} \\right) - \\phi_1 \\left(A_1 z_1^{-k+1} + \\cdots + A_p z_p^{-k+1} \\right) - \\phi_2 \\left(A_1 z_1^{-k+2} + \\cdots + A_p z_p^{-k+2} \\right) \\\\\n&- \\cdots - \\phi_p \\left(A_1 z_1^{-k+p} + \\cdots + A_p z_p^{-k+p} \\right) \\\\\n=& A_1 z_1^{-k} (1 - \\phi_1 z_1^1 - \\phi_2 z_1^2 - \\cdots - \\phi_p z_1^p) + \\cdots + A_p z_p^{-k} (1 - \\phi_1 z_p^1 - \\phi_2 z_p^2 - \\cdots - \\phi_p z_p^p) \\\\\n=& A_1 z_1^{-k} \\Phi(z_1) + \\cdots + A_p z_p^{-k} \\Phi(z_p) = 0.\n\\end{split}\n\\] So the constructed \\(\\{\\gamma_k\\}\\) satisfy the recursion equations.\nTo determine the \\((A_1,...,A_p)\\), we use the initial conditions for \\((\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\) (solved from the first \\(p+1\\) YW equations) to solve \\((A_1,...,A_p)\\).",
    "crumbs": [
      "Lecture 6"
    ]
  },
  {
    "objectID": "TSA-Lecture06.html#acvf-for-ar2",
    "href": "TSA-Lecture06.html#acvf-for-ar2",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "",
    "text": "Assume \\(\\phi_1,\\phi_2 \\in \\mathbb{R}\\) and \\(\\phi_2 \\neq 0\\). Consider the AR(\\(2\\)) equation: \\[\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t .\n\\] The AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x - \\phi_2 x^2\\), which has two roots \\[\nz_{1,2} = \\frac{-\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{2\\phi_2} .\n\\] There are 3 cases for the roots \\(z_1,z_2\\):\n\n\ntwo distinct real roots.\n\n\ntwo repeated real roots. (Note: we can show that if \\(z_1=z_2\\), then they must be real.)\n\n\ntwo distinct (non-real) complex roots.\n\n\nCase (a): \\(z_1, z_2\\in \\mathbb{R}\\) and \\(z_1 \\neq z_2\\). By the claim above, \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k}, \\quad \\forall k \\ge 0 .\n\\] We can use \\(\\gamma_0,\\gamma_1\\) to determine \\((A_1,A_2)\\).\nCase (b): \\(z_1= z_2\\in \\mathbb{R}\\). We claim without proof that, in this case, there exist complex numbers \\((A_1,A_2)\\) such that \\[\n\\gamma_k = (A_1 + A_2 k) z_1^{-k} , \\quad \\forall k \\ge 0 .\n\\] Exercise: verify that \\(\\gamma_k = (A_1 + A_2 k) z_1^{-k}\\) satisfy the recursion equation \\(\\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2}\\).\nIn this case, we still have \\(\\gamma_k \\to 0\\) as \\(k\\to \\infty\\) (assuming \\(|z_1|&gt;1\\), i.e., the causality condition holds) and it decays exponentially.\nWe can also derive the following result: \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\left(1+ \\frac{1+\\phi_2}{1-\\phi_2} k \\right) \\left(\\frac{\\phi_1}{2} \\right)^k,\n\\] and we also have \\(\\rho_k \\to 0\\) exponentially as \\(k\\to \\infty\\) under the causality condition.\nCase (c): \\(z_1 \\neq z_2\\) and \\(z_1,z_2 \\notin \\mathbb{R}\\). In this case, \\(z_1,z_2 \\in \\mathbb{C}\\), and \\(z_2 = \\overline{z_1}\\) since \\(\\phi_1,\\phi_2 \\in \\mathbb{R}\\). By the earlier claim, there exist \\(A_1,A_2 \\in \\mathbb{C}\\) such that \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k}, \\quad \\forall k \\ge 0 .\n\\] We can show that \\(A_2 = \\overline{A_1}\\) using the fact that all the ACVFs \\(\\gamma_k\\) are real.\nWe can also derive the following result: \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = R^k \\cdot \\frac{\\sin(k\\Theta + \\Phi)}{\\sin(\\Phi)} ,\n\\] where the amplitude term \\(R = \\sqrt{-\\phi_2}\\), the frequency term \\(\\Theta\\) satisfies \\(\\cos \\Theta = \\frac{\\phi_1}{2}/ \\sqrt{-\\phi_2}\\), and the phase term \\(\\Phi\\) satisfies \\(\\tan \\Phi = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2}}{\\phi_1} \\cdot \\frac{1-\\phi_2}{1+\\phi_2}\\).\nFrom this result, we can see that under the causality condition, \\(\\{\\gamma_k\\}\\) and \\(\\{\\rho_k\\}\\) both converge to \\(0\\) exponentially as \\(k\\to \\infty\\), since \\(|R|&lt;1\\) (see the exercise below) and \\(\\sin(k\\Theta + \\Phi)\\) is bounded.\nNote: A small issue here is \\(\\Theta\\) and \\(\\Phi\\) are not uniquely determined (modulo \\(2\\pi\\)) by \\(\\cos \\Theta\\) and \\(\\tan \\Phi\\). To make them well defined, they should also satisfy \\(\\sin \\Theta = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\sqrt{-\\phi_2}\\) (or \\(\\tan \\Theta = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\frac{\\phi_1}{2}\\)) and \\(\\sin \\Phi = \\frac{\\sqrt{-\\phi_1^2 - 4\\phi_2} }{2} \\Big/ \\sqrt{\\frac{\\phi_2 (\\phi_1^2 - (1-\\phi_2)^2)}{(1-\\phi_2)^2}}\\) (or \\(\\cos \\Phi = \\frac{\\phi_1(1+\\phi_2)}{2(1-\\phi_2)} \\Big/ \\sqrt{\\frac{\\phi_2 (\\phi_1^2 - (1-\\phi_2)^2)}{(1-\\phi_2)^2}}\\)).\nNote: Our results are similar to the textbook (page 73 of Cryan and Chan) with slight difference in \\(\\tan \\Phi\\).\nExercise: Why \\(-\\phi_2 &gt;0\\)? Why \\(|R|&lt;1\\) under the causality condition? (A harder exercise: try to derive the results above.)",
    "crumbs": [
      "Lecture 6"
    ]
  },
  {
    "objectID": "TSA-Lecture06.html#some-other-remarks-on-acvfs",
    "href": "TSA-Lecture06.html#some-other-remarks-on-acvfs",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "",
    "text": "For AR(\\(p\\)), if the AR polynomial has one root \\(z_1\\) with multiplicity \\(r\\) (i.e. this root repeated \\(r\\) times, \\(z_1 = \\cdots = z_r\\)) and all the other roots are distinct (\\(z_1, z_{r+1}, z_{r+2},\\dots,z_p\\) are distinct), then the ACVFs are in the following form: \\[\n\\gamma_k = (A_1 + A_2 k + \\cdots + A_r k^{r-1}) z_1^{-k} + A_{r+1} z_{r+1}^{-k} + \\cdots + A_{p} z_{p}^{-k}, \\quad \\forall k \\ge 0 .\n\\] which is analogous to a combination of the repeated roots case (see case (b)) and the distinct roots case (the claim at the beginning, or cases (a)(c)).\nFor a generic AR(\\(p\\)), the AR polynomial may have different roots with various multiplicities (the previous case is a special example where the multiplicities of all distinct roots are \\((r,1,\\dots,1)\\)). Then the form of ACVF is a linear combination of cases (a)(b)(c) in a more general way.\nFor MA(\\(q\\)) process, we always have \\[\n\\gamma_k = 0, \\text{ for all } k\\ge q+1.\n\\]",
    "crumbs": [
      "Lecture 6"
    ]
  },
  {
    "objectID": "TSA-Lecture06.html#a-simple-example-arma11",
    "href": "TSA-Lecture06.html#a-simple-example-arma11",
    "title": "25 Spring 439/639 TSA: Lecture 6",
    "section": "3.1 A simple example: ARMA(\\(1,1\\))",
    "text": "3.1 A simple example: ARMA(\\(1,1\\))\nConsider the ARMA(\\(1,1\\)) model \\[\nY_t - \\phi Y_{t-1} = e_t - \\theta e_{t-1}.\n\\] Assume \\(\\phi \\ne \\theta\\).\nQuestion: what happens if \\(\\phi = \\theta\\)? (Answer: the ARMA equation \\((1-\\phi B)Y_t = (1-\\phi B)e_t\\) reduces to \\(Y_t=e_t\\), which is the white noise model.)\nWhen is this process \\((Y_t)\\) causal? By the causality condition above, we need to look at the AR polynomial \\(\\Phi(x) = 1-\\phi x\\). It has a single root \\(z= \\frac{1}{\\phi}\\). So the process is causal if and only if \\(|\\phi|&lt;1\\).\nHereafter, we only consider \\(|\\phi|&lt;1\\), which makes the process causal. Let’s derive the causal GLP representation of \\((Y_t)\\). Suppose \\(Y_t = \\sum_{j=0}^{\\infty} \\psi_j e_{t-j}\\). In last lecture, there were two methods to find \\(\\{\\psi_j\\}\\), (i) inverting the operator, or (ii) plugging into the equation. Here we can simply plug the desired GLP form into the ARMA equation \\(Y_t - \\phi Y_{t-1} = e_t - \\theta e_{t-1}\\): \\[\n\\left( \\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots \\right)\n- \\phi \\left( \\psi_0 e_{t-1} + \\psi_1 e_{t-2} + \\psi_2 e_{t-3} + \\cdots \\right)\n= e_t - \\theta e_{t-1} ,\n\\] \\[\n\\psi_0 e_t\n+ (\\psi_1 - \\phi \\psi_0) e_{t-1}\n+ (\\psi_2 - \\phi \\psi_1) e_{t-2}\n+ (\\psi_3 - \\phi \\psi_2) e_{t-3} + \\cdots\n= e_t - \\theta e_{t-1} .\n\\] So we get \\[\n\\begin{cases}\n\\psi_0 = 1 \\\\\n\\psi_1 - \\phi \\psi_0 = -\\theta \\quad \\implies \\quad \\psi_1 = \\phi - \\theta \\\\\n\\psi_{k} - \\phi \\psi_{k-1} = 0,\\ k \\geq 2\n\\quad \\implies \\quad\n\\psi_{k} = \\phi^{k-1} (\\phi - \\theta) \\text{ for } k \\geq 2 .\n\\end{cases}\n\\] So the GLP representation of ARMA(\\(1,1\\)) is \\[\nY_t = e_t + \\sum_{k=1}^{\\infty} \\phi^{k-1} (\\phi - \\theta) e_{t-k}.\n\\]\nExercise: verify that \\(\\sum_{j=0}^{\\infty} |\\psi_j| &lt; \\infty\\).\nNote: if \\(\\theta=0\\), then the GLP above becomes \\(Y_t = \\sum_{k=0}^{\\infty} \\phi^k e_{t-k}\\), which recovers the same result of AR(\\(1\\)).",
    "crumbs": [
      "Lecture 6"
    ]
  },
  {
    "objectID": "TSA-Lecture04.html",
    "href": "TSA-Lecture04.html",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "",
    "text": "Last time, we defined AR(\\(1\\)), autoregressive of order \\(1\\), as \\[\nY_t = \\phi Y_{t-1} + e_t, \\quad e_t \\sim \\text{iid}(0, \\sigma_e^2)\n\\] and we derived the GLP representaion of AR(1) \\[\nY_t = \\sum_{i=0}^{\\infty} \\phi^i e_{t-i}.\n\\]\n\n\nAssume \\(|\\phi|&lt;1\\). Using GLP results, we can get the mean function, variance function, ACVF, ACF of AR(1).\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = 0\\).\nVariance function: \\[\n\\sigma_t^2 = \\operatorname{Var}(Y_t) = \\sigma_e^2 \\sum_{j=0}^{\\infty} \\psi_j^2\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\phi^{2j} = \\frac{\\sigma_e^2}{1 - \\phi^2}.\n\\]\nACVF: \\[\n\\gamma_k = \\operatorname{Cov}(Y_t, Y_{t-k})\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\psi_j \\psi_{j+k}\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\phi^j \\phi^{j+k}\n= \\sigma_e^2 \\phi^k \\sum_{j=0}^{\\infty} \\phi^{2j}\n= \\frac{\\sigma_e^2 \\phi^k}{1 - \\phi^2} .\n\\]\nACF: \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\phi^k\\).\n\nRemark: \\(\\rho_k \\to 0\\) geometrically as \\(k \\to \\infty\\). And we can see AR(\\(1\\)) is not \\(q\\)-dependent for any finite \\(q\\), since \\(\\rho_k \\ne 0\\) for any \\(k\\) (consider generic \\(\\phi \\neq 0\\)).\nFurther Remark: All AR(\\(p\\)) have this type of behavior, \\(\\gamma_k\\) and \\(\\rho_k\\) decays geometrically.\n\n\n\nUsing the backshift operator \\(B\\) from last lecture, for AR(\\(1\\)), we have \\[\n\\begin{split}\nY_t = \\phi Y_{t-1} + e_t &\\implies e_t = Y_t - \\phi B Y_t = (1-\\phi B) Y_t \\\\\n& \\implies Y_t = (1-\\phi B)^{-1} e_t .\n\\end{split}\n\\] Note that the basic operation \\((1-x)^{-1} = \\frac{1}{1-x} = 1+x+x^2+\\cdots\\) for \\(|x|&lt;1\\). For \\(|\\phi|&lt;1\\), we can do a similar expansion for \\((1-\\phi B)^{-1}\\) here: \\[\n\\begin{split}\nY_t &= (1-\\phi B)^{-1} e_t\n= \\left(1 + \\phi B + (\\phi B)^2 + (\\phi B)^3 + \\cdots \\right) e_t \\\\\n& = \\left(1 + \\phi B + \\phi^2 B^2 + \\phi^3 B^3 + \\cdots \\right) e_t \\\\\n& = e_t + \\phi (B e_t) + \\phi^2 (B^2 e_t) + \\cdots \\\\\n& = e_t + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\cdots \\\\\n& = \\sum_{j=0}^{\\infty} \\phi^j e_{t-j}\n\\end{split}\n\\] which gives the same GLP we derived before.\n\n\n\nIn the previous derivation, we assumed \\(|\\phi|&lt;1\\). What happens if \\(|\\phi|&gt;1\\) for AR(\\(1\\))? In this case, we cannot directly expand the term \\((1-\\phi B)^{-1}\\) since it will not converge. In comparison to \\((1-x)^{-1} = 1+x+x^2+\\cdots\\) for \\(|x|&lt;1\\), we have the following convergent reformulation for the \\(|x|&gt;1\\) case: \\[\n\\begin{split}\n\\frac{1}{1-x}\n& = \\frac{1}{x} \\cdot \\frac{1}{\\frac{1}{x} - 1}\n= -\\frac{1}{x} \\cdot \\frac{1}{1 - \\frac{1}{x}} \\\\\n& = -\\frac{1}{x} -\\frac{1}{x^2} -\\frac{1}{x^3} -\\frac{1}{x^4} - \\cdots \\\\\n& = -\\sum_{j=1}^{\\infty} x^{-j} .\n\\end{split}\n\\] Then we can apply this idea to \\(Y_t = (1-\\phi B)^{-1} e_t\\): \\[\nY_t = (1-\\phi B)^{-1} e_t\n= -\\sum_{j=1}^{\\infty}(\\phi B)^{-j} e_t\n= -\\sum_{j=1}^{\\infty} \\phi^{-j} \\left(B^{-j} e_t\\right)\n= -\\sum_{j=1}^{\\infty} \\phi^{-j} e_{t+j},\n\\] which is still a GLP, but it is a .\nRemark: to see this is still a GLP, \\(Y_t = -\\sum_{j=1}^{\\infty} \\phi^{-j} e_{t+j}\\) can be written in the form \\(Y_t = \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\) and it satisfy \\(\\sum_{j=-\\infty}^{+\\infty} |\\psi_j| &lt; \\infty\\).\nIn summary, considering the parameter \\(\\phi\\) in AR(\\(1\\)), we have:\n\nIf \\(|\\phi|&lt;1\\): it can be represented as a causal GLP, and it is stationary.\nIf \\(|\\phi|&gt;1\\): it can be represented as a non-causal GLP, and it is stationary.\nIf \\(|\\phi|=1\\): it can be shown that this process is not stationary.",
    "crumbs": [
      "Lecture 4"
    ]
  },
  {
    "objectID": "TSA-Lecture04.html#applying-the-glp-results-on-ar1",
    "href": "TSA-Lecture04.html#applying-the-glp-results-on-ar1",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "",
    "text": "Assume \\(|\\phi|&lt;1\\). Using GLP results, we can get the mean function, variance function, ACVF, ACF of AR(1).\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = 0\\).\nVariance function: \\[\n\\sigma_t^2 = \\operatorname{Var}(Y_t) = \\sigma_e^2 \\sum_{j=0}^{\\infty} \\psi_j^2\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\phi^{2j} = \\frac{\\sigma_e^2}{1 - \\phi^2}.\n\\]\nACVF: \\[\n\\gamma_k = \\operatorname{Cov}(Y_t, Y_{t-k})\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\psi_j \\psi_{j+k}\n= \\sigma_e^2 \\sum_{j=0}^{\\infty} \\phi^j \\phi^{j+k}\n= \\sigma_e^2 \\phi^k \\sum_{j=0}^{\\infty} \\phi^{2j}\n= \\frac{\\sigma_e^2 \\phi^k}{1 - \\phi^2} .\n\\]\nACF: \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\phi^k\\).\n\nRemark: \\(\\rho_k \\to 0\\) geometrically as \\(k \\to \\infty\\). And we can see AR(\\(1\\)) is not \\(q\\)-dependent for any finite \\(q\\), since \\(\\rho_k \\ne 0\\) for any \\(k\\) (consider generic \\(\\phi \\neq 0\\)).\nFurther Remark: All AR(\\(p\\)) have this type of behavior, \\(\\gamma_k\\) and \\(\\rho_k\\) decays geometrically.",
    "crumbs": [
      "Lecture 4"
    ]
  },
  {
    "objectID": "TSA-Lecture04.html#another-method-using-the-operator",
    "href": "TSA-Lecture04.html#another-method-using-the-operator",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "",
    "text": "Using the backshift operator \\(B\\) from last lecture, for AR(\\(1\\)), we have \\[\n\\begin{split}\nY_t = \\phi Y_{t-1} + e_t &\\implies e_t = Y_t - \\phi B Y_t = (1-\\phi B) Y_t \\\\\n& \\implies Y_t = (1-\\phi B)^{-1} e_t .\n\\end{split}\n\\] Note that the basic operation \\((1-x)^{-1} = \\frac{1}{1-x} = 1+x+x^2+\\cdots\\) for \\(|x|&lt;1\\). For \\(|\\phi|&lt;1\\), we can do a similar expansion for \\((1-\\phi B)^{-1}\\) here: \\[\n\\begin{split}\nY_t &= (1-\\phi B)^{-1} e_t\n= \\left(1 + \\phi B + (\\phi B)^2 + (\\phi B)^3 + \\cdots \\right) e_t \\\\\n& = \\left(1 + \\phi B + \\phi^2 B^2 + \\phi^3 B^3 + \\cdots \\right) e_t \\\\\n& = e_t + \\phi (B e_t) + \\phi^2 (B^2 e_t) + \\cdots \\\\\n& = e_t + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\cdots \\\\\n& = \\sum_{j=0}^{\\infty} \\phi^j e_{t-j}\n\\end{split}\n\\] which gives the same GLP we derived before.",
    "crumbs": [
      "Lecture 4"
    ]
  },
  {
    "objectID": "TSA-Lecture04.html#a-brief-discussion-on-the-case-phi1",
    "href": "TSA-Lecture04.html#a-brief-discussion-on-the-case-phi1",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "",
    "text": "In the previous derivation, we assumed \\(|\\phi|&lt;1\\). What happens if \\(|\\phi|&gt;1\\) for AR(\\(1\\))? In this case, we cannot directly expand the term \\((1-\\phi B)^{-1}\\) since it will not converge. In comparison to \\((1-x)^{-1} = 1+x+x^2+\\cdots\\) for \\(|x|&lt;1\\), we have the following convergent reformulation for the \\(|x|&gt;1\\) case: \\[\n\\begin{split}\n\\frac{1}{1-x}\n& = \\frac{1}{x} \\cdot \\frac{1}{\\frac{1}{x} - 1}\n= -\\frac{1}{x} \\cdot \\frac{1}{1 - \\frac{1}{x}} \\\\\n& = -\\frac{1}{x} -\\frac{1}{x^2} -\\frac{1}{x^3} -\\frac{1}{x^4} - \\cdots \\\\\n& = -\\sum_{j=1}^{\\infty} x^{-j} .\n\\end{split}\n\\] Then we can apply this idea to \\(Y_t = (1-\\phi B)^{-1} e_t\\): \\[\nY_t = (1-\\phi B)^{-1} e_t\n= -\\sum_{j=1}^{\\infty}(\\phi B)^{-j} e_t\n= -\\sum_{j=1}^{\\infty} \\phi^{-j} \\left(B^{-j} e_t\\right)\n= -\\sum_{j=1}^{\\infty} \\phi^{-j} e_{t+j},\n\\] which is still a GLP, but it is a .\nRemark: to see this is still a GLP, \\(Y_t = -\\sum_{j=1}^{\\infty} \\phi^{-j} e_{t+j}\\) can be written in the form \\(Y_t = \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\) and it satisfy \\(\\sum_{j=-\\infty}^{+\\infty} |\\psi_j| &lt; \\infty\\).\nIn summary, considering the parameter \\(\\phi\\) in AR(\\(1\\)), we have:\n\nIf \\(|\\phi|&lt;1\\): it can be represented as a causal GLP, and it is stationary.\nIf \\(|\\phi|&gt;1\\): it can be represented as a non-causal GLP, and it is stationary.\nIf \\(|\\phi|=1\\): it can be shown that this process is not stationary.",
    "crumbs": [
      "Lecture 4"
    ]
  },
  {
    "objectID": "TSA-Lecture04.html#y-w-method-applied-on-causal-ar1",
    "href": "TSA-Lecture04.html#y-w-method-applied-on-causal-ar1",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "3.1 Y-W method applied on causal AR(\\(1\\))",
    "text": "3.1 Y-W method applied on causal AR(\\(1\\))\nFirst write down the AR(\\(1\\)) equation \\[\nY_t - \\phi Y_{t-1} = e_t .\n\\] Step 1: multiply by \\(Y_{t-k}\\) (for some \\(k\\ge 0\\)) \\[\nY_t Y_{t-k} - \\phi Y_{t-1} Y_{t-k} = e_t Y_{t-k} .\n\\] Step 2: take expectation, and we call the following \\[\n\\mathbb{E}[Y_t Y_{t-k}] - \\phi \\mathbb{E}[Y_{t-1} Y_{t-k}] = \\mathbb{E}[e_t Y_{t-k}] .\n\\] Note that the AR(\\(1\\)) process \\((Y_t)\\) we considered here is mean zero and stationary, we have \\(\\mathbb{E}[Y_t Y_{t-k}] = \\gamma_k\\) and \\(\\mathbb{E}[Y_{t-1} Y_{t-k}] = \\gamma_{k-1}\\). So the \\(k\\)-th YW equation become \\[\n\\gamma_k - \\phi \\gamma_{k-1} = \\mathbb{E}[e_t Y_{t-k}].\n\\] It remains to deal with the term \\(\\mathbb{E}[e_t Y_{t-k}]\\) in the equation above. For \\(k=0\\), (the \\(0\\)-th YW equation) \\[\n\\mathbb{E}[e_t Y_t]\n    = \\mathbb{E} \\left[ e_t \\sum_{j=0}^{\\infty} \\phi^j e_{t-j} \\right] = \\mathbb{E} \\left[ e_t^2 + \\phi e_t e_{t-1} + \\phi^2 e_t e_{t-2} + \\cdots \\right] = \\mathbb{E}[e_t^2] = \\sigma_e^2 .\n\\] For \\(k\\ge 1\\), note that \\((Y_t)\\) is causal, \\[\n\\mathbb{E}[e_t Y_{t-k}] = \\mathbb{E}\\left[ e_t \\left( e_{t-k} + \\phi e_{t-k-1} + \\cdots \\right) \\right] = 0.\n\\] So the Y-W equations are: (also note \\(\\gamma_{-1} = \\gamma_1\\) in the \\(0\\)-th YW equation) \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 ,\\quad &\\text{($0$th YW eq)}\\\\\n\\gamma_{1} - \\phi \\gamma_{0} = 0 ,\\quad &\\text{($1$st YW eq)}\\\\\n\\gamma_{2} - \\phi \\gamma_{1} = 0 ,\\quad &\\text{($2$nd YW eq)}\\\\\n\\gamma_{3} - \\phi \\gamma_{2} = 0 ,\\quad &\\text{($3$rd YW eq)}\\\\\n\\cdots &\n\\end{cases}\n\\] Then we can solve the \\(\\gamma_k\\) from this system. \\[\n\\begin{cases}\n\\gamma_{0} - \\phi \\gamma_{1} = \\sigma_e^2 \\\\\n\\gamma_{1} - \\phi \\gamma_{0} = 0\n\\end{cases}\n\\implies\n\\gamma_{0} = \\frac{\\sigma_e^2}{1-\\phi^2} .\n\\] The remaining equations give the recursive result: \\(\\gamma_{k} = \\phi^{k} \\frac{\\sigma_e^2}{1-\\phi^2}\\) for any \\(k\\ge 0\\).\nRemark: for YW method to work, the process should be causal, as we highlighted above. Question: what would go wrong if causality do not hold?",
    "crumbs": [
      "Lecture 4"
    ]
  },
  {
    "objectID": "TSA-Lecture04.html#ar-polynomial",
    "href": "TSA-Lecture04.html#ar-polynomial",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "5.1 AR polynomial",
    "text": "5.1 AR polynomial\nNote that the AR(\\(p\\)) equation is \\(Y_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2} - \\cdots - \\phi_p Y_{t-p} = e_t\\), we define the following useful tool.\nDefinition: the AR polynomial (for the AR(\\(p\\)) above) is defined as the following \\(p\\)-th order polynomial (i.e., polynomial with order/degree \\(p\\)) \\[\n\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p .\n\\]\nUsing this notation and the backshift operator, the AR(\\(p\\)) equation can be reformulated as \\[\ne_t = Y_t - \\phi_1 B Y_t - \\cdots - \\phi_p B^p Y_t = \\left(1 - \\phi_1 B - \\phi_2 B^2 - \\cdots - \\phi_p B^p \\right) Y_t = \\Phi(B) Y_t\n\\] so we reach the simple form \\(\\Phi(B) Y_t = e_t\\).",
    "crumbs": [
      "Lecture 4"
    ]
  },
  {
    "objectID": "TSA-Lecture04.html#causality-condition-for-an-arp-process",
    "href": "TSA-Lecture04.html#causality-condition-for-an-arp-process",
    "title": "25 Spring 439/639 TSA: Lecture 4",
    "section": "5.2 Causality condition for an AR(\\(p\\)) process",
    "text": "5.2 Causality condition for an AR(\\(p\\)) process\nThe sufficient and necessary condition for an AR(\\(p\\)) process to be causal is:\nCausality condition: All (complex) roots of the AR polynomial \\(\\Phi(x)\\), are strictly greater than \\(1\\) in absolute value (the modulus of complex number).\nExplanation: \\(\\Phi(x)\\) is a real polynomial of degree \\(p\\), so it has \\(p\\) complex roots, namely \\(z_1,...,z_p \\in \\mathbb{C}\\). Then the conditions says \\(|z_i| &gt;1\\) for all \\(i=1,...,p\\). This also means all the \\(p\\) roots are outside the unit disk in \\(\\mathbb{C}\\).\nExample: consider \\(p=1\\). The AR polynomial AR(\\(1\\)) is \\(\\Phi(x) = 1-\\phi x\\). This \\(\\Phi(x)\\) only has one root \\(z_1 = \\frac{1}{\\phi}\\). The causality condition above reduces to \\(|z_1|&gt;1\\), i.e., \\(|\\frac{1}{\\phi}| &gt; 1\\), which is equivalent to \\(|\\phi| &lt;1\\). This is same as our earlier discussion in this lecture (see the ``\\(|\\phi|&lt;1, |\\phi|&gt;1, |\\phi|=1\\) part”).\nLet’s look at the causality condition above. Assume \\(\\phi_p \\neq 0\\), so the AR polynomial is of order \\(p\\). Consider the AR polynomial with \\(p\\) roots \\(z_1,...,z_p \\in \\mathbb{C}\\). Then we have \\[\n\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p = - \\phi_p (x-z_1) (x-z_2) \\cdots (x-z_p) .\n\\] Note that \\(1 = \\Phi(0) = - \\phi_p (0-z_1) (0-z_2) \\cdots (0-z_p) = (-1)^{p+1} \\phi_p z_1\\cdots z_p\\), which also implies \\(z_1,...,z_p \\neq 0\\). So \\[\n\\begin{split}\n\\Phi(x) &= - \\phi_p (x-z_1) (x-z_2) \\cdots (x-z_p) \\\\\n&= -\\phi_p (-z_1) \\cdots (-z_p) \\left(1-\\frac{x}{z_1}\\right) \\cdots \\left(1-\\frac{x}{z_p}\\right) \\\\\n&= \\left(1-\\frac{x}{z_1}\\right) \\cdots \\left(1-\\frac{x}{z_p}\\right) .\n\\end{split}\n\\] So \\(\\Phi(x) = \\left(1-\\frac{x}{z_1}\\right) \\cdots \\left(1-\\frac{x}{z_p}\\right)\\), and the AR(\\(p\\)) equation becomes \\[\ne_t = \\phi(B) Y_t = \\left(1-\\frac{B}{z_1}\\right) \\cdots \\left(1-\\frac{B}{z_p}\\right) Y_t .\n\\] Since the causality condition requires \\(|z_i|&gt;1\\), so \\(|\\frac{1}{z_i}|&lt;1\\), and each \\(\\left(1-\\frac{B}{z_i}\\right)\\) is invertible in the same way as before, \\(\\left(1-\\frac{B}{z_i}\\right) ^{-1} = 1+ \\frac{B}{z_i} + (\\frac{B}{z_i})^2 + \\cdots\\). The main idea here is, we can invert each \\(\\left(1-\\frac{B}{z_i}\\right)\\), then multiplying them together gives a GLP form of \\(Y_t\\): \\[\n\\begin{split}\nY_t &= \\left(1-\\frac{B}{z_1}\\right)^{-1} \\cdots \\left(1-\\frac{B}{z_p}\\right)^{-1} e_t \\\\\n&= \\left(\\sum_{j=0}^\\infty \\frac{B^j}{z_1^j} \\right) \\cdots \\left(\\sum_{j=0}^\\infty \\frac{B^j}{z_p^j} \\right) e_t \\\\\n&= \\sum_{j=0}^\\infty \\psi_j e_{t-j} .\n\\end{split}\n\\] We will discuss more on this next time.",
    "crumbs": [
      "Lecture 4"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html",
    "href": "TSA-Lecture02.html",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "",
    "text": "Lecture 1: A time series is strictly stationary if all finite dimensional joint distributions are time invariant, i.e., \\[\nF_{Y_{t_1}, \\dots, Y_{t_n}} = F_{Y_{t_1 -k}, \\dots, Y_{t_n -k}}, \\quad \\forall n, \\forall t_1,\\dots,t_n, \\forall k.\n\\]\n\n\n\nSuppose \\((Y_t)\\) is a strictly stationary time series.\n\n\\(Y_t \\overset{D}{=} Y_0\\) for all \\(t\\), i.e., all \\(Y_t\\) are identically distributed. Proof: take \\(n=1\\) in the definition.\nIf the distribution (of \\(Y_t\\)) has finite 2nd moment (which implies finite first moment by \\((\\mathbb{E}[Y])^2 \\le \\mathbb{E}[Y^2] &lt; \\infty\\)), then for any \\(t\\),\n\n\\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[Y_0]\\) does not depend on \\(t\\).\n\\(\\sigma_t^2 = Var(Y_t) = Var(Y_0)\\) does not depend on \\(t\\).\n\nLet \\(n=2\\) in the definition, we have \\((Y_s,Y_t) \\overset{D}{=} (Y_{s-k},Y_{t-k}) \\overset{D}{=} (Y_0, Y_{t-s})\\). Consider the covariance, \\(Cov(Y_s,Y_t) = Cov(Y_0, Y_{t-s})\\), i.e., \\(\\gamma_{s,t} = \\gamma_{0, t-s}\\) which only depends on the lag \\(t-s\\). So for strictly stationary time series, we can simplify the notation by the following definition\n\n\\[\n\\gamma_{t-s} := \\gamma_{0, t-s} = \\gamma_{s,t}.\n\\] By the symmetry of ACVF, \\(\\gamma_{s-t} = \\gamma_{t,s} = \\gamma_{s,t} = \\gamma_{t-s}\\), so \\(\\gamma_k\\) is an even function of \\(k\\) in the sense that \\(\\gamma_{s-t} = \\gamma_{t-s} = \\gamma_{|t-s|}\\). Example: for a strictly stationary time series, \\(\\gamma_5 = \\gamma_{-5}= Cov(Y_0,Y_5)= Cov(Y_1,Y_6) = Cov(Y_t,Y_{t+5}), \\forall t\\).\nIn summary, for a strictly stationary time series with finite 2nd moment,\n\nMean Function: \\(\\mu_t=\\mu\\), \\(\\forall t\\in \\mathbb{Z}\\).\nVariance Function: \\(Var(Y_t) = \\gamma_{t,t}= \\gamma_0 = \\sigma^2\\), \\(\\forall t\\in \\mathbb{Z}\\).\nACVF: \\(\\gamma_k = Cov(Y_t,Y_{t-k}) = Cov(Y_{t-k},Y_t) = \\gamma_{-k}\\), \\(\\forall k,t\\in \\mathbb{Z}\\).\nACF: \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\\) (assuming \\(\\gamma_0 \\neq 0\\)).\n\n\n\n\nDefinition: A time series \\((Y_t)\\), \\(t\\in \\mathbb{Z}\\), satisfying the following three conditions,\n\n\\(\\mu_t=\\mu\\) for some (finite) constant \\(\\mu\\), \\(\\forall t\\in \\mathbb{Z}\\),\n\\(Var(Y_t) =\\sigma^2\\) for some (finite) constant \\(\\sigma^2\\), \\(\\forall t\\in \\mathbb{Z}\\),\n\\(Cov(Y_t,Y_{t-k}) = \\gamma_k\\) for some function \\(\\gamma_k\\) that only depends on the lag \\(k\\) and does not depend on the time \\(t\\),\n\nis called weakly stationary/ stationary / second order stationary / covariance stationary.",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html#strict-stationarity",
    "href": "TSA-Lecture02.html#strict-stationarity",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "",
    "text": "Lecture 1: A time series is strictly stationary if all finite dimensional joint distributions are time invariant, i.e., \\[\nF_{Y_{t_1}, \\dots, Y_{t_n}} = F_{Y_{t_1 -k}, \\dots, Y_{t_n -k}}, \\quad \\forall n, \\forall t_1,\\dots,t_n, \\forall k.\n\\]",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html#properties-of-strict-stationarity",
    "href": "TSA-Lecture02.html#properties-of-strict-stationarity",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "",
    "text": "Suppose \\((Y_t)\\) is a strictly stationary time series.\n\n\\(Y_t \\overset{D}{=} Y_0\\) for all \\(t\\), i.e., all \\(Y_t\\) are identically distributed. Proof: take \\(n=1\\) in the definition.\nIf the distribution (of \\(Y_t\\)) has finite 2nd moment (which implies finite first moment by \\((\\mathbb{E}[Y])^2 \\le \\mathbb{E}[Y^2] &lt; \\infty\\)), then for any \\(t\\),\n\n\\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[Y_0]\\) does not depend on \\(t\\).\n\\(\\sigma_t^2 = Var(Y_t) = Var(Y_0)\\) does not depend on \\(t\\).\n\nLet \\(n=2\\) in the definition, we have \\((Y_s,Y_t) \\overset{D}{=} (Y_{s-k},Y_{t-k}) \\overset{D}{=} (Y_0, Y_{t-s})\\). Consider the covariance, \\(Cov(Y_s,Y_t) = Cov(Y_0, Y_{t-s})\\), i.e., \\(\\gamma_{s,t} = \\gamma_{0, t-s}\\) which only depends on the lag \\(t-s\\). So for strictly stationary time series, we can simplify the notation by the following definition\n\n\\[\n\\gamma_{t-s} := \\gamma_{0, t-s} = \\gamma_{s,t}.\n\\] By the symmetry of ACVF, \\(\\gamma_{s-t} = \\gamma_{t,s} = \\gamma_{s,t} = \\gamma_{t-s}\\), so \\(\\gamma_k\\) is an even function of \\(k\\) in the sense that \\(\\gamma_{s-t} = \\gamma_{t-s} = \\gamma_{|t-s|}\\). Example: for a strictly stationary time series, \\(\\gamma_5 = \\gamma_{-5}= Cov(Y_0,Y_5)= Cov(Y_1,Y_6) = Cov(Y_t,Y_{t+5}), \\forall t\\).\nIn summary, for a strictly stationary time series with finite 2nd moment,\n\nMean Function: \\(\\mu_t=\\mu\\), \\(\\forall t\\in \\mathbb{Z}\\).\nVariance Function: \\(Var(Y_t) = \\gamma_{t,t}= \\gamma_0 = \\sigma^2\\), \\(\\forall t\\in \\mathbb{Z}\\).\nACVF: \\(\\gamma_k = Cov(Y_t,Y_{t-k}) = Cov(Y_{t-k},Y_t) = \\gamma_{-k}\\), \\(\\forall k,t\\in \\mathbb{Z}\\).\nACF: \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\\) (assuming \\(\\gamma_0 \\neq 0\\)).",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html#weak-stationarity",
    "href": "TSA-Lecture02.html#weak-stationarity",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "",
    "text": "Definition: A time series \\((Y_t)\\), \\(t\\in \\mathbb{Z}\\), satisfying the following three conditions,\n\n\\(\\mu_t=\\mu\\) for some (finite) constant \\(\\mu\\), \\(\\forall t\\in \\mathbb{Z}\\),\n\\(Var(Y_t) =\\sigma^2\\) for some (finite) constant \\(\\sigma^2\\), \\(\\forall t\\in \\mathbb{Z}\\),\n\\(Cov(Y_t,Y_{t-k}) = \\gamma_k\\) for some function \\(\\gamma_k\\) that only depends on the lag \\(k\\) and does not depend on the time \\(t\\),\n\nis called weakly stationary/ stationary / second order stationary / covariance stationary.",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html#example-1",
    "href": "TSA-Lecture02.html#example-1",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.1 Example 1",
    "text": "2.1 Example 1\nConsider the time series \\((e_t)\\) where \\(e_t \\sim IID(0,\\sigma_e^2)\\).\n\nShow the joint cdf of \\((e_{t_1},\\cdots,e_{t_n})\\) and \\((e_{t_1-k},\\cdots,e_{t_n-k})\\) are the same:\n\n\\[\n\\begin{split}\nF_{e_{t_1},\\cdots,e_{t_n}}(a_1,\\cdots,a_n) &= \\mathbb{P}(e_{t_1}\\le a_1, \\cdots, e_{t_n}\\le a_n) \\\\\n&= \\mathbb{P}(e_{t_1}\\le a_1) \\mathbb{P}(e_{t_2}\\le a_2) \\cdots \\mathbb{P}(e_{t_n}\\le a_n) \\\\\n&= \\mathbb{P}(e_{t_1-k}\\le a_1) \\mathbb{P}(e_{t_2-k}\\le a_2) \\cdots \\mathbb{P}(e_{t_n-k}\\le a_n) \\\\\n&= \\mathbb{P}(e_{t_1-k}\\le a_1, \\cdots, e_{t_n-k}\\le a_n) = F_{e_{t_1-k},\\cdots,e_{t_n-k}}(a_1,\\cdots,a_n)\n\\end{split}\n\\] By definition, \\((e_t)\\) is strictly stationary.\n\nIt is also weakly stationary, since\n\n\\(\\mu_t = 0\\), does not depend on \\(t\\),\n\\(Var(e_t) = \\sigma_e^2\\), does not depend on \\(t\\),\nThe ACVF only depends on the lag \\(k\\) as follows\n\n\\[\nCov(e_t, e_{t-k}) = \\begin{cases}\n0, & k\\neq 0\\\\\n\\sigma_e^2, & k=0.\n\\end{cases}\n\\]",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html#example-2",
    "href": "TSA-Lecture02.html#example-2",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.2 Example 2",
    "text": "2.2 Example 2\nLet \\(U_t \\overset{iid}{\\sim} N(0,1)\\). Define \\(X_t\\) as follows \\[\nX_t = \\begin{cases}\nU_t, & t \\text{ is even}\\\\\n\\frac{1}{\\sqrt{2}}(U_t^2 - 1), & t \\text{ is odd}.\n\\end{cases}\n\\] It is weakly stationary since\n\nFor even \\(t\\), \\(\\mu_t= \\mathbb{E}[U_t] = 0\\). For odd \\(t\\), \\(\\mu_t= \\mathbb{E}[\\frac{1}{\\sqrt{2}}(U_t^2 - 1)] = \\frac{1}{\\sqrt{2}}(\\mathbb{E}[U_t^2]-1) = 0\\). So \\(\\mu_t=0\\) for all \\(t\\).\nFor even \\(t\\), \\(Var(X_t) = Var(U_t)=1\\). For odd \\(t\\), \\(Var(X_t) = Var(\\frac{1}{\\sqrt{2}}(U_t^2 - 1)) = \\frac{1}{2} Var(U_t^2) = \\frac{1}{2} (\\mathbb{E}[U_t^4] - (\\mathbb{E}[U_t^2])^2 ) = \\frac{1}{2}(3-1) = 1\\). So \\(Var(X_t) = 1\\) for all \\(t\\).\n\n\n\\[\nCov(Y_t,Y_{t-k}) = \\begin{cases}\n0, \\quad k\\neq 0 \\\\\n1, \\quad k=0.\n\\end{cases}\n\\] Exercise: show that \\(Cov(Y_t,Y_{t-k}) = 0\\) for \\(k\\neq 0\\).\nFor this example \\((X_t)\\), we can show that it is not strictly stationary by proving \\(X_1\\) and \\(X_2\\) are not identically distributed.\nExercise: prove the claim above. (Hint: find some real number \\(a\\) such that \\(\\mathbb{P}(\\frac{1}{\\sqrt{2}}(U_1^2 - 1) \\le a) \\neq \\mathbb{P}(U_2 \\le a)\\).)",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html#clarification-on-some-notations-and-concepts",
    "href": "TSA-Lecture02.html#clarification-on-some-notations-and-concepts",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.3 Clarification on some notations and concepts",
    "text": "2.3 Clarification on some notations and concepts\n\nIID noise: In the previous Example 1, \\(e_t \\sim IID(0,\\sigma_e^2)\\). In this course (TSA), we call a time series \\((e_t)\\) iid noise, denoted by \\(e_t \\sim IID(0,\\sigma_e^2)\\), if it satisfies: mean \\(0\\), variance \\(\\sigma_e^2\\), all \\(e_t\\) are iid (independently identically distributed).\nWhite noise: In this course, we call a time series \\((e_t)\\) white noise, denoted by \\(e_t \\sim WN(0,\\sigma_e^2)\\), if it satisfies: mean \\(0\\), variance \\(\\sigma_e^2\\), all \\(e_t\\) are uncorrelated (pairwise uncorrelated).\nIn general, iid noise implies white noise, and the inverse is not true. Under the assumption of normality, they are equivalent. (Note: here normality refers to the assumption that the time series \\((e_t)\\) is a Gaussian process.)\n\n\\[\n\\begin{split}\n\\text{iid noise} \\quad & \\Rightarrow \\quad \\text{white noise}, \\\\\n\\text{iid noise} \\quad & \\not\\Leftarrow \\quad \\text{white noise}, \\\\\n\\text{iid Normal(Gaussian) noise} \\quad & \\Leftrightarrow \\quad \\text{Normal(Gaussian) white noise}.\n\\end{split}\n\\]\n\nWarning: in Cryer and Chan, they often use iid noise and white noise interchangeably.",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html#more-examples-details-omitted",
    "href": "TSA-Lecture02.html#more-examples-details-omitted",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.4 More examples (details omitted)",
    "text": "2.4 More examples (details omitted)\n\n“Linear regression” example from lecture 1, where \\(Y_t = a+bt+e_t\\), and \\(e_t \\sim WN(0,\\sigma_e^2)\\). This is not stationary since \\(\\mu_t = a+bt\\) (whenever \\(b\\neq 0\\)).\n“Random walk” example from lecture 1. For positive integer \\(t\\), \\(Y_t = \\sum_{i}^t e_i\\), where \\(e_t \\sim WN(0,\\sigma_e^2)\\). We have \\(\\mu_t=0\\), \\(Var(Y_t) = t \\sigma_e^2\\). This \\((Y_t)\\) is not stationary.\n“Moving average” example from lecture 1, where \\(Y_t = \\frac{e_t+ e_{t-1}}{2}\\), and \\(e_t \\sim WN(0,\\sigma_e^2)\\). In lecture 1, we already calculated its mean function, variance function and ACVF. This \\((Y_t)\\) is stationary.",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "TSA-Lecture02.html#example-3",
    "href": "TSA-Lecture02.html#example-3",
    "title": "25 Spring 439/639 TSA: Lecture 2",
    "section": "2.5 Example 3",
    "text": "2.5 Example 3\nLet \\(A,B\\) be two iid random variables, with \\(\\mathbb{E}[A]=\\mathbb{E}[B]=0\\) and \\(Var(A)= Var(B) = \\sigma^2\\). Let \\(w\\in \\mathbb{R}\\) be a fixed real number. For each \\(t\\in \\mathbb{Z}\\), define \\(Y_t = A \\cos(wt) + B \\sin(wt)\\). Remark: note that \\(A,B\\) are random, but they are “same” for all \\(t\\).\n\n\\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[A \\cos(wt) + B \\sin(wt)] = \\cos(wt)\\mathbb{E}[A] + \\sin(wt) \\mathbb{E}[B] =0\\).\n\\(Var(Y_t)= Var(A \\cos(wt) + B \\sin(wt)) = \\cos^2(wt)Var(A) + \\sin^2(wt)Var(B) + 0 = \\sigma^2\\).\nThe ACVF only depends on the lag \\(k\\) as follows\n\n\\[\n\\begin{split}\nCov(Y_t,Y_{t-k}) &= Cov(A \\cos(wt) + B \\sin(wt), A \\cos(w(t-k)) + B \\sin(w(t-k))) \\\\\n&= \\cos(wt)\\cos(w(t-k)) Var(A) + \\sin(wt)\\sin(w(t-k)) Var(B) + 0 + 0 \\\\\n&= \\sigma^2 (\\cos(wt)\\cos(w(t-k)) + \\sin(wt)\\sin(w(t-k))) \\\\\n&= \\sigma^2 \\cos(wt - w(t-k)) = \\sigma^2 \\cos(wk)\n\\end{split}\n\\] So \\((Y_t)\\) is weakly stationary. In general (for generic choice of \\(w\\), generic distribution of \\(A,B\\), etc.), \\((Y_t)\\) is not strictly stationary.\nExercise: show that \\(Y_0\\) and \\(Y_1\\) are not identically distributed in general.",
    "crumbs": [
      "Lecture 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis: Lecture Notes",
    "section": "",
    "text": "These are the lecture notes for EN.553.439/639 Time Series Analysis, based on the Cryer and Chen textbook Time Series Analysis: With Applications in R and Time Series Lecture notes of Dr Torcaso.\nThe notes are typed by Yue Wu. The notes are authored and maintained by Sergey Kushnarev, and include:\n\nTopic summaries\nMathematical derivations\nR notebooks with code and data examples\nFigures and simulation results\n\n\nThink of these notes as a companion to our lectures and discussions, not a substitute. You’ll likely spot some typos and errors, so always double-check with the textbook and other course materials. If you catch an issue or have a question, please don’t hesitate to reach out to me via email or during office hours.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Time Series Analysis: Lecture Notes",
    "section": "",
    "text": "These are the lecture notes for EN.553.439/639 Time Series Analysis, based on the Cryer and Chen textbook Time Series Analysis: With Applications in R and Time Series Lecture notes of Dr Torcaso.\nThe notes are typed by Yue Wu. The notes are authored and maintained by Sergey Kushnarev, and include:\n\nTopic summaries\nMathematical derivations\nR notebooks with code and data examples\nFigures and simulation results\n\n\nThink of these notes as a companion to our lectures and discussions, not a substitute. You’ll likely spot some typos and errors, so always double-check with the textbook and other course materials. If you catch an issue or have a question, please don’t hesitate to reach out to me via email or during office hours.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Time Series Analysis: Lecture Notes",
    "section": "📚 Lecture Notes",
    "text": "📚 Lecture Notes\n\nLecture 1: Introduction to Time Series Analysis\nLecture 2: Stationarity\nLecture 3: Q-dependent CLT. MA(q), General Linear Process. Causality\nLecture 4: AR(1), Yule-Walker method, AR-polynomial\nLecture 5: AR(p): causality, YW method, GLP representation,\nLecture 6: Recursive equations, ACVF for AR(2), AR(p), Invertibility\nLecture 7: ARMA(1,1), ARMA(p,q): GLP representation, YW method, ACVF.\n\nRnotebook: Visualizing Time Series\n\nLecture 8: Trends, Estimation of the mean Yt, Rnotebook: regression approach, residual analysis\n\nRnotebook: Regression Methods in Time Series\n\nLecture 9: Residual Analysis. Sample ACF\nLecture 10: ARIMA(\\(p,d,q\\)), Overdifferencing, GLP-like representation\nLecture 11: Transformations of TS, sample ACF, Bartletts Theorem, Hypothesis test for MA(q)\nLecture 12: PACF, sample PACF, Durbin-Levinson recursion\nLecture 13: EACF, Specification of some TS, Unit Root test.\nLecture 14: Parameter Estimation: method of moments\n\nRnotebook: Model Specification\n\nLecture 15: Parameter Estimation: Method of Moments, condtional least squares\nLecture 16: Parameter Estimation: MLE, unconditional least squares.\n\nRnotebook: Parameter Estimation\n\nLecture 17: Diagnostics: residuals, overfitting\n\nRnotebook: Model Diagnostics\n\nLecture 18: Forecasting\nLecture 19: Forecasting: RW, ARMA(1,1), ARIMA, EWMA\n\nRnotebook: Forecasting\n\nLecture 20: SARIMA = Seasonal ARIMA\n\nRnotebook: SARIMA = Seasonal ARIMA\n\nLecture 21: Cross-Correlation Function, Spurious Regression.\nLecture 22: Spurious Regression, Prewhitening\n\nRnotebook: Spurious Regression\n\nLecture 23: ARCH/GARCH Models\n\nRnotebook: ARCH/GARCH",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Time Series Analysis: Lecture Notes",
    "section": "🔗 Resources",
    "text": "🔗 Resources\n\n[Course Syllabus (PDF)] TBD\nRStudio Cloud Project",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About these Notes",
    "section": "",
    "text": "Dr Sergey Kushnarev\nSenior Lecturer, AMS JHU",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#current-courses",
    "href": "about.html#current-courses",
    "title": "About these Notes",
    "section": "Current Courses",
    "text": "Current Courses\n\nTime Series Analysis (TSA)\nLecture notes: TSA Lecture Notes\nBased on Cryer and Chen’s Time Series Analysis: With Applications in R, 2nd edition.\nAudience: Upper-level undergraduates and master’s students in Statistics/Applied Math.\nTopics: Stationarity, ARMA/ARIMA/SARIMA models, forecasting.\nBayesian Statistics\nNotes: Bayesian Notes To be posted Based on Peter Hoff’s A First Course in Bayesian Statistical Methods.\nAudience: Upper-level undergraduates and master’s students in Statistics/Applied Math.\nTopics: Conjugate priors, Gibbs sampling, Metropolis-Hastings, model comparison, etc.\nApplied Statistics and Data Analysis I\nNotes: ASDA1 To be posted Based on Kutner, Nachtsheim, Neter, and Li’s Applied Linear Statistical Models.\nAudience: Advanced undergraduates and master’s students.\nTopics: Simple and multiple linear regression, ANOVA, model selection, diagnostics, etc.\nApplied Statistics and Data Analysis II\nNotes: ASDA2 To be posted Based on Alan Agresti’s book Foundations of Linear and Generalized Linear Models.\nAudience: Advanced undergraduates and master’s students.\nTopics: Linear regression, logistic regression, Poisson regression, GLMs.\nElements of Statistical Learning Notes: ESL To be posted Based on Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani’s An Introduction to Statistical Learning in R, 2nd edition. Audience: Advanced undergraduates and master’s students. Topics: Supervised/unsupervised learning, model assessment, regularization, tree-based methods, etc.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#a-note-on-the-material",
    "href": "about.html#a-note-on-the-material",
    "title": "About these Notes",
    "section": "A Note on the Material",
    "text": "A Note on the Material\nThese notes are designed as teaching resources and not polished textbooks.\nThey include: - Theory, examples and proofs - Code snippets and exercises\nFeedback is always welcome — feel free to suggest improvements or point out typos via GitHub.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "About these Notes",
    "section": "License",
    "text": "License\nContent is © 2025 Sergey Kushnarev unless otherwise noted. You may share or adapt the materials for non-commercial purposes with attribution.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index-total.html",
    "href": "index-total.html",
    "title": "Lecture Notes",
    "section": "",
    "text": "This site hosts lecture notes for several courses I have taught or am currently teaching at the Applied Mathematics and Statistics department of Johns Hopkins University.\n\n\n\nTime Series Analysis\nElements of Statistical Learning\nBayesian Statistics\nApplied Statistics and Data Analysis I\nApplied Statistics and Data Analysis II"
  },
  {
    "objectID": "index-total.html#courses",
    "href": "index-total.html#courses",
    "title": "Lecture Notes",
    "section": "",
    "text": "Time Series Analysis\nElements of Statistical Learning\nBayesian Statistics\nApplied Statistics and Data Analysis I\nApplied Statistics and Data Analysis II"
  },
  {
    "objectID": "TSA-Lecture01.html",
    "href": "TSA-Lecture01.html",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "",
    "text": "We use lower case letters to denote Observed T.S.(Time Series). Example: \\(y_0,y_1,y_2,\\dots,\\) or \\(\\dots,y_{-2},y_{-1},y_0,y_1,y_2,\\dots.\\)\nWe use capital letters to denote Probabilistic Model for T.S., which is a series/sequence of RVs. Example: \\(Y_0,Y_1,Y_2,\\dots,\\) or \\(\\dots,Y_{-2},Y_{-1},Y_0,Y_1,Y_2,\\dots.\\) We may also use the following shorthand notations: \\((Y_i), (Y_t), (Y_t)_{t=0}^{+\\infty}, (Y_t)_{t=1}^{+\\infty}, (Y_t)_{t=-\\infty}^{+\\infty},\\) etc.\n\n\n\n\nDefinition: A time series model for the observed data \\((y_t)\\) is a specification of the joint distribution, or possibly just the means and covariances, of a sequence of random variables \\(\\{ Y(\\omega,t), t\\in T\\}\\) of which \\((y_t)\\) is a realization.\nWe will frequently use the term time series to denote both the model and the observed data.\nIn this definition, the stochastic process can be seen as a set/sequence of random variables \\(Y(\\omega,t)\\) indexed by time \\(t\\). The \\(T\\) is an index set for time \\(t\\). The randomness comes from \\(\\omega\\), which takes values from the sample space \\(\\Omega\\). For this course TSA, in most cases, we have the following setting:\n\nThe time index set \\(T\\) is often \\(\\mathbb{Z}\\) (the set of integers) or \\(\\mathbb{Z}_{\\ge 0}\\) (nonnegative integers).\nThe random variables here are continuous, which means for any fixed \\(t\\), the random variable \\(Y(\\omega,t)\\) follows a continuous distribution.",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture01.html#observed-t.s.-vs.-probabilistic-model-for-t.s.",
    "href": "TSA-Lecture01.html#observed-t.s.-vs.-probabilistic-model-for-t.s.",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "",
    "text": "We use lower case letters to denote Observed T.S.(Time Series). Example: \\(y_0,y_1,y_2,\\dots,\\) or \\(\\dots,y_{-2},y_{-1},y_0,y_1,y_2,\\dots.\\)\nWe use capital letters to denote Probabilistic Model for T.S., which is a series/sequence of RVs. Example: \\(Y_0,Y_1,Y_2,\\dots,\\) or \\(\\dots,Y_{-2},Y_{-1},Y_0,Y_1,Y_2,\\dots.\\) We may also use the following shorthand notations: \\((Y_i), (Y_t), (Y_t)_{t=0}^{+\\infty}, (Y_t)_{t=1}^{+\\infty}, (Y_t)_{t=-\\infty}^{+\\infty},\\) etc.",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture01.html#formal-definition-of-time-series",
    "href": "TSA-Lecture01.html#formal-definition-of-time-series",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "",
    "text": "Definition: A time series model for the observed data \\((y_t)\\) is a specification of the joint distribution, or possibly just the means and covariances, of a sequence of random variables \\(\\{ Y(\\omega,t), t\\in T\\}\\) of which \\((y_t)\\) is a realization.\nWe will frequently use the term time series to denote both the model and the observed data.\nIn this definition, the stochastic process can be seen as a set/sequence of random variables \\(Y(\\omega,t)\\) indexed by time \\(t\\). The \\(T\\) is an index set for time \\(t\\). The randomness comes from \\(\\omega\\), which takes values from the sample space \\(\\Omega\\). For this course TSA, in most cases, we have the following setting:\n\nThe time index set \\(T\\) is often \\(\\mathbb{Z}\\) (the set of integers) or \\(\\mathbb{Z}_{\\ge 0}\\) (nonnegative integers).\nThe random variables here are continuous, which means for any fixed \\(t\\), the random variable \\(Y(\\omega,t)\\) follows a continuous distribution.",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture01.html#definitions",
    "href": "TSA-Lecture01.html#definitions",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "2.1 Definitions",
    "text": "2.1 Definitions\n\nMean Function: for any \\(t\\), define \\[\\mu_t := \\mathbb{E}[Y_t].\\]\nAutocovariance Function(ACVF): for any two time indices \\(t,s\\), define the corresponding ACVF as the covariance of \\(Y_t\\) and \\(Y_s\\), i.e., \\[\\gamma_{t,s} := Cov(Y_t, Y_s) = \\mathbb{E}[(Y_t-\\mu_t)(Y_s-\\mu_s)] = \\mathbb{E}[Y_t Y_s] -\\mu_t\\mu_s.\\]\nVariance Function is the special case of ACVF with \\(t=s\\), which is equal to the variance of \\(Y_t\\), \\[\\gamma_{t,t} = Cov(Y_t, Y_t) = Var(Y_t).\\]\nAutocorrelation Function(ACF): for any two time indices \\(t,s\\), define the corresponding ACF as the correlation of \\(Y_t\\) and \\(Y_s\\), i.e., \\[\\rho_{t,s} := \\frac{Cov(Y_t, Y_s)}{\\sqrt{Var(Y_t) \\cdot Var(Y_s)}} = \\frac{\\gamma_{t,s}}{\\sqrt{\\gamma_{t,t} \\cdot \\gamma_{s,s}}}.\\]",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture01.html#properties",
    "href": "TSA-Lecture01.html#properties",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "2.2 Properties",
    "text": "2.2 Properties\n\nWhen \\(t=s\\): \\[\\gamma_{t,t} = Var(Y_t), \\quad \\rho_{t,t}=1.\\]\nSymmetry: \\[\\gamma_{t,s} = \\gamma_{s,t}, \\quad \\rho_{t,s} = \\rho_{s,t}.\\]\nRecall that the Cauchy–Schwarz inequality \\(Var(X) \\cdot Var(Y) \\ge |Cov(X,Y)|^2\\) holds for any two random variables \\(X,Y\\). This immediately gives \\[|\\gamma_{t,s}| \\le \\sqrt{\\gamma_{t,t} \\cdot \\gamma_{s,s} }, \\quad |\\rho_{t,s}| \\le 1.\\] Some further remarks:\n\nIf \\(\\rho_{t,s} \\approx \\pm 1\\), then \\(Y_t\\) and \\(Y_s\\) are strongly linearly related.\nIf \\(\\rho_{t,s} \\approx 0\\), then \\(Y_t\\) and \\(Y_s\\) are weakly linearly related.\nIf \\(\\rho_{t,s} = 0\\), then \\(Y_t\\) and \\(Y_s\\) are uncorrelated.\n\nIf \\(Y_t\\) and \\(Y_s\\) are independent, then \\(Y_t\\) and \\(Y_s\\) are uncorrelated (assuming both have nonzero and finite second moments). This is because independence directly implies \\(\\gamma_{t,s}=0\\), which further implies \\(\\rho_{t,s} = 0\\).\nBilinearity: For any positive integers \\(n,m\\), any real numbers \\(a_1,\\dots,a_n, b_1,\\dots,b_m \\in \\mathbb{R}\\), and any time points/indices \\(t_1,\\dots,t_n,s_1,\\dots,s_m\\), the following holds \\[Cov\\left( \\sum_{i=1}^n a_i Y_{t_i}, \\sum_{j=1}^m b_j Y_{s_j} \\right) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j Cov(Y_{t_i}, Y_{s_j}).\\] Examples:\n\n\\(Cov(a_1 Y_{t_1} + a_2 Y_{t_2}, Y_s) = a_1 Cov(Y_{t_1}, Y_s) + a_2 Cov(Y_{t_2}, Y_s).\\)\n\\(Cov(a_1 Y_{t_1} + a_2 Y_{t_2}, b_1 Y_{s_1} + b_2 Y_{s_2}) = a_1 b_1 Cov(Y_{t_1}, Y_{s_1}) + a_2 b_1 Cov(Y_{t_2}, Y_{s_1}) + a_1 b_2 Cov(Y_{t_1}, Y_{s_2}) + a_2 b_2 Cov(Y_{t_2}, Y_{s_2}).\\)",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture01.html#example-1-linear-regression",
    "href": "TSA-Lecture01.html#example-1-linear-regression",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "3.1 Example 1: Linear Regression",
    "text": "3.1 Example 1: Linear Regression\nConsider the time series defined by \\[Y_t= a+ bt + e_t, \\quad e_t \\overset{iid}{\\sim} N(0,\\sigma_e^2).\\] Remark: In time series models, such \\(e_t\\) terms are called innovation terms, error terms, or noise terms. For this example,\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[a+ bt + e_t] = a+ bt.\\)\nACVF:\n\n\\[\\gamma_{t,s} = Cov(a+ bt + e_t, a+ bs + e_s) = Cov(e_t, e_s) = \\begin{cases}\n0, &\\text{if } t\\neq s\\\\\n\\sigma_e^2, &\\text{if } t=s.\n\\end{cases}\\]\n\nACF:\n\n\\[\\rho_{t,s} = \\begin{cases}\n0, &\\text{if } t\\neq s\\\\\n1, &\\text{if } t=s.\n\\end{cases}\\]",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture01.html#example-2-random-walk",
    "href": "TSA-Lecture01.html#example-2-random-walk",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "3.2 Example 2: Random Walk",
    "text": "3.2 Example 2: Random Walk\nLet \\(e_1,e_2,e_3,\\dots \\sim IID(0,\\sigma_e^2)\\), which means they are iid random variables with mean \\(0\\) and variance \\(\\sigma_e^2\\). Define Random Walk as follows: \\[\n\\begin{split}\n&Y_0 = 0 \\\\\n&Y_1 = e_1 \\\\\n&Y_2 = e_1+e_2 \\\\\n&Y_3 = e_1+e_2+e_3 \\\\\n&\\cdots\n\\end{split}\n\\] Remark: Alternatively, it can be defined by \\(Y_0=0\\) and \\(Y_{t+1}= Y_t+ e_{t+1}\\). For this example,\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = \\mathbb{E}[e_1+\\cdots+e_t] = 0.\\)\nVariance function: Using the dependence between the \\(e_t\\) terms, we have\n\n\\[\\gamma_{t,t} = Var(Y_t) = Var(e_1+\\cdots+e_t) = \\sum_{i=1}^t Var(e_i) + 2\\sum_{1\\le i&lt;j\\le t} Cov(e_i,e_j) = t \\sigma_e^2.\\] So this variance function grows linearly with time \\(t\\).\n\nACVF: Suppose \\(1\\le s\\le t\\), then we have\n\n\\[\\begin{split}\n\\gamma_{s,t} &= Cov(Y_s,Y_t) = Cov(Y_s, Y_s+(Y_t-Y_s)) \\\\\n&= Var(Y_s) + Cov(Y_s, e_{s+1}+ e_{s+2}+ \\cdots +e_t) = s \\sigma_e^2.\n\\end{split}\\] where the last step is because \\(Y_s\\) is independent of the terms \\(e_{s+1},\\dots,e_t\\). If \\(1\\le t\\le s\\), we can derive \\(\\gamma_{s,t} = t \\sigma_e^2\\) in the same way. So we conclude that \\(\\gamma_{s,t} = \\min\\{s,t\\} \\cdot \\sigma_e^2\\) for any \\(s,t\\).\n\nACF: Using ACVF, \\(\\rho_{s,t}= \\frac{\\gamma_{s,t}}{ \\sqrt{\\gamma_{s,s}\\cdot \\gamma_{t,t}}} = \\frac{\\min\\{s,t\\} \\cdot \\sigma_e^2}{\\sqrt{s \\sigma_e^2} \\sqrt{t \\sigma_e^2}} = \\frac{\\min\\{s,t\\}}{ \\sqrt{st}}\\). It can also be rewritten as \\(\\rho_{s,t} = \\min\\{ \\sqrt{\\frac{s}{t}}, \\sqrt{\\frac{t}{s}}\\}\\). Example: \\(\\rho_{1,2}= \\sqrt{\\frac{1}{2}}\\), \\(\\rho_{2,3}= \\sqrt{\\frac{2}{3}}\\). Some further observations: \\(\\rho_{t,t+1}= \\sqrt{\\frac{t}{t+1}} \\to 1\\) as \\(t\\to\\infty\\); \\(\\rho_{1,t}= \\sqrt{\\frac{1}{t}} \\to 0\\) as \\(t\\to\\infty\\).\n\n\n\n\n\n\nTwo Sample Paths of a Random Walk",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture01.html#example-3-moving-average",
    "href": "TSA-Lecture01.html#example-3-moving-average",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "3.3 Example 3: “Moving Average”",
    "text": "3.3 Example 3: “Moving Average”\nLet \\((e_t) \\sim IID(0,\\sigma_e^2)\\), and define \\(Y_t = \\frac{1}{2} (e_t + e_{t-1})\\). For this example,\n\nMean function: \\(\\mu_t = \\mathbb{E}[Y_t] = \\frac{1}{2} \\mathbb{E}[e_t + e_{t-1}] = 0.\\) Note that this mean function does not depend on time \\(t\\).\nVariance function: \\(\\gamma_{t,t} = Var(Y_t) = Var(\\frac{1}{2} (e_t + e_{t-1})) = \\frac{1}{4} (Var(e_t) + Var(e_{t-1}) + 2 Cov(e_t, e_{t-1})) = \\frac{1}{2} \\sigma_e^2\\). Note that this variance function does not depend on time \\(t\\).\nACVF: The case \\(\\gamma_{t,t}\\) reduces to the variance function. Next, we compute \\(\\gamma_{t,t-1}\\).\n\n\\[\\begin{split}\n\\gamma_{t,t-1} &= Cov(Y_t, Y_{t-1}) = Cov(\\frac{1}{2} (e_t + e_{t-1}), \\frac{1}{2} (e_{t-1} + e_{t-2})) \\\\\n&= \\frac{1}{4} (Cov(e_{t}, e_{t-1}) + Cov(e_{t}, e_{t-2}) + Cov(e_{t-1}, e_{t-1}) + Cov(e_{t-1}, e_{t-2})) = \\frac{1}{4} \\sigma_e^2.\n\\end{split}\\]\nExercise: Show that \\(\\gamma_{t,t-k}=0\\) for \\(k\\ge 2\\).\nCombining all the cases above, we get\n\\[\\gamma_{t,s} = \\begin{cases}\n  \\frac{1}{2} \\sigma_e^2, &\\text{if } t=s \\\\\n  \\frac{1}{4} \\sigma_e^2, &\\text{if } |t-s|=1 \\\\\n  0, &\\text{if } |t-s|\\ge 2.\n  \\end{cases}\\]\n\nACF: Using ACVF, we get\n\\[\\rho_{t,s} = \\begin{cases}\n1 , &\\text{if } t=s \\\\\n\\frac{1}{2} , &\\text{if } |t-s|=1 \\\\\n0, &\\text{if } |t-s|\\ge 2.\n\\end{cases}\\] Note that in this example, both ACVF and ACF only depend on the lag \\(t-s\\).",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture01.html#strict-stationarity",
    "href": "TSA-Lecture01.html#strict-stationarity",
    "title": "25 Spring 439/639 TSA: Lecture 1",
    "section": "4.1 Strict stationarity",
    "text": "4.1 Strict stationarity\nDefinition: A stochastic process is strictly stationary if all finite dimensional joint distributions do not change if their indices are shifted by the same amount. (In other words, finite dimensional joint distributions are time invariant.)\nThis definition is for general stochastic processes. For a time series (suppose the time index set is \\(\\mathbb{Z}\\)), the definition can be stated as follows.\nDefinition: A time series \\((\\dots,Y_{-1},Y_0,Y_1,\\dots)\\) is strictly stationary if for any positive integer \\(n\\), any distinct integers \\(t_1,\\dots,t_n \\in \\mathbb{Z}\\), and any integer \\(k\\in \\mathbb{Z}\\), the following holds \\[F_{Y_{t_1}, \\dots, Y_{t_n}} = F_{Y_{t_1 -k}, \\dots, Y_{t_n -k}},\\] i.e., the joint cdf of \\((Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n})\\) is same as the joint cdf of \\((Y_{t_1 -k}, Y_{t_2 -k}, \\dots, Y_{t_n -k})\\).\nRemark: we may also use the notation \\(\\overset{D}{=}\\) or \\(\\overset{D}{\\equiv}\\) to denote two random variables/vectors have same (joint) distributions.\nBy this definition, a strictly stationary time series must satisfy\n\n\\(Y_1 \\overset{D}{=} Y_2 \\overset{D}{=} Y_3 \\overset{D}{=} Y_t\\) for any \\(t\\);\n\\((Y_1,Y_3) \\overset{D}{=} (Y_2,Y_4) \\overset{D}{=} (Y_{10},Y_{12}) \\overset{D}{=} (Y_t,Y_{t+2})\\) for any \\(t\\);\n\\((Y_1,Y_3,Y_7) \\overset{D}{=} (Y_{10},Y_{12},Y_{16}) \\overset{D}{=} (Y_{t},Y_{t+2},Y_{t+6})\\) for any \\(t\\);\n\\(\\dots\\)\n\n(Note: the listed properties here are for illustration only. They are just some necessary conditions for a strictly stationary time series.)\nExercise: Show that for a strictly stationary time series, its ACVF \\(\\gamma_{t,s}\\) only depends on the lag \\(t-s\\). Moreover, by symmetry of ACVF, we can immediately show \\(\\gamma_{t,s}\\) only depends on \\(|t-s|\\).\nConsequently, for a strictly stationary time series, we can replace the notation \\(\\gamma_{t,s}\\) by \\(\\gamma_{|t-s|}\\) to simplify the notation, since this transformation is well defined by the previous exercise.",
    "crumbs": [
      "Lecture 1"
    ]
  },
  {
    "objectID": "TSA-Lecture03.html",
    "href": "TSA-Lecture03.html",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "",
    "text": "Last time we defined \\(q\\)-dependent and \\(q\\)-correlated time series.\n\n\n\\(q\\)-dependent CLT: Let \\((Y_t)\\) be a \\(q\\)-dependent (\\(q \\geq 0\\)) stationary time series with \\(\\mu = \\mathbb{E}[Y_t]\\) and \\(\\sigma^2 = Var(Y_t)\\). Then as long as \\[\n\\sigma^2 + 2\\operatorname{Cov}(Y_1, Y_2) + \\cdots + 2\\operatorname{Cov}(Y_1, Y_{q+1}) &gt; 0,\n\\]\nthen \\[\n\\frac{\\sum_{i=1}^n Y_i - n\\mu}{\\sqrt{\\mathrm{Var}\\left(\\sum_{i=1}^n Y_i\\right)}} \\xrightarrow{D} N(0, 1), \\quad \\text{as } n\\to \\infty,\n\\] or equivalently, \\[\n\\frac{\\overline{Y} - \\mu}{\\sqrt{\\mathrm{Var}(\\overline{Y})}} \\xrightarrow{D} N(0, 1), \\quad \\text{as } n \\to \\infty.\n\\]\nRemark: the last statement can be loosely thought of as \\(\\overline{Y} \\approx N(\\mu, \\mathrm{Var}(\\overline{Y}))\\).\n\n\n\nSuppose \\(n &gt; q\\). Since \\((Y_t)\\) is stationary, using ACVF we have \\[\n\\begin{split}\n\\operatorname{Var} \\left( \\sum_{i=1}^n Y_i \\right) &=\n    \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(Y_i, Y_j) = \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_{|i-j|} \\\\\n&=n\\gamma_0 + 2(n-1)\\gamma_1 + 2(n-2)\\gamma_2 + \\cdots + 2\\gamma_{n-1} \\\\\n&= n\\gamma_0 + 2\\sum_{j=1}^q (n-j)\\gamma_j\n\\end{split}\n\\] where the last step is because \\(\\gamma_j = 0\\) for \\(j &gt; q\\) (by \\(q\\)-dependence). So \\[\n\\mathrm{Var}(\\overline{Y}) = \\frac{1}{n^2} \\mathrm{Var} \\left( \\sum_{i=1}^n Y_i \\right)\n= \\frac{1}{n} \\gamma_0 + \\frac{2}{n^2} \\sum_{j=1}^{q} (n-j) \\gamma_j .\n\\] As \\(n \\to \\infty\\), this variance \\(\\mathrm{Var}(\\overline{Y}) \\approx \\frac{1}{n}(\\sigma^2 + 2\\operatorname{Cov}(Y_1, Y_2) + \\cdots + 2\\operatorname{Cov}(Y_1, Y_{q+1}))\\), which goes to \\(0\\) at the rate \\(O(\\frac{1}{n})\\). This behavior looks similar to the standard CLT (with iid setting). Then, (following the idea of standard CLT) \\[\n\\frac{\\overline{Y} - \\mu}{\\sqrt{\\mathrm{Var}(\\overline{Y})}} \\xrightarrow{D} N(0,1), \\quad \\text{as } n \\to \\infty.\n\\]\nRemark: the CLT can be even generalized to some time series that are not \\(q\\)-dependent, as long as \\(\\gamma_k\\) decays to zero (as \\(k\\to \\infty\\)) sufficiently fast.",
    "crumbs": [
      "Lecture 3"
    ]
  },
  {
    "objectID": "TSA-Lecture03.html#statement-of-the-theorem",
    "href": "TSA-Lecture03.html#statement-of-the-theorem",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "",
    "text": "\\(q\\)-dependent CLT: Let \\((Y_t)\\) be a \\(q\\)-dependent (\\(q \\geq 0\\)) stationary time series with \\(\\mu = \\mathbb{E}[Y_t]\\) and \\(\\sigma^2 = Var(Y_t)\\). Then as long as \\[\n\\sigma^2 + 2\\operatorname{Cov}(Y_1, Y_2) + \\cdots + 2\\operatorname{Cov}(Y_1, Y_{q+1}) &gt; 0,\n\\]\nthen \\[\n\\frac{\\sum_{i=1}^n Y_i - n\\mu}{\\sqrt{\\mathrm{Var}\\left(\\sum_{i=1}^n Y_i\\right)}} \\xrightarrow{D} N(0, 1), \\quad \\text{as } n\\to \\infty,\n\\] or equivalently, \\[\n\\frac{\\overline{Y} - \\mu}{\\sqrt{\\mathrm{Var}(\\overline{Y})}} \\xrightarrow{D} N(0, 1), \\quad \\text{as } n \\to \\infty.\n\\]\nRemark: the last statement can be loosely thought of as \\(\\overline{Y} \\approx N(\\mu, \\mathrm{Var}(\\overline{Y}))\\).",
    "crumbs": [
      "Lecture 3"
    ]
  },
  {
    "objectID": "TSA-Lecture03.html#sketch-of-the-proof",
    "href": "TSA-Lecture03.html#sketch-of-the-proof",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "",
    "text": "Suppose \\(n &gt; q\\). Since \\((Y_t)\\) is stationary, using ACVF we have \\[\n\\begin{split}\n\\operatorname{Var} \\left( \\sum_{i=1}^n Y_i \\right) &=\n    \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(Y_i, Y_j) = \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_{|i-j|} \\\\\n&=n\\gamma_0 + 2(n-1)\\gamma_1 + 2(n-2)\\gamma_2 + \\cdots + 2\\gamma_{n-1} \\\\\n&= n\\gamma_0 + 2\\sum_{j=1}^q (n-j)\\gamma_j\n\\end{split}\n\\] where the last step is because \\(\\gamma_j = 0\\) for \\(j &gt; q\\) (by \\(q\\)-dependence). So \\[\n\\mathrm{Var}(\\overline{Y}) = \\frac{1}{n^2} \\mathrm{Var} \\left( \\sum_{i=1}^n Y_i \\right)\n= \\frac{1}{n} \\gamma_0 + \\frac{2}{n^2} \\sum_{j=1}^{q} (n-j) \\gamma_j .\n\\] As \\(n \\to \\infty\\), this variance \\(\\mathrm{Var}(\\overline{Y}) \\approx \\frac{1}{n}(\\sigma^2 + 2\\operatorname{Cov}(Y_1, Y_2) + \\cdots + 2\\operatorname{Cov}(Y_1, Y_{q+1}))\\), which goes to \\(0\\) at the rate \\(O(\\frac{1}{n})\\). This behavior looks similar to the standard CLT (with iid setting). Then, (following the idea of standard CLT) \\[\n\\frac{\\overline{Y} - \\mu}{\\sqrt{\\mathrm{Var}(\\overline{Y})}} \\xrightarrow{D} N(0,1), \\quad \\text{as } n \\to \\infty.\n\\]\nRemark: the CLT can be even generalized to some time series that are not \\(q\\)-dependent, as long as \\(\\gamma_k\\) decays to zero (as \\(k\\to \\infty\\)) sufficiently fast.",
    "crumbs": [
      "Lecture 3"
    ]
  },
  {
    "objectID": "TSA-Lecture03.html#backshift-operator",
    "href": "TSA-Lecture03.html#backshift-operator",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "3.1 Backshift operator",
    "text": "3.1 Backshift operator\nDefinition: For a sequence \\((y_t)\\), the backshift operator \\(B\\) is defined by \\(B Y_t = Y_{t-1}\\).\nExample: \\(B^2 Y_t = B(Y_{t-1}) = Y_{t-2}\\), \\(B^k Y_t = Y_{t-k}\\) for \\(k \\ge 0\\).\nThe inverse of \\(B\\) is considered as forwardshift: \\(B^{-1} Y_t = Y_{t+1}\\), \\(B^{-2} Y_t = Y_{t+2}\\), etc.\nDefinition: A linear filter is an operator defined as \\[\n\\Psi(B) = \\sum_{j=-\\infty}^{+\\infty} \\psi_j B^j.\n\\] By this definition, the GLP process \\(Y_t = \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\) can be written as \\(Y_t = \\Psi(B) e_t\\), since \\[\n\\left(\\sum_{j=-\\infty}^{+\\infty} \\psi_j B^j\\right) e_t\n= \\sum_{j=-\\infty}^{+\\infty} \\psi_j B^j e_t\n= \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j} = Y_t .\n\\]",
    "crumbs": [
      "Lecture 3"
    ]
  },
  {
    "objectID": "TSA-Lecture03.html#glp-is-stationary",
    "href": "TSA-Lecture03.html#glp-is-stationary",
    "title": "25 Spring 439/639 TSA: Lecture 3",
    "section": "3.2 GLP is stationary",
    "text": "3.2 GLP is stationary\nIn this part, we will derive the mean function, variance function, ACVF, ACF of a GLP. Then we can see that a GLP is stationary.\n\nMean function \\[\n\\mu_t = \\mathbb{E} [Y_t] = \\mathbb{E}\\left[\\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\right]\n= \\sum_{j=-\\infty}^{+\\infty} \\psi_j \\mathbb{E}[e_{t-j}] = 0.\n\\]\nVariance function \\[\n\\begin{split}\n\\operatorname{Var}(Y_t) &= \\operatorname{Var}\\left(\\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\right)\n= \\sum_{j=-\\infty}^{+\\infty} \\psi_j^2 \\operatorname{Var}(e_{t-j}) + \\ 2 \\sum_{i&lt;j} \\psi_i \\psi_j \\operatorname{Cov}(e_{t-i}, e_{t-j}) = \\sigma_e^2 \\left( \\sum_{j=-\\infty}^{+\\infty} \\psi_j^2 \\right)\n\\end{split}\n\\] which is a finite constant (see the following exercise).\n\nExercise: Show that \\(\\sum_{j=-\\infty}^{+\\infty} |\\psi_j| &lt; \\infty\\) implies \\(\\sum_{j=-\\infty}^{+\\infty} \\psi_j^2 &lt; \\infty\\) (i.e., absolute summability implies square summability/convergence).\n\nACVF \\[\n\\begin{split}\n\\operatorname{Cov}(Y_t, Y_{t+k}) &= \\mathbb{E}\\left[Y_t Y_{t+k}\\right] - \\left(\\mathbb{E} Y_t\\right)\\left(\\mathbb{E} Y_{t+k}\\right) = \\mathbb{E}\\left[Y_t Y_{t+k}\\right]\n= \\mathbb{E}\\left[\n  \\left( \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j} \\right)\n  \\left( \\sum_{i=-\\infty}^{+\\infty} \\psi_i e_{t+k-i} \\right)\n\\right] \\\\\n& = \\sum_{i=-\\infty}^{+\\infty} \\sum_{j=-\\infty}^{+\\infty} \\psi_i \\psi_j \\mathbb{E}[e_{t-j} e_{t+k-i}] .\n\\end{split}\n\\] Note that \\(\\mathbb{E}[e_{t-j} e_{t+k-i}] = \\mathbb{E}[e_{t-j}] \\cdot \\mathbb{E}[e_{t+k-i}] = 0\\) if \\(i \\neq j+k\\), and \\(\\mathbb{E}[e_{t-j} e_{t+k-i}] = \\sigma_e^2\\) if \\(i=j+k\\). So the ACVF depends only on the lag \\(k\\): \\[\n\\gamma_k = \\sum_{j=-\\infty}^{+\\infty} \\psi_{k+j}\\psi_j \\sigma_e^2 .\n\\]\nACF \\[\n\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\n= \\frac{ \\displaystyle\\sum_{j=-\\infty}^{+\\infty} \\psi_{k+j} \\psi_j }\n      { \\displaystyle\\sum_{j=-\\infty}^{+\\infty} \\psi_j^2 } .\n\\]\n\nRemark: If we want a GLP (\\(Y_t\\)) with mean \\(\\mu\\), then just add \\(\\mu\\). Let \\(Y_t = \\mu + \\sum_{j=-\\infty}^{+\\infty} \\psi_j e_{t-j}\\). ACVF, ACF remain the same.",
    "crumbs": [
      "Lecture 3"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html",
    "href": "TSA-Lecture05.html",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "Recall last time, from the AR(\\(p\\)) \\[\nY_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2} - \\cdots - \\phi_p Y_{t-p} = e_t, \\quad\ne_t \\sim \\mathrm{iid}(0, \\sigma_e^2),\n\\] we got the reformulated form \\(\\Phi(B) Y_t = e_t\\), where the AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p\\). If all the \\(p\\) complex roots of the AR polynomial satisfy \\(|z_i| &gt;1\\) for all \\(i=1,...,p\\), i.e., all the \\(p\\) roots are outside the unit disc in \\(\\mathbb{C}\\), then we showed that the AR(\\(p\\)) equation became \\[\ne_t = \\left(1-\\frac{B}{z_1}\\right) \\cdots \\left(1-\\frac{B}{z_p}\\right) Y_t .\n\\] Since each \\(|\\frac{1}{z_i}|&lt;1\\), we have \\(\\left(1-\\frac{B}{z_i}\\right) ^{-1} = \\sum_{j=0}^\\infty z_i^{-j} B^j\\). Then \\[\n\\begin{split}\nY_t &= \\left(1-\\frac{B}{z_1}\\right)^{-1} \\cdots \\left(1-\\frac{B}{z_p}\\right)^{-1} e_t \\\\\n&= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) e_t .\n\\end{split}\n\\] As we mentioned last time, this product can be written as a GLP. To see this, we first consider the simple case \\(p=2\\). Suppose \\(a_j = z_1^{-j}\\) and \\(b_j = z_2^{-j}\\). Then \\[\n\\begin{split}\n& \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\left(\\sum_{j=0}^\\infty z_2^{-j} B^j \\right) = \\left(\\sum_{j=0}^\\infty a_j B^j \\right) \\left(\\sum_{j=0}^\\infty b_j B^j \\right) \\\\\n&= \\left(a_0 + a_1 B^1 + a_2 B^2 + \\cdots \\right) \\left(b_0 + b_1 B^1 + b_2 B^2 + \\cdots \\right) \\\\\n&= a_0 b_0 + (a_0 b_1 + a_1 b_0) B^1 + (a_0 b_2 + a_1 b_1 + a_2 b_0) B^2 + (a_0 b_3 + a_1 b_2 + a_2 b_1 + a_3 b_0) B^3 + \\cdots \\\\\n&= \\sum_{n=0}^\\infty \\underbrace{\\left( \\sum_{j_1+j_2 = n} a_{j_1} b_{j_2} \\right)}_{c_n} B^n .\n\\end{split}\n\\] For general \\(p\\), we have the similar result \\[\n\\left(\\sum_{j=0}^\\infty a_{1,j} B^j \\right) \\left(\\sum_{j=0}^\\infty a_{2,j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty a_{p,j} B^j \\right) = \\sum_{n=0}^\\infty c_n B^n\n\\] where \\(c_n = \\sum_{j_1+j_2+\\cdots + j_p = n} a_{1,j_1} a_{2,j_2} \\cdots a_{p,j_p}\\). Note: this can be seen as a \\(p\\)-fold convolution. Using this result, \\[\n\\begin{split}\n& \\Phi(B)^{-1}\n= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) = \\sum_{n=0}^\\infty \\psi_n B^n ,\\\\\n& \\text{ where}\\quad \\psi_n = \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p} .\n\\end{split}\n\\] So the AR(\\(p\\)) can be written as \\[\n\\begin{split}\nY_t\n&= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) e_t = \\left(\\sum_{n=0}^\\infty \\psi_n B^n \\right) e_t = \\sum_{n=0}^\\infty \\psi_n e_{t-n}\n\\end{split}\n\\] which looks like a GLP, with the \\(\\{\\psi_n\\}\\) specified above. To ensure this is a GLP, we still need to verify \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\) (see the definition of GLP).\n\n\n\nThe sketch of the proof for \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\): \\[\n\\begin{split}\n\\sum_{n=0}^\\infty |\\psi_n| &= \\sum_{n=0}^\\infty \\left| \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p} \\right| \\le \\sum_{n=0}^\\infty \\sum_{j_1+j_2+\\cdots + j_p = n} |z_1|^{-j_i} |z_2|^{-j_2} \\cdots |z_p|^{-j_p} \\\\\n&\\le \\sum_{n=0}^\\infty \\sum_{j_1+j_2+\\cdots + j_p = n} |z^*|^{-j_1-j_2-\\cdots - j_p}\n\\le \\sum_{n=0}^\\infty c(p) n^p |z^*|^{-n} &lt; \\infty .\n\\end{split}\n\\] Notes on the missing details (without proof):\n\nLet \\(z^*\\) be the root among \\(\\{z_1,\\dots, z_p\\}\\) with the smallest modulus, so \\(1&lt;|z^*| \\le \\min\\{ |z_1|,\\dots, |z_p| \\}\\).\nThe number of terms in the summation \\(\\sum_{j_1+j_2+\\cdots + j_p = n}\\) can be upper bounded by \\(c(p) n^p\\) where \\(c(p)\\) is a constant that only depends on \\(p\\). (Since \\({n+p-1\\choose p-1} = \\frac{(n+p-1)(n+p-2) \\cdots (n+1)}{ (p-1)!}\\) is a polynomial of \\(n\\) with degree less than \\(p\\) and coefficients only depend on \\(p\\).)\nThe last step is because \\(\\sum_{n=0}^\\infty n^p |z^*|^{-n}\\) converges for \\(|z^*| &gt;1\\).\n\nSo we just showed \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\), which finishes the proof that \\(Y_t = \\Phi(B)^{-1} e_t\\) is a GLP (as long as all roots of \\(\\Phi(x)\\) are outside the unit disc in \\(\\mathbb{C}\\)).\n\n\n\nIn summary, for AR(\\(p\\)) process, we have the following results. (Look similar to the three cases for AR(\\(1\\)) from last lecture.)\n\nIf all roots of the AR polynomial are outside the unit disc (\\(|z_i| &gt; 1\\) for all \\(i\\)), then \\((Y_t)\\) is causal and stationary.\nIf at least one of the roots is inside the unit disc (\\(|z_i| &lt; 1\\) for one \\(z_i\\)), and none of the roots is on the unit circle (\\(|z_i| \\neq 1\\) for all \\(i\\)), then \\((Y_t)\\) is non-causal (future-dependent) and stationary.\nIf at least one root is a unit root (i.e. \\(|z_i| = 1\\) for one \\(z_i\\)), then \\((Y_t)\\) is not stationary.\n\n\n\n\nWe start with a concrete AR(\\(2\\)) example. Suppose the AR(\\(2\\)) equation is \\[\nY_t = Y_{t-1} + Y_{t-2} + e_t .\n\\] We can use the previous results to find whether this process is causal or stationary. The AR polynomial for this example is \\(\\Phi(x) = 1 - x - x^2\\). Solving \\(\\Phi(x)=0\\), we get two roots \\[\nz_{1,2} = \\frac{-1 \\pm \\sqrt{1 + 4}}{2} = \\frac{-1 \\pm \\sqrt{5}}{2} .\n\\] Since \\(\\left| \\frac{-1 - \\sqrt{5}}{2} \\right| &gt;1\\) and \\(\\left| \\frac{-1 + \\sqrt{5}}{2} \\right| &lt;1\\), this process is stationary but non-causal. Note: in this example \\(z_{1,2}\\) are both real, so the modulus \\(|z|\\) reduces to the absolute value of real number.\nIn general, for a generic AR(\\(2\\)) equation \\[\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t ,\n\\] the AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x - \\phi_2 x^2\\), which has two roots \\[\nz_{1,2} = \\frac{-\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{2\\phi_2} .\n\\] The causality condition can be explicitly characterized by \\[\n|z_{1,2}| &gt; 1 \\iff\n\\begin{cases}\n\\phi_1 + \\phi_2 &lt; 1 \\\\\n\\phi_2 - \\phi_1 &lt; 1 \\\\\n|\\phi_2| &lt; 1\n\\end{cases}\n\\]",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html#derive-the-glp-form",
    "href": "TSA-Lecture05.html#derive-the-glp-form",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "Recall last time, from the AR(\\(p\\)) \\[\nY_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2} - \\cdots - \\phi_p Y_{t-p} = e_t, \\quad\ne_t \\sim \\mathrm{iid}(0, \\sigma_e^2),\n\\] we got the reformulated form \\(\\Phi(B) Y_t = e_t\\), where the AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x^1 - \\phi_2 x^2 - \\cdots - \\phi_p x^p\\). If all the \\(p\\) complex roots of the AR polynomial satisfy \\(|z_i| &gt;1\\) for all \\(i=1,...,p\\), i.e., all the \\(p\\) roots are outside the unit disc in \\(\\mathbb{C}\\), then we showed that the AR(\\(p\\)) equation became \\[\ne_t = \\left(1-\\frac{B}{z_1}\\right) \\cdots \\left(1-\\frac{B}{z_p}\\right) Y_t .\n\\] Since each \\(|\\frac{1}{z_i}|&lt;1\\), we have \\(\\left(1-\\frac{B}{z_i}\\right) ^{-1} = \\sum_{j=0}^\\infty z_i^{-j} B^j\\). Then \\[\n\\begin{split}\nY_t &= \\left(1-\\frac{B}{z_1}\\right)^{-1} \\cdots \\left(1-\\frac{B}{z_p}\\right)^{-1} e_t \\\\\n&= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) e_t .\n\\end{split}\n\\] As we mentioned last time, this product can be written as a GLP. To see this, we first consider the simple case \\(p=2\\). Suppose \\(a_j = z_1^{-j}\\) and \\(b_j = z_2^{-j}\\). Then \\[\n\\begin{split}\n& \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\left(\\sum_{j=0}^\\infty z_2^{-j} B^j \\right) = \\left(\\sum_{j=0}^\\infty a_j B^j \\right) \\left(\\sum_{j=0}^\\infty b_j B^j \\right) \\\\\n&= \\left(a_0 + a_1 B^1 + a_2 B^2 + \\cdots \\right) \\left(b_0 + b_1 B^1 + b_2 B^2 + \\cdots \\right) \\\\\n&= a_0 b_0 + (a_0 b_1 + a_1 b_0) B^1 + (a_0 b_2 + a_1 b_1 + a_2 b_0) B^2 + (a_0 b_3 + a_1 b_2 + a_2 b_1 + a_3 b_0) B^3 + \\cdots \\\\\n&= \\sum_{n=0}^\\infty \\underbrace{\\left( \\sum_{j_1+j_2 = n} a_{j_1} b_{j_2} \\right)}_{c_n} B^n .\n\\end{split}\n\\] For general \\(p\\), we have the similar result \\[\n\\left(\\sum_{j=0}^\\infty a_{1,j} B^j \\right) \\left(\\sum_{j=0}^\\infty a_{2,j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty a_{p,j} B^j \\right) = \\sum_{n=0}^\\infty c_n B^n\n\\] where \\(c_n = \\sum_{j_1+j_2+\\cdots + j_p = n} a_{1,j_1} a_{2,j_2} \\cdots a_{p,j_p}\\). Note: this can be seen as a \\(p\\)-fold convolution. Using this result, \\[\n\\begin{split}\n& \\Phi(B)^{-1}\n= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) = \\sum_{n=0}^\\infty \\psi_n B^n ,\\\\\n& \\text{ where}\\quad \\psi_n = \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p} .\n\\end{split}\n\\] So the AR(\\(p\\)) can be written as \\[\n\\begin{split}\nY_t\n&= \\left(\\sum_{j=0}^\\infty z_1^{-j} B^j \\right) \\cdots \\left(\\sum_{j=0}^\\infty z_p^{-j} B^j \\right) e_t = \\left(\\sum_{n=0}^\\infty \\psi_n B^n \\right) e_t = \\sum_{n=0}^\\infty \\psi_n e_{t-n}\n\\end{split}\n\\] which looks like a GLP, with the \\(\\{\\psi_n\\}\\) specified above. To ensure this is a GLP, we still need to verify \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\) (see the definition of GLP).",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html#sketch-of-the-remaining-proof",
    "href": "TSA-Lecture05.html#sketch-of-the-remaining-proof",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "The sketch of the proof for \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\): \\[\n\\begin{split}\n\\sum_{n=0}^\\infty |\\psi_n| &= \\sum_{n=0}^\\infty \\left| \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p} \\right| \\le \\sum_{n=0}^\\infty \\sum_{j_1+j_2+\\cdots + j_p = n} |z_1|^{-j_i} |z_2|^{-j_2} \\cdots |z_p|^{-j_p} \\\\\n&\\le \\sum_{n=0}^\\infty \\sum_{j_1+j_2+\\cdots + j_p = n} |z^*|^{-j_1-j_2-\\cdots - j_p}\n\\le \\sum_{n=0}^\\infty c(p) n^p |z^*|^{-n} &lt; \\infty .\n\\end{split}\n\\] Notes on the missing details (without proof):\n\nLet \\(z^*\\) be the root among \\(\\{z_1,\\dots, z_p\\}\\) with the smallest modulus, so \\(1&lt;|z^*| \\le \\min\\{ |z_1|,\\dots, |z_p| \\}\\).\nThe number of terms in the summation \\(\\sum_{j_1+j_2+\\cdots + j_p = n}\\) can be upper bounded by \\(c(p) n^p\\) where \\(c(p)\\) is a constant that only depends on \\(p\\). (Since \\({n+p-1\\choose p-1} = \\frac{(n+p-1)(n+p-2) \\cdots (n+1)}{ (p-1)!}\\) is a polynomial of \\(n\\) with degree less than \\(p\\) and coefficients only depend on \\(p\\).)\nThe last step is because \\(\\sum_{n=0}^\\infty n^p |z^*|^{-n}\\) converges for \\(|z^*| &gt;1\\).\n\nSo we just showed \\(\\sum_{n=0}^\\infty |\\psi_n| &lt; \\infty\\), which finishes the proof that \\(Y_t = \\Phi(B)^{-1} e_t\\) is a GLP (as long as all roots of \\(\\Phi(x)\\) are outside the unit disc in \\(\\mathbb{C}\\)).",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html#discussion-on-other-cases-without-proof",
    "href": "TSA-Lecture05.html#discussion-on-other-cases-without-proof",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "In summary, for AR(\\(p\\)) process, we have the following results. (Look similar to the three cases for AR(\\(1\\)) from last lecture.)\n\nIf all roots of the AR polynomial are outside the unit disc (\\(|z_i| &gt; 1\\) for all \\(i\\)), then \\((Y_t)\\) is causal and stationary.\nIf at least one of the roots is inside the unit disc (\\(|z_i| &lt; 1\\) for one \\(z_i\\)), and none of the roots is on the unit circle (\\(|z_i| \\neq 1\\) for all \\(i\\)), then \\((Y_t)\\) is non-causal (future-dependent) and stationary.\nIf at least one root is a unit root (i.e. \\(|z_i| = 1\\) for one \\(z_i\\)), then \\((Y_t)\\) is not stationary.",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html#example-ar2",
    "href": "TSA-Lecture05.html#example-ar2",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "",
    "text": "We start with a concrete AR(\\(2\\)) example. Suppose the AR(\\(2\\)) equation is \\[\nY_t = Y_{t-1} + Y_{t-2} + e_t .\n\\] We can use the previous results to find whether this process is causal or stationary. The AR polynomial for this example is \\(\\Phi(x) = 1 - x - x^2\\). Solving \\(\\Phi(x)=0\\), we get two roots \\[\nz_{1,2} = \\frac{-1 \\pm \\sqrt{1 + 4}}{2} = \\frac{-1 \\pm \\sqrt{5}}{2} .\n\\] Since \\(\\left| \\frac{-1 - \\sqrt{5}}{2} \\right| &gt;1\\) and \\(\\left| \\frac{-1 + \\sqrt{5}}{2} \\right| &lt;1\\), this process is stationary but non-causal. Note: in this example \\(z_{1,2}\\) are both real, so the modulus \\(|z|\\) reduces to the absolute value of real number.\nIn general, for a generic AR(\\(2\\)) equation \\[\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + e_t ,\n\\] the AR polynomial is \\(\\Phi(x) = 1 - \\phi_1 x - \\phi_2 x^2\\), which has two roots \\[\nz_{1,2} = \\frac{-\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{2\\phi_2} .\n\\] The causality condition can be explicitly characterized by \\[\n|z_{1,2}| &gt; 1 \\iff\n\\begin{cases}\n\\phi_1 + \\phi_2 &lt; 1 \\\\\n\\phi_2 - \\phi_1 &lt; 1 \\\\\n|\\phi_2| &lt; 1\n\\end{cases}\n\\]",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html#method-1-using-convolution",
    "href": "TSA-Lecture05.html#method-1-using-convolution",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "3.1 Method 1: using convolution",
    "text": "3.1 Method 1: using convolution\nWe can use the formula \\(\\psi_n = \\sum_{j_1+j_2+\\cdots + j_p = n} z_1^{-j_i} z_2^{-j_2} \\cdots z_p^{-j_p}\\) from the first part of this lecture.\nExample: consider the AR(\\(2\\)) equation \\(Y_t = \\frac{1}{6} Y_{t-1} + \\frac{1}{6} Y_{t-2} + e_t\\).\nExercise: verify this AR(\\(2\\)) process is causal, and the roots of the AR polynomial are \\(\\{-3, 2\\}\\).\nThen we can use the formula above to calculate \\(\\psi_n\\): \\[\n\\begin{split}\n\\psi_0 &= z_1^0 z_2^0 = 1 \\\\\n\\psi_1 &= z_1^0 z_2^{-1} + z_1^{-1} z_2^0 = 1 \\times \\frac{1}{2} + \\frac{1}{-3} \\times 1 = \\frac{1}{2} + \\left(-\\frac{1}{3}\\right) = \\frac{1}{6} \\\\\n\\psi_2 &= z_1^0 z_2^{-2} + z_1^{-1} z_2^{-1} + z_1^{-2} z_2^0 = \\frac{1}{4} + \\frac{1}{-3} \\cdot \\frac{1}{2} + \\frac{1}{9} = \\frac{7}{36} \\\\\n\\cdots\n\\end{split}\n\\]",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html#method-2-using-arp-equation",
    "href": "TSA-Lecture05.html#method-2-using-arp-equation",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "3.2 Method 2: using AR(\\(p\\)) equation",
    "text": "3.2 Method 2: using AR(\\(p\\)) equation\nWe can use the AR(\\(p\\)) equation and directly solve a system for \\(\\{\\psi_n\\}\\). Consider the same example \\[\nY_t = \\frac{1}{6} Y_{t-1} + \\frac{1}{6} Y_{t-2} + e_t .\n\\] Since we want to get the GLP form \\(Y_t = \\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots\\), we can plug it into the equation above: \\[\n\\psi_0 e_t + \\psi_1 e_{t-1} + \\psi_2 e_{t-2} + \\cdots = \\frac{1}{6} (\\psi_0 e_{t-1} + \\psi_1 e_{t-2} + \\psi_2 e_{t-3} + \\cdots) + \\frac{1}{6} (\\psi_0 e_{t-2} + \\psi_1 e_{t-3} + \\psi_2 e_{t-4} + \\cdots) + e_t .\n\\] For this to hold, the coefficients for each \\(e_{t-n}\\) term (\\(n=0,1,\\dots\\)) on both sides should match. So we get the following system of equations \\[\n\\begin{split}\n\\psi_0 &= 1 \\\\\n\\psi_1 &= \\frac{1}{6} \\psi_0 \\\\\n\\psi_n &= \\frac{1}{6} \\psi_{n-1} + \\frac{1}{6} \\psi_{n-2} \\quad\\text{for } n\\ge 2\n\\end{split}\n\\] So \\(\\psi_0 = 1\\), \\(\\psi_1 = \\frac{1}{6} \\psi_0 = \\frac{1}{6}\\), \\(\\psi_2 = \\frac{1}{6} \\psi_1 + \\frac{1}{6} \\psi_0 = \\frac{1}{36} + \\frac{1}{6} = \\frac{7}{36}\\). These results are same as the earlier convolution method.",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html#a-property-of-the-recursion-part",
    "href": "TSA-Lecture05.html#a-property-of-the-recursion-part",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "4.1 A property of the recursion part",
    "text": "4.1 A property of the recursion part\nIf \\(k\\) is large, solving \\(\\gamma_k\\) recursively from \\((\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\) may be hard. We have the following useful property.\nClaim: If the roots of the AR polynomial \\(\\Phi(x)\\) are distinct, then there exist (complex) numbers \\(A_1, \\dots, A_p\\), such that the solution to \\[\n\\begin{cases}\n\\text{initial conditions for } (\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\\\\n\\text{recursion equation: } \\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2} + \\cdots + \\phi_p \\gamma_{k-p}, \\quad \\forall k\\ge p\n\\end{cases}\n\\] is given by \\[\n\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}, \\quad \\forall k \\ge 0 .\n\\]\nThis gives another idea to calculate \\(\\gamma_k\\) in the final step of the YW method (useful when we are targeting for a large \\(k\\)): after getting the values of \\((\\gamma_0,\\gamma_1, \\dots, \\gamma_p)\\), we can solve \\((A_1,...,A_p)\\) such that \\(A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k} = \\gamma_k\\) hold for all \\(0\\le k\\le p-1\\). Then for this solution \\((A_1,...,A_p)\\), the formula \\(\\gamma_k = A_1 z_1^{-k} + A_2 z_2^{-k} + \\cdots + A_p z_p^{-k}\\) (for \\(k\\ge 0\\)) gives the ACVF we wanted.",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture05.html#a-related-observation",
    "href": "TSA-Lecture05.html#a-related-observation",
    "title": "25 Spring 439/639 TSA: Lecture 5",
    "section": "4.2 A related observation",
    "text": "4.2 A related observation\nFor causal AR(\\(p\\)) process, the ACVF has the following asymptotic rate (up to some constant factor) \\[\n\\gamma_k \\approx \\Bigl[ \\min \\left\\{ |z_1|, \\ldots, |z_p| \\right\\} \\Bigr]^{-k}, \\text{ as } k\\to \\infty.\n\\] It decays exponentially, and never stays at zero for a ``long time”.\nExample: if the AR polynomial for an AR(\\(3\\)) has roots \\(\\{z_1 = 2, z_2=3, z_3=10\\}\\), then \\(\\gamma_k \\approx 2^{-k}\\) for large \\(k\\).\nNote: should be \\(\\gamma_k \\approx c \\cdot \\left[ \\min \\left\\{ |z_1|, \\ldots, |z_p| \\right\\} \\right]^{-k}\\).",
    "crumbs": [
      "Lecture 5"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html",
    "href": "TSA-Lecture07-plots.html",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "",
    "text": "This R notebook is based on a book by Cryer and Chen, “Time Series Analysis: With Applications in R” (2nd edition). We will reproduce some of the plots from the Chapters 1 and 4. These will cover the following topics:",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#los-angeles-annual-rainfall",
    "href": "TSA-Lecture07-plots.html#los-angeles-annual-rainfall",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "0.1 Los Angeles Annual Rainfall",
    "text": "0.1 Los Angeles Annual Rainfall\n\n\nCode\n#install.packages(\"TSA\")\nlibrary(TSA)\ndata(larain)\nplot(larain, type = \"o\", col = \"blue\", xlab = \"Year\", ylab = \"Rainfall (inches)\", \n     main = \"Annual Rainfall in Los Angeles\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Lag plot\nplot(y=larain,\n     x=zlag(larain),\n     ylab='Inches',\n     xlab='Previous Year Inches',\n     col = 'blue',\n     main='Lag Plot of Annual Rainfall in Los Angeles')",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#color-property-from-chemical-process",
    "href": "TSA-Lecture07-plots.html#color-property-from-chemical-process",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "0.2 Color Property from Chemical Process",
    "text": "0.2 Color Property from Chemical Process\n\n\nCode\ndata(color)\nplot(color,\n     ylab='Color Property',\n     xlab='Batch',\n     type='o',\n     main='Color Property from Chemical Process',\n     col = 'blue')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Lag plot\nplot(y=color,\n     x=zlag(color),\n     ylab='Color Property',\n     xlab='Previous Batch Color Property', \n     col = 'blue',\n     main='Lag Plot of Color Property from Chemical Process')",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#hare-abundance-in-canada",
    "href": "TSA-Lecture07-plots.html#hare-abundance-in-canada",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "0.3 Hare Abundance in Canada",
    "text": "0.3 Hare Abundance in Canada\n\n\nCode\ndata(hare)\nplot(hare,\n     ylab='Abundance',\n     xlab='Year',\n     type='o', col = 'blue',\n     main='Hare Abundance in Canada')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Lag plot\nplot(y=hare,\n     x=zlag(hare),\n     ylab='Abundance',\n     xlab='Previous Year Abundance', \n     col = 'blue',\n     main='Lag Plot of Hare Abundance in Canada')",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#average-monthly-temperatures-dubuque-iowa",
    "href": "TSA-Lecture07-plots.html#average-monthly-temperatures-dubuque-iowa",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "0.4 Average Monthly Temperatures, Dubuque, Iowa",
    "text": "0.4 Average Monthly Temperatures, Dubuque, Iowa\n\n\nCode\n# Average Monthly Temperatures, Dubuque, Iowa\ndata(tempdub) \nplot(tempdub,\n     ylab='Temperature',\n     type='o', \n     col = 'blue',\n     xlab='Month',\n     main='Average Monthly Temperatures, Dubuque, Iowa')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Lag plot\nplot(y=tempdub,\n     x=zlag(tempdub),\n     ylab='Temperature',\n     xlab='Previous Month Temperature', \n     col = 'blue',\n     main='Lag Plot of Average Monthly Temperatures, Dubuque, Iowa')",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#oil-filter-sales",
    "href": "TSA-Lecture07-plots.html#oil-filter-sales",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "0.5 Oil Filter Sales",
    "text": "0.5 Oil Filter Sales\n\n\nCode\ndata(oilfilters) \noilfilters\n\n\n      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n1983                               2385 3302 3958 3302 2441 3107\n1984 5862 4536 4625 4492 4486 4005 3744 2546 1954 2285 1778 3222\n1985 5472 5310 1965 3791 3622 3726 3370 2535 1572 2146 2249 1721\n1986 5357 5811 2436 4608 2871 3349 2909 2324 1603 2148 2245 1586\n1987 5332 5787 2886 5475 3843 2537                              \n\n\nCode\nplot(oilfilters,\n     type='o',\n     ylab='Sales',\n     xlab='Month',\n     main='Oil Filter Sales',\n     col = 'blue')\n\n\n\n\n\n\n\n\n\nCode\nplot(oilfilters,\n     type='l',\n     ylab='Sales', \n     col = 'blue',\n     xlab='Month',\n     main='Oil Filter Sales')\npoints(y=oilfilters,\n      x=time(oilfilters),\n      pch=as.vector(season(oilfilters)),\n      col = 1:4)",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#random-walk",
    "href": "TSA-Lecture07-plots.html#random-walk",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "0.6 Random Walk",
    "text": "0.6 Random Walk\n\n\nCode\nset.seed(439)\ndata(rwalk) # rwalk contains a simulated random walk\nrw2 &lt;- cumsum(rnorm(length(rwalk)))\nrw3 &lt;- cumsum(rnorm(length(rwalk)))\n# plot all three series\nplot(rwalk,\n     type='o',\n     ylab='Random Walk', \n     col = 'blue')\npoints(rw2,\n       type='o',\n       ylab='Random Walk', \n       col = 'red')\npoints(rw3,\n       type='o',\n       ylab='Random Walk', \n       col = 'green')\nlegend('topleft',\n       legend=c('rwalk','rw2','rw3'),\n       col=c('blue','red','green'),\n       lty=1)",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#maq-processes",
    "href": "TSA-Lecture07-plots.html#maq-processes",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "1.1 MA(q) processes",
    "text": "1.1 MA(q) processes\n\n\nCode\n# Lag 1 autocorrelation for MA(1) model\ntheta &lt;- seq(-10,10,by=0.1)\nrho1 &lt;- -theta/(1+theta^2)\nplot(theta,rho1,\n     type='l',\n     ylab='rho(1)',\n     xlab='theta',\n     col = 'blue',\n     main='Lag 1 Autocorrelation for MA(1) model')   \n\n\n\n\n\n\n\n\n\nCode\n# Lag 1 autocorrelation for MA(1) model\ntheta1 &lt;- theta[theta &gt;= -1 & theta &lt;= 1]\nrho11 &lt;- -theta1/(1+theta1^2)\nplot(theta1,rho11,\n     type='l',\n     ylab='rho(1)',\n     xlab='theta',\n     col = 'blue',\n     main='Lag 1 Autocorrelation for MA(1) model')   \n\n\n\n\n\n\n\n\n\n\n\nCode\n# Time plot of an MA(1) process with theta = -0.9\ndata(ma1.2.s)\nplot(ma1.2.s,\n     ylab=expression(Y[t]),\n     type='o', \n     col = 'blue',\n     xlab='Time',\n     main='MA(1) process with theta = -0.9')\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 2), mar = c(4, 4, 2, 2), oma = c(0, 0, 5, 0))\n# Lag plot Yt vs Y_(t-1) for an MA(1) process with theta = -0.9\n# with the lowess smoother\nplot_lag_lowess &lt;- function(ts_data, num_lags, main_title = \"Lag Plots with LOWESS Smoother\") {\n  par(mfrow = c(ceiling(num_lags / 2), 2),  # Arrange plots in a grid\n      mar = c(4, 4, 2, 2),  # Margins: (bottom, left, top, right)\n      oma = c(1, 1, 2, 1),\n      pty = 's')  # Outer margins for better spacing\n\n  for (i in 1:num_lags) {\n    x &lt;- ts_data[1:(length(ts_data) - i)]  # Lagged values\n    y &lt;- ts_data[(i + 1):length(ts_data)]  # Corresponding Y_t values\n\n    # Plot scatter plot\n    plot(x, y, pch = 'o', col = 'blue',\n         #main = paste('Lag Plot Y_t vs Y_(t-', i, ')'),\n         xlab = paste('Y_(t-', i, ')', sep = ''),\n         ylab = 'Y_t',\n         asp = 1)\n\n    # Add LOWESS fitted line\n    lines(lowess(x, y, f = 2/3), col = 'red', lwd = 2)\n\n    # Add main title across all subplots\n     mtext(main_title, outer = TRUE, cex = 1.5, font = 2)\n  }\n # Reset plotting layout and default shape\n  par(mfrow = c(1, 1), pty = \"m\")\n}\n\nplot_lag_lowess(ma1.2.s, num_lags = 4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Time plot of an MA(1) process with theta = +0.9\ndata(ma1.1.s)\nplot(ma1.1.s,\n     ylab=expression(Y[t]),\n     type='o', \n     col = 'blue',\n     xlab='Time',\n     main='MA(1) process with theta = +0.9')\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 2), mar = c(4, 4, 2, 2), oma = c(0, 0, 2, 0))\n# Lag plot Yt vs Y_(t-1) for MA(1) th=+0.9\n# with the lowess smoother\nplot_lag_lowess(ma1.1.s, num_lags = 4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Time Plot of an MA(2) Process with theta1 = 1 and theta2 = -0.6\ndata(ma2.s)\nplot(ma2.s,\n     ylab=expression(Y[t]),\n     type='o',\n     xlab='Time',\n     main='MA(2) process with theta1 = 1 and theta2 = -0.6',\n     col = 'blue')\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 2), oma = c(0, 0, 5, 0))\n\nplot_lag_lowess(ma2.s, num_lags = 4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Auto-correlation function for several MA(1) and MA(2) models\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 2), oma = c(0, 0, 5, 0))\n\n# Function to compute theoretical ACF for MA(1) process\ncompute_acf_ma1 &lt;- function(theta, max_lag = 12) {\n  acf_values &lt;- numeric(max_lag + 1)\n  acf_values[1] &lt;- 1  # ACF at lag 0 is always 1\n  acf_values[2] &lt;- -theta / (1 + theta^2)\n\n for (k in 3:(max_lag + 1)) {\n    acf_values[k] &lt;- 0\n  }\n  return(acf_values)  # Include lag 0\n}\n\n# Function to plot ACF\nplot_acf_ma1 &lt;- function(theta, max_lag = 12) {\n  lags &lt;- 0:max_lag\n  acf_values &lt;- compute_acf_ma1(theta, max_lag)\n\n  plot(lags, acf_values, type = \"h\", lwd = 2, ylim = c(-1, 1),\n       xlab = \"Lag\", ylab = expression(rho[k]), main = \"\", axes = FALSE)\n  points(lags, acf_values, pch = 19, cex = 1.5)\n\n  axis(1, at = lags)\n  axis(2, las = 1)\n  abline(h = 0)\n\n  text(max(lags) * 0.7, \n     max(abs(acf_values), na.rm = TRUE) * 0.8, \n     labels = bquote(theta == .(theta)), \n     cex = 1.2)\n\n   # Label zero ACF values\n  zero_lags &lt;- which(acf_values == 0)\n  text(lags[zero_lags], \n       acf_values[zero_lags], \n       labels = \"0\", \n       pos = 3, \n       cex = 0.8)\n}\n\ncompute_acf_ma2 &lt;- function(theta1, theta2, max_lag = 12) {\n  acf_values &lt;- numeric(max_lag + 1)\n  acf_values[1] &lt;- 1  # ACF at lag 0 is always 1\n\n  acf_values[2] &lt;- -theta1 / (1 + theta1^2 + theta2^2)\n  acf_values[3] &lt;- -theta1 * theta2 / (1 + theta1^2 + theta2^2)\n\n  for (k in 4:(max_lag + 1)) {\n    acf_values[k] &lt;- 0\n  }\n  return(acf_values)  # Include lag 0\n}\n\nplot_acf_ma2 &lt;- function(theta1, theta2, max_lag = 12) {\n  lags &lt;- 0:max_lag\n  acf_values &lt;- compute_acf_ma2(theta1, theta2, max_lag)\n\n  plot(lags, acf_values, type = \"h\", lwd = 2, ylim = c(-1, 1),\n       xlab = \"Lag\", ylab = expression(rho[k]), main = \"\", axes = FALSE)\n  points(lags, acf_values, pch = 19)\n\n  axis(1, at = lags)\n  axis(2, las = 1)\n  abline(h = 0)\n\n  label_text &lt;- bquote(theta[1] == .(theta1) ~ \",\" ~ theta[2] == .(theta2))\n\n  text(max(lags) * 0.7, \n      max(abs(acf_values), na.rm = TRUE) * 0.8, \n      labels = label_text, \n      cex = 1.2)\n  # Label zero ACF values\n  zero_lags &lt;- which(acf_values == 0)\n  text(lags[zero_lags], \n       acf_values[zero_lags], \n       labels = \"0\", \n       pos = 3, \n       cex = 0.8)\n}\n\n# Plot for different theta values\nplot_acf_ma1(0.9)\nplot_acf_ma1(-0.9)\nplot_acf_ma2(0.5, 0.25)\nplot_acf_ma2(1.0, -0.25)\n# Main title\nmtext(\"Theoretical ACFs for various MA(1) and MA(2)\", \n      outer = TRUE, \n      cex = 1.8, \n      line = 2, \n      font = 2)",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#arp-processes",
    "href": "TSA-Lecture07-plots.html#arp-processes",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "1.2 AR(p) processes",
    "text": "1.2 AR(p) processes\n\n\nCode\n# Auto-correlation function for several AR(1) models\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 2), oma = c(0, 0, 5, 0))\n\n# Function to plot theoretical ACF for AR(1) process\nplot_acf_ar1 &lt;- function(phi, max_lag = 12) {\n  lags &lt;- 0:max_lag\n  acf_values &lt;- phi^lags\n  \n  plot(lags, acf_values, type = \"h\", lwd = 2, ylim = c(-1, 1),\n       xlab = \"Lag\", ylab = expression(rho[k]), main = \"\", axes = FALSE)\n  points(lags, acf_values, pch = 19, cex = 1.5)\n  \n  axis(1, at = lags)\n  axis(2, las = 1)\n  abline(h = 0)\n  \n  text(max_lag * 0.7, \n      max(acf_values) * 0.8, \n      labels = bquote(phi == .(phi)), \n      cex = 1.2)\n  # Label zero ACF values\n  zero_lags &lt;- which(acf_values == 0)\n  if (length(zero_lags) &gt; 0)\n  {\n  text(lags[zero_lags], \n       acf_values[zero_lags], \n       labels = \"0\", \n       pos = 3, \n       cex = 0.8)\n  }\n}\n\n# Plot for different phi values\nplot_acf_ar1(0.9)\nplot_acf_ar1(0.4)\nplot_acf_ar1(-0.8)\nplot_acf_ar1(-0.5)\n# Main title\nmtext(\"Theoretical ACFs for various AR(1) Models\", \n      outer = TRUE, \n      cex = 1.8, \n      line = 2, \n      font = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Time plot of an AR(1) process with phi = 0.9\ndata(ar1.s)\nplot(ar1.s,\n     ylab=expression(Y[t]),\n     type='o',\n     xlab='Time',\n     main='AR(1) process with phi = 0.9',\n     col = 'blue')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Lag plots Yt vs Y_(t-k) for an AR(1) process with phi = 0.9\nplot_lag_lowess(ar1.s, num_lags = 4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Auto-correlation function for several AR(2) models\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 2), oma = c(0, 0, 5, 0))\n\n# Function to compute theoretical ACF for AR(2) process\ncompute_acf_ar2 &lt;- function(phi1, phi2, max_lag = 12) {\n  acf_values &lt;- numeric(max_lag + 1)\n  acf_values[1] &lt;- 1  # ACF at lag 0 is always 1\n\n  # Solve Yule-Walker equations for first two autocorrelations\n  acf_values[2] &lt;- phi1 / (1 - phi2)\n  acf_values[3] &lt;- (phi2*(1-phi2) + phi1^2) / (1 - phi2)\n\n  # Recursively compute the rest\n  for (k in 4:(max_lag + 1)) {\n    acf_values[k] &lt;- phi1 * acf_values[k - 1] + phi2 * acf_values[k - 2]\n  }\n\n  return(acf_values)  # Include lag 0\n}\n\n# Function to plot ACF\nplot_acf_ar2 &lt;- function(phi1, phi2, max_lag = 12) {\n  lags &lt;- 0:max_lag\n  acf_values &lt;- compute_acf_ar2(phi1, phi2, max_lag)\n\n  plot(lags, acf_values, type = \"h\", lwd = 2, ylim = c(-1, 1),\n       xlab = \"Lag\", ylab = expression(rho[k]), main = \"\", axes = FALSE)\n  points(lags, acf_values, pch = 19)\n\n  axis(1, at = lags)\n  axis(2, las = 1)\n  abline(h = 0)\n\n  label_text &lt;- bquote(phi[1] == .(phi1) ~ \",\" ~ phi[2] == .(phi2))\n\n  text(max(lags) * 0.7, \n      max(acf_values, na.rm = TRUE) * 0.8, \n      labels = label_text, \n      cex = 1.2)\n  # Label zero ACF values\n  zero_lags &lt;- which(acf_values == 0)\n  if (length(zero_lags) &gt; 0)\n  {\n  text(lags[zero_lags], \n       acf_values[zero_lags], \n       labels = \"0\", \n       pos = 3, \n       cex = 0.8)\n  }\n}\n\n# Plot for different phi1 and phi2 values\nplot_acf_ar2(0.5, 0.25)\nplot_acf_ar2(1.0, -0.25)\nplot_acf_ar2(1.5, -0.75)\nplot_acf_ar2(1.0, -0.6)\n# Main title\nmtext(\"Theoretical ACFs for various AR(2) Models\", \n      outer = TRUE, \n      cex = 1.8, \n      line = 2, \n      font = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Time plot of an AR(2) process with phi1 = 1.5 and phi2 = -0.75\ndata(ar2.s)\nplot(ar2.s,\n     ylab=expression(Y[t]),\n     type='o',\n     xlab='Time',\n     main='AR(2) process with phi1 = 1.5 and phi2 = -0.75',\n     col = 'blue', \n     xaxt = 'n')\naxis(1, at = seq(0, length(ar2.s), by = 12))",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture07-plots.html#armapq-processes",
    "href": "TSA-Lecture07-plots.html#armapq-processes",
    "title": "Lecture 7 R notebook: Time Series Plots",
    "section": "1.3 ARMA(p,q) processes",
    "text": "1.3 ARMA(p,q) processes\n\n\nCode\n# ARMA(1,1) process\ndata(arma11.s)\nplot(arma11.s,\n     ylab=expression(Y[t]),\n     type='o',\n     xlab='Time',\n     main='ARMA(1,1) process with phi = 0.6 and theta = -0.3',\n     col = 'blue')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Lag plots Yt vs Y_(t-k) for an ARMA(1,1) process with phi = 0.6 and theta = -0.3\n\nplot_lag_lowess(arma11.s, num_lags = 4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Auto-correlation function for ARMA(1,1) model\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 2), oma = c(0, 0, 5, 0))\n\n# Function to compute theoretical ACF for ARMA(1,1) process\ncompute_acf_arma11 &lt;- function(phi, theta, max_lag = 12) {\n  acf_values &lt;- numeric(max_lag + 1)\n  acf_values[1] &lt;- 1  # ACF at lag 0 is always 1\n\n\n  # Recursively compute the rest\n  for (k in 2:(max_lag + 1)) {\n    acf_values[k] &lt;- (1-theta*phi)*(phi-theta)*phi^(k-1)/(1-2*theta*phi+theta^2)\n  }\n\n  return(acf_values)  # Include lag 0\n}\n\n# Function to plot ACF\nplot_acf_arma11 &lt;- function(phi, theta, max_lag = 12) {\n  lags &lt;- 0:max_lag\n  acf_values &lt;- compute_acf_arma11(phi, theta, max_lag)\n\n  plot(lags, acf_values, type = \"h\", lwd = 2, ylim = c(-1, 1),\n       xlab = \"Lag\", ylab = expression(rho[k]), main = \"\", axes = FALSE)\n  points(lags, acf_values, pch = 19)\n\n  axis(1, at = lags)\n  axis(2, las = 1)\n  abline(h = 0)\n\n  label_text &lt;- bquote(phi == .(phi) ~ \",\" ~ theta == .(theta))\n\n  text(max(lags) * 0.7, \n      max(acf_values, na.rm = TRUE) * 0.8, \n      labels = label_text, \n      cex = 1.2)\n\n  # Label zero ACF values\n  zero_lags &lt;- which(acf_values == 0)\n  if (length(zero_lags) &gt; 0)\n  {\n  text(lags[zero_lags], \n       acf_values[zero_lags], \n       labels = \"0\", \n       pos = 3, \n       cex = 0.8)\n  }\n}\n\n# Plot for different phi and theta values\nplot_acf_arma11(0.6, -0.3)\nplot_acf_arma11(-0.6, 0.3)\nplot_acf_arma11(0.6, 0.3)\nplot_acf_arma11(-0.6, -0.3)\n# Main title\nmtext(\"Theoretical ACFs for various ARMA(1,1) Models\", \n      outer = TRUE, \n      cex = 1.8, \n      line = 2, \n      font = 2)",
    "crumbs": [
      "R notebook: Plots"
    ]
  },
  {
    "objectID": "TSA-Lecture08-regression-methods.html",
    "href": "TSA-Lecture08-regression-methods.html",
    "title": "Regression Methods in Time Series Analysis",
    "section": "",
    "text": "Random Walk\nUS Population: Quadratic Trend\nSeasonal Means\nResidual Analysis\nAssessing Normality\nThe Sample Autocorrelation Function\nQuantile-Quantile Plot of Los Angeles Annual Rainfall Series",
    "crumbs": [
      "R notebook: Regression Methods"
    ]
  },
  {
    "objectID": "TSA-Lecture08-regression-methods.html#linear-and-quadratic-trends-in-time-series",
    "href": "TSA-Lecture08-regression-methods.html#linear-and-quadratic-trends-in-time-series",
    "title": "Regression Methods in Time Series Analysis",
    "section": "2.1 Linear and Quadratic Trends in Time Series",
    "text": "2.1 Linear and Quadratic Trends in Time Series\n\n2.1.1 Random Walk: apparent linear trend\nWe are going to fit a linear trend to a random walk\n\n\nCode\n# Set seed for reproducibility\nset.seed(439)\n# Generate a random walk\nrw &lt;- cumsum(rnorm(60))\n# Fit a linear trend to the random walk\nmodel1 &lt;- lm(rw ~ time(rw))\n# Print the summary of the model\nsummary(model1)\n\n\n\nCall:\nlm(formula = rw ~ time(rw))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.101 -2.414  1.064  1.970  4.063 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.50344    0.70526  -0.714    0.478    \ntime(rw)     0.14957    0.02011   7.438 5.37e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.697 on 58 degrees of freedom\nMultiple R-squared:  0.4882,    Adjusted R-squared:  0.4794 \nF-statistic: 55.33 on 1 and 58 DF,  p-value: 5.371e-10\n\n\nCode\n# Plot the random walk\nplot(rw,\n     type='o',\n     ylab='y',\n     xlab='Time',\n     main='Random Walk',\n     col='blue',\n     pch=20,\n     cex=0.5)\nabline(model1) # add the fitted least squares line from model1\nlegend('topleft',\n       legend=c('Fitted Trend Line','Random Walk'),\n       col=c('black','blue'),\n       lty=1)\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 US population: quadratic trend\n\n\nCode\n# Load the US population data\ndata(\"uspop\")\n# Fit a quadratic trend to the US population data\nmodel.pop &lt;- lm(uspop~time(uspop)+I(time(uspop)^2)) \n# Extract the coefficients\ncoef &lt;- model.pop$coefficients\n# Print the summary of the model\nsummary(model.pop)\n\n\n\nCall:\nlm(formula = uspop ~ time(uspop) + I(time(uspop)^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5997 -0.7105  0.2669  1.4065  3.9879 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       2.045e+04  8.431e+02   24.25 4.81e-14 ***\ntime(uspop)      -2.278e+01  8.974e-01  -25.38 2.36e-14 ***\nI(time(uspop)^2)  6.345e-03  2.387e-04   26.58 1.14e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.78 on 16 degrees of freedom\nMultiple R-squared:  0.9983,    Adjusted R-squared:  0.9981 \nF-statistic:  4645 on 2 and 16 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Plot the US population data\nplot(y=uspop,\n     x=as.vector(time(uspop)),\n     xlab='Time',\n     ylab='US population 1790-1970',\n     type='o',\n     col = 'blue',\n     main='US Population 1790-1970')\n# Add the fitted quadratic trend line\nlines(x=as.vector(time(uspop)),\n      y=coef[1]+coef[2]*as.vector(time(uspop))+coef[3]*as.vector(time(uspop))^2,\n      col='red')\n\n# Add the legend\nlegend('topleft',\n       legend=c('Fitted Quadratic Trend Line','US Population'),\n       col=c('red','blue'),\n       lty=1)\n\n\n\n\n\n\n\n\n\n\n\n2.1.3 Seasonal (or Cyclical) Means\n\\[\nY_t = \\mu_{t} + X_t\n\\] Here, \\(\\mu_{t}\\) is the seasonal mean and \\(X_t\\) is the seasonal deviation.\n\\[\n\\mu_1 = \\text{mean for January}, \\mu_2 = \\text{mean for February}, \\ldots, \\mu_{12} = \\text{mean for December}\n\\]\nDataset: Average monthly temperatures, Dubuque, Iowa.\n\n\nCode\ndata(\"tempdub\")\nplot(tempdub,\n     ylab='Temperature',\n     type='o',\n     xlab='Time',\n     main='Average Monthly Temperatures, Dubuque, Iowa')\n\n\n\n\n\n\n\n\n\n\n\n2.1.4 Fitting a seasonal means model (without intercept)\n\n\nCode\nmonth. &lt;- season(tempdub) # period added to improve table display\nmodel2 &lt;- lm(tempdub ~ month. + 0) # 0 removes the intercept term\n#model2 &lt;- lm(tempdub ~ month. - 1) # -1 also removes the intercept term\nsummary(model2)\n\n\n\nCall:\nlm(formula = tempdub ~ month. + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.2750 -2.2479  0.1125  1.8896  9.8250 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \nmonth.January     16.608      0.987   16.83   &lt;2e-16 ***\nmonth.February    20.650      0.987   20.92   &lt;2e-16 ***\nmonth.March       32.475      0.987   32.90   &lt;2e-16 ***\nmonth.April       46.525      0.987   47.14   &lt;2e-16 ***\nmonth.May         58.092      0.987   58.86   &lt;2e-16 ***\nmonth.June        67.500      0.987   68.39   &lt;2e-16 ***\nmonth.July        71.717      0.987   72.66   &lt;2e-16 ***\nmonth.August      69.333      0.987   70.25   &lt;2e-16 ***\nmonth.September   61.025      0.987   61.83   &lt;2e-16 ***\nmonth.October     50.975      0.987   51.65   &lt;2e-16 ***\nmonth.November    36.650      0.987   37.13   &lt;2e-16 ***\nmonth.December    23.642      0.987   23.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.419 on 132 degrees of freedom\nMultiple R-squared:  0.9957,    Adjusted R-squared:  0.9953 \nF-statistic:  2569 on 12 and 132 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nplot(tempdub,\n     ylab='Temperature',\n     type='o',\n     xlab='Time',\n     main='Average Monthly Temperatures, Dubuque, Iowa')\npoints(time(tempdub),fitted(model2), col = \"red\", type='o')\n\n\n\n\n\n\n\n\n\nAbove, we have \\[\n\\mu_{t} = \\beta_{t}\n\\]\n\n\n2.1.5 With the intercept term\n\n\nCode\nmodel3 &lt;- lm(tempdub ~ month.) # January is dropped automatically\nsummary(model3)\n\n\n\nCall:\nlm(formula = tempdub ~ month.)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.2750 -2.2479  0.1125  1.8896  9.8250 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       16.608      0.987  16.828  &lt; 2e-16 ***\nmonth.February     4.042      1.396   2.896  0.00443 ** \nmonth.March       15.867      1.396  11.368  &lt; 2e-16 ***\nmonth.April       29.917      1.396  21.434  &lt; 2e-16 ***\nmonth.May         41.483      1.396  29.721  &lt; 2e-16 ***\nmonth.June        50.892      1.396  36.461  &lt; 2e-16 ***\nmonth.July        55.108      1.396  39.482  &lt; 2e-16 ***\nmonth.August      52.725      1.396  37.775  &lt; 2e-16 ***\nmonth.September   44.417      1.396  31.822  &lt; 2e-16 ***\nmonth.October     34.367      1.396  24.622  &lt; 2e-16 ***\nmonth.November    20.042      1.396  14.359  &lt; 2e-16 ***\nmonth.December     7.033      1.396   5.039 1.51e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.419 on 132 degrees of freedom\nMultiple R-squared:  0.9712,    Adjusted R-squared:  0.9688 \nF-statistic: 405.1 on 11 and 132 DF,  p-value: &lt; 2.2e-16\n\n\nWhen we fit with the intercept term, we have \\[\n\\mu_1 = \\beta_0,\\qquad \\mu_{t} = \\beta_0 + \\beta_{t}, \\quad t=2,3,\\ldots,12\n\\]\n\n\n2.1.6 Fitting a seasonal means model with a cosine trend\n\n\nCode\n# Fitting a cosine trend model\nhar. &lt;- harmonic(tempdub,1)\nmodel4 &lt;- lm(tempdub ~ har.)\nsummary(model4)\n\n\n\nCall:\nlm(formula = tempdub ~ har.)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.1580  -2.2756  -0.1457   2.3754  11.2671 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      46.2660     0.3088 149.816  &lt; 2e-16 ***\nhar.cos(2*pi*t) -26.7079     0.4367 -61.154  &lt; 2e-16 ***\nhar.sin(2*pi*t)  -2.1697     0.4367  -4.968 1.93e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.706 on 141 degrees of freedom\nMultiple R-squared:  0.9639,    Adjusted R-squared:  0.9634 \nF-statistic:  1882 on 2 and 141 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Plot the data and the fitted cosine trend\nplot(tempdub,\n     ylab='Temperature',\n     type='o',\n     xlab='Time',\n     main='Average Monthly Temperatures, Dubuque, Iowa')\npoints(time(tempdub),fitted(model4),col='red', type='o')",
    "crumbs": [
      "R notebook: Regression Methods"
    ]
  },
  {
    "objectID": "TSA-Lecture08-regression-methods.html#residual-analysis",
    "href": "TSA-Lecture08-regression-methods.html#residual-analysis",
    "title": "Regression Methods in Time Series Analysis",
    "section": "2.2 Residual Analysis",
    "text": "2.2 Residual Analysis\nAfter estimating the trend by \\(\\hat\\mu_t\\), we can predict the unobserved values of the stochastic component \\(X_t\\) by \\(\\hat{X}_t = Y_t - \\hat{\\mu}_t\\).\n\n2.2.1 Seasonal model without the intercept term\n\n\nCode\nplot(y=rstudent(model2),\n     x=as.vector(time(tempdub)),\n     xlab='Time',\n     ylab='Standardized Residuals',\n     type='o',\n     main = 'Residuals from Seasonal Means Model w/o Intercept')\n\n\n\n\n\n\n\n\n\nCode\nplot(y=rstudent(model3),\n     x=as.vector(time(tempdub)),\n     xlab='Time',\n     ylab='Standardized Residuals',\n     type='l',\n     main = 'Residuals from Seasonal Means Model')\n\npoints(y=rstudent(model3),\n       x=as.vector(time(tempdub)),\n       pch=as.vector(season(tempdub)),\n       col = 1:4)\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 Residuals versus Fitted Values from Seasonal Means Model\n\n\nCode\nplot(y=rstudent(model3),\n     x=as.vector(fitted(model3)),\n     xlab='Fitted Trend Values',\n     ylab='Standardized Residuals',type='n',\n     main='Residuals vs Fitted Values from Seasonal Means Model')\n     points(y=rstudent(model3),\n     x=as.vector(fitted(model3)),\n     pch=as.vector(season(tempdub)), \n     col = 1:4)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(y=rstudent(model4),\n     x=as.vector(fitted(model4)),\n     xlab='Fitted Trend Values',\n     ylab='Standardized Residuals',type='n',\n     main='Residuals vs Fitted Values from Cosine Trends')\n     points(y=rstudent(model4),\n     x=as.vector(fitted(model4)),\n     pch=as.vector(season(tempdub)), \n     col = 1:4)\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 Assessing normality\nHistogram is a very rough tool to assess normality\n\n\nCode\nhist(rstudent(model3),\n     xlab='Standardized Residuals',\n     main='Histogram of Residuals from Seasonal Means Model')\n\n\n\n\n\n\n\n\n\nQQ-plot is a much better tool:\n\n\nCode\n# Plotting the QQ-plot\nqqnorm(rstudent(model3),\n       main='QQ-plot of Residuals from Seasonal Means Model')\nqqline(rstudent(model3),col='red',lty=2)\n\n\n\n\n\n\n\n\n\nQQ-plot is an excellent visual diagnostic. We can use a Shapiro-Wilk test to assess normality:\n\n\nCode\n# Shapiro-Wilk test for normality. H0: data is normally distributed\nshapiro.test(rstudent(model3))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rstudent(model3)\nW = 0.9929, p-value = 0.6954\n\n\nTest for independence: runs test\n\n\nCode\n# Runs test for independence. H0: data is independent\nruns(rstudent(model3))\n\n\n$pvalue\n[1] 0.216\n\n$observed.runs\n[1] 65\n\n$expected.runs\n[1] 72.875\n\n$n1\n[1] 69\n\n$n2\n[1] 75\n\n$k\n[1] 0",
    "crumbs": [
      "R notebook: Regression Methods"
    ]
  },
  {
    "objectID": "TSA-Lecture08-regression-methods.html#the-sample-autocorrelation-function",
    "href": "TSA-Lecture08-regression-methods.html#the-sample-autocorrelation-function",
    "title": "Regression Methods in Time Series Analysis",
    "section": "2.3 The Sample Autocorrelation Function",
    "text": "2.3 The Sample Autocorrelation Function\nAnother tool for assessing dependency is a sample ACF.\nSample autocorrelation function, for \\(k=1, 2, 3,\\ldots\\):\n\\[\nr_k=\\dfrac{\\sum_{t=k+1}^n(Y_t-\\bar{Y})(Y_{t-k}-\\bar{Y})}{\\sum_{t=1}^n(Y_t-\\bar{Y})^2}\n\\]\n\n2.3.1 Plot of Sample ACF of residuals for seasonal means model versus \\(k\\) (correlogram)\n\n\nCode\nacf(rstudent(model3),\n    main='ACF of Residuals from Seasonal Means Model')\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Sample ACF for residuals of a linear fit to a random walk\n\n\nCode\nplot(y=rstudent(model1),\n     x=as.vector(time(rw)),\n     ylab='Standardized Residuals',\n     xlab='Time',\n     type='o',\n     main='Residuals from Straight Line Fit to Random Walk')\n\n\n\n\n\n\n\n\n\nResiduals versus Fitted Values from Straight Line Fit: larger values of residuals correspond to larger fitted values.\n\n\nCode\nplot(y=rstudent(model1),\n     x=fitted(model1),\n     ylab='Standardized Residuals',\n     xlab='Fitted Trend Line Values',\n     type='p')\n\n\n\n\n\n\n\n\n\nSample Autocorrelation of Residuals from the Straight Line fit to the random walk\n\n\nCode\nacf(rstudent(model1),\n    main='ACF of Residuals from Straight Line Fit to Random Walk')\n\n\n\n\n\n\n\n\n\nWhat if we try completely different approach to the random walk data. Let’s difference the data instead and plot the sample ACF of the differenced data?\n\\[\n\\nabla Y_t = Y_t-Y_{t-1}\n\\]\n\n\nCode\nacf(diff(rw),\n    main='ACF of Differenced Random Walk')\n\n\n\n\n\n\n\n\n\n\n\n2.3.3 Quantile-Quantile Plot of Los Angeles Annual Rainfall Series\nTurns out it’s an iid noise:\n\n\nCode\ndata(\"larain\")\nacf(larain,\n    main='ACF of Los Angeles Annual Rainfall Series')\n\n\n\n\n\n\n\n\n\nIs it normal? Let’s check the QQ-plot:\n\n\nCode\nqqnorm(larain,\n       main='QQ-plot of Los Angeles Annual Rainfall Series') \nqqline(larain)\n\n\n\n\n\n\n\n\n\nQ: is it left-skewed or right-skewed?\n\n\nCode\n# Shapiro Wilk test\nshapiro.test(larain)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  larain\nW = 0.94617, p-value = 0.0001614\n\n\nIf we log-transform the data, we can see that it becomes more symmetric:\n\n\nCode\nqqnorm(log(larain),\n       main='QQ-plot of log-transformed Los Angeles Annual Rainfall Series') \nqqline(log(larain))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro Wilk test\nshapiro.test(log(larain))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  log(larain)\nW = 0.98742, p-value = 0.3643",
    "crumbs": [
      "R notebook: Regression Methods"
    ]
  },
  {
    "objectID": "TSA-Lecture09.html",
    "href": "TSA-Lecture09.html",
    "title": "25 Spring 439/639 TSA: Lecture 9",
    "section": "",
    "text": "Recall that we use regression method to deal with the additive model \\[\n\\underbrace{Y_t}_{observed} = \\underbrace{\\mu_t}_{deterministic} + \\underbrace{X_t}_{stochastic} .\n\\] \\(\\mu_t\\) is estimated (via regression) as \\(\\widehat{\\mu}_t\\). Then we fit some time series model for \\(\\widehat{X}_t = Y_t - \\widehat{\\mu}_t\\).\nLogically, there is an issue here. In regression, errors \\(\\varepsilon_t\\) are assumed iid and normal in the models \\(Y_t = \\mu_t + \\varepsilon_t\\). Typically, we have \\[\n\\varepsilon_t \\overset{iid}\\sim N(0,\\sigma) \\implies \\vec\\varepsilon \\sim MN(0, \\sigma^2 I) \\implies \\operatorname{Var}(\\widehat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}.\n\\] But for time series model, the \\((X_t)\\) are not modeled as iid, (so the covariance matrix \\(V\\) of the error vector in regression step is no longer diagonal) \\[\n\\vec\\varepsilon \\sim MN(0, V)\n\\implies \\operatorname{Var}(\\widehat{\\beta}) = (X^\\top X)^{-1} (X^\\top V X) (X^\\top X)^{-1} .\n\\] So the (co)variance and standard errors of estimates \\(\\widehat{\\beta}\\) are not reliable (in the sense that they are not correctly reported by regression software since regression model has different assumptions).\nDespite this issue, we still have the following result.\nClaim: If the trend is polynomial, trigonometric, trigonometric polynomial, seasonal means, or a linear combination of the above, then for a stationary stochastic component \\((X_t)\\): The least square estimate of the trend has the same variance as the Best linear Unbiased Estimator for large sample sizes.",
    "crumbs": [
      "Lecture 9"
    ]
  },
  {
    "objectID": "TSA-Lecture09.html#differencing-operators",
    "href": "TSA-Lecture09.html#differencing-operators",
    "title": "25 Spring 439/639 TSA: Lecture 9",
    "section": "3.1 Differencing operators",
    "text": "3.1 Differencing operators\nDefine the differencing operator \\(\\nabla\\) (pronounced as nabla) as \\[\n\\nabla Y_t = (1 - B) Y_t = Y_t - Y_{t-1}.\n\\] Example: we can take difference twice: \\[\n\\begin{split}\n\\nabla^2 Y_t &= \\nabla \\left( Y_t - Y_{t-1} \\right) = \\left( Y_t - Y_{t-1} \\right) - \\left( Y_{t-1} - Y_{t-2} \\right) \\\\\n             &= Y_t - 2Y_{t-1} + Y_{t-2} \\\\\n             &= \\left(1 - 2B + B^2\\right) Y_t = (1 - B)^2 Y_t .\n\\end{split}\n\\] Another different but related operation is lag \\(d\\) differencing (which is useful in seasonal models), defined as \\[\n\\nabla_d Y_t = Y_t - Y_{t-d} = \\left( 1 - B^d \\right) Y_t.\n\\] This is different from taking difference \\(d\\) times: \\[\n\\nabla^d Y_t = (1 - B)^d Y_t.\n\\] As a combined example, in a seasonal model, we may take lag \\(s\\) differencing \\(d\\) times: \\[\n\\nabla_s^d Y_t = (1 - B^s)^d Y_t.\n\\]",
    "crumbs": [
      "Lecture 9"
    ]
  },
  {
    "objectID": "TSA-Lecture09.html#more-examples-of-using-differencing-operators",
    "href": "TSA-Lecture09.html#more-examples-of-using-differencing-operators",
    "title": "25 Spring 439/639 TSA: Lecture 9",
    "section": "3.2 More examples of using differencing operators",
    "text": "3.2 More examples of using differencing operators\nExample 1: consider a trend + stationary model, where the trend is linear in \\(t\\). \\[\nY_t = \\beta_0 + \\beta_1 t + X_t.\n\\] Suppose \\((X_t)\\) is stationary, then \\((Y_t)\\) is not stationary.\nExercise: Why is \\((Y_t)\\) not stationary?\nIf we take the difference: \\[\n\\nabla Y_t = \\left( \\beta_0 + \\beta_1 t + X_t \\right) - \\left( \\beta_0 + \\beta_1 (t-1) + X_{t-1} \\right) = \\beta_1 + \\left( X_t - X_{t-1} \\right).\n\\] We can show \\(\\nabla X_t = X_t - X_{t-1}\\) is stationary from the stationarity of \\((X_t)\\). So in this example, \\(\\nabla Y_t\\) is staionary although \\((Y_t)\\) is not stationary.\nExample 2: consider a random walk model \\[\nY_1=e_1, \\quad Y_t = Y_{t-1} + e_t, \\quad e_t \\sim \\mathrm{iid}(0,\\sigma_e^2).\n\\] As we have seen many times in the course, \\((Y_t)\\) is not stationary. If we take the difference, \\[\n\\nabla Y_t= Y_t -Y_{t-1} = e_t\n\\] \\(\\nabla Y_t\\) is stationary in this example.",
    "crumbs": [
      "Lecture 9"
    ]
  },
  {
    "objectID": "TSA-Lecture09.html#arimapdq",
    "href": "TSA-Lecture09.html#arimapdq",
    "title": "25 Spring 439/639 TSA: Lecture 9",
    "section": "3.3 ARIMA(\\(p,d,q\\))",
    "text": "3.3 ARIMA(\\(p,d,q\\))\nIn Example 2 (random walk) above, we had \\(\\nabla Y_t= e_t\\). Note that \\((e_t)\\) can be seen as an AR(\\(0\\)) or MA(\\(0\\)) or ARMA(\\(0,0\\)) model.\nAlso observe that (suppose \\(W_t = \\nabla Y_t\\)) \\[\n\\begin{split}\nY_t &= Y_{t-1} + W_t = Y_{t-2} + W_{t-1} + W_t \\\\\n&= \\cdots \\\\\n&=\n\\begin{cases}\n\\sum_{i=1}^{t} W_i, & \\text{as a special case, in random walk, } Y_0 = 0\\\\\n\\sum_{i=-m}^{t} W_i, & \\text{more generally, start at } (-m)\n\\end{cases}\n\\end{split}\n\\] which looks like a integrated sum of \\((W_i)\\). So the idea here is: \\(Y_t\\) is ``integrated” \\(W_t\\). In the random walk example above, \\(W_t = e_t\\) is ARMA(\\(0,0\\)) (or AR(\\(0\\)) or MA(\\(0\\))), so we say the random walk \\((Y_t)\\) is ARIMA(\\(0,1,0\\)) (or ARI(\\(0,1\\)) or IMA(\\(1,0\\))). The letter I stands for integrated. In general, we have the following definition.\nDefinition: If \\(\\nabla^d Y_t\\) is an ARMA(\\(p,q\\)), then \\(Y_t\\) is ARIMA(\\(p,d,q\\)).\nLet’s look at another example. Consider the random walk + noise model \\[\nY_t = X_t + \\eta_t = \\sum_{j=1}^t e_j + \\eta_t, \\quad \\eta_t \\sim \\mathrm{iid}(0,\\sigma_\\eta^2),\\quad e_t \\sim \\mathrm{iid}(0,\\sigma_e^2)\n\\] where \\((X_t)\\) is a random walk defined as usual, \\((\\eta_t)\\) is another sequence of noise and \\((\\eta_t)\\) is independent of \\((e_t)\\).\nExercise: show \\((Y_t)\\) is not stationary. (Hint: show \\(Var(Y_t) = t\\sigma_e^2 + \\sigma_\\eta^2\\).)\nBy taking the difference, \\[\n\\begin{split}\nW_t = \\nabla Y_t\n    &= (X_t + \\eta_t) - (X_{t-1} + \\eta_{t-1}) \\\\\n    &= (X_t - X_{t-1}) + \\eta_t - \\eta_{t-1} \\\\\n    &= e_t + \\eta_t - \\eta_{t-1}.\n\\end{split}\n\\] We can see that the ACVF of \\((W_t)\\) satisfies \\(\\gamma_k = 0\\) for \\(k\\ge 2\\). This structure look like the ACVF of an MA(\\(1\\)) process.\nIn fact \\((W_t)\\) is indeed an MA(\\(1\\)) process: The ACVF of \\((W_t)\\) are all zero for lag \\(k\\ge 2\\), so \\((W_t)\\) is \\(1\\)-correlated. Recall an earlier theorem (see lecture 3 when we first defined MA(\\(q\\))), there exist an uncorrelated stationary process \\((\\widetilde{\\epsilon}_t)\\) and a constant \\(\\widetilde{\\theta}\\) such that the time series \\(Z_t = \\widetilde{\\epsilon}_t - \\widetilde{\\theta} \\widetilde{\\epsilon}_{t-1}\\) will have the same ACVF as \\(W_t\\).\nSo \\(W_t \\sim \\mathrm{MA}(1)\\), which implies \\(Y_t \\sim \\mathrm{ARIMA}(0,1,1)\\) (or IMA(\\(1,1\\))).",
    "crumbs": [
      "Lecture 9"
    ]
  },
  {
    "objectID": "TSA-Lecture11.html",
    "href": "TSA-Lecture11.html",
    "title": "25 Spring 439/639 TSA: Lecture 11",
    "section": "",
    "text": "The goal of transformation is to make the time series “more” stationary, and/or make the time series a normal process (if possible). Last time we already introduced Variance Stabilizing Transformation (under certain settings).\nAnother option: Box–Cox transformations, transforming \\(y\\) to \\(g(y)\\) as follows \\[\ng(y) =\n\\begin{cases}\n  \\dfrac{y^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n  \\log y, & \\lambda = 0\n\\end{cases}\n\\]\nExercise: Show \\(\\lim_{\\lambda \\to 0} \\frac{y^\\lambda - 1}{\\lambda} = \\log y\\).\nThe \\(\\lambda\\) in Box–Cox transformation above is chosen via an MLE approach. See R notebook later.\nAnother common way is to take the difference of logarithm, i.e., log-differences. This can be practically useful in specific applications.\nIn finance, suppose the time series \\((Y_t)\\) can be written as follows \\[\nY_t = Y_{t-1} + X_t \\cdot Y_{t-1} = Y_{t-1}\\left(1 + X_t \\right).\n\\] So \\((X_t)\\) is the percentage change of \\((Y_t)\\), and we have \\[\n\\log Y_t = \\log Y_{t-1} + \\log(1 + X_t).\n\\] Then the log-difference, (or the log-returns, the returns) of \\((Y_t)\\) is \\[\n\\nabla \\log Y_t = \\log Y_t - \\log Y_{t-1} = \\log(1 + X_t) \\approx X_t\n\\] where the last step is because the percentage change \\(X_t\\) is usually small in finance. (Note: \\(X_t\\) can be positive or negative, but close to \\(0\\).) And in practice, the time series \\((X_t)\\) is usually stationary. So taking the transformation \\(\\nabla \\log Y_t\\) gives a more stationary time series.\nSummary: to make a time series more stationary, we can consider differencing, variance stabilizing transformation, taking logarithm, Box-Cox transformations, and combination of these.",
    "crumbs": [
      "Lecture 11"
    ]
  },
  {
    "objectID": "TSA-Lecture11.html#example-white-noise",
    "href": "TSA-Lecture11.html#example-white-noise",
    "title": "25 Spring 439/639 TSA: Lecture 11",
    "section": "3.1 Example: white noise",
    "text": "3.1 Example: white noise\nSuppose \\(Y_t \\sim \\operatorname{WN}(0, \\sigma^2)\\), then \\[\n\\rho_0 = 1, \\text{ and } \\rho_i = 0 \\text{ for } i \\geq 1.\n\\] Let’s look at \\[\nc_{ii} = \\sum_{k=-\\infty}^{+\\infty} \\left( \\rho_{k+i}^2 + \\rho_{k-i} \\rho_{k+i} - 4 \\rho_i \\rho_k \\rho_{k+i} + 2 \\rho_i^2 \\rho_k^2 \\right).\n\\] If \\(i\\ge 1\\), then \\(\\rho_i=0\\), \\(c_{ii} = \\sum_{k=-\\infty}^{+\\infty} \\left( \\rho_{k+i}^2 + \\rho_{k-i} \\rho_{k+i} \\right)\\). Also note that \\(\\rho_{k-i} \\rho_{k+i} =0\\) for any \\(k\\), and \\(\\sum_{k=-\\infty}^{+\\infty} \\rho_{k+i}^2 = \\rho_0^2 =1\\), so \\(c_{ii}= 1\\).\nBy Bartlett’s Theorem, for any fixed \\(i\\ge 1\\), \\(r_i \\sim N(0, \\frac{1}{n})\\) (when sample size \\(n\\) is large). Using this result, we can construct 95% CI for \\(\\rho_i\\) (for this example): \\[\n\\left[ r_i - \\frac{2}{\\sqrt{n}}, r_i + \\frac{2}{\\sqrt{n}} \\right].\n\\] We can also compute \\(c_{00}\\). If \\(i=0\\), then \\(c_{ii} = \\sum_{k=-\\infty}^{+\\infty} \\left( \\rho_{k+i}^2 + \\rho_{k}^2 - 4  \\rho_k^2 + 2 \\rho_k^2 \\right) = 0\\). By a similar statement from Bartlett’s Theorem, \\(Var(r_0) = 0\\). This is not a surprise since the sample ACF at lag \\(0\\), i.e. \\(r_0\\), is always \\(1\\).",
    "crumbs": [
      "Lecture 11"
    ]
  },
  {
    "objectID": "TSA-Lecture11.html#example-ar1",
    "href": "TSA-Lecture11.html#example-ar1",
    "title": "25 Spring 439/639 TSA: Lecture 11",
    "section": "3.2 Example: AR(\\(1\\))",
    "text": "3.2 Example: AR(\\(1\\))\nSuppose \\((Y_t)\\) follows AR(\\(1\\)), then \\[\n\\rho_k = \\phi^k \\text{ for } k \\geq 0, \\text{ and } \\rho_k = \\phi^{|k|} \\text{ for } k &lt; 0.\n\\] Using the \\(c_{ii}\\) formula from Bartlett’s Theorem, we can derive that: (for large \\(n\\)) \\[\n\\operatorname{Var}(r_i) = \\frac{c_{ii}}{n} = \\frac{1}{n} \\left[\n    \\frac{(1 + \\phi^2)(1 - \\phi^{2i})}{1 - \\phi^2} - 2i\\, \\phi^{2i} \\right].\n\\] In particular, when \\(i=1\\), we have \\(\\operatorname{Var}(r_1) = \\frac{1-\\phi^2}{n}\\). So if \\(\\phi\\) is close to \\(\\pm 1\\), then \\(r_1\\) is a very precise estimate of \\(\\rho_1\\).\nIf the lag \\(i\\) is very large, then \\(\\operatorname{Var}(r_i) \\approx \\frac{1}{n} \\cdot \\frac{1+ \\phi^2}{1- \\phi^2}\\). So if \\(\\phi\\) is close to \\(\\pm 1\\), then for large \\(i\\), \\(r_i\\) is not a precise estimate of \\(\\rho_i\\) (in the sense that the variance is very large).",
    "crumbs": [
      "Lecture 11"
    ]
  },
  {
    "objectID": "TSA-Lecture11.html#example-ma1",
    "href": "TSA-Lecture11.html#example-ma1",
    "title": "25 Spring 439/639 TSA: Lecture 11",
    "section": "3.3 Example: MA(\\(1\\))",
    "text": "3.3 Example: MA(\\(1\\))\nSuppose \\((Y_t)\\) follows MA(\\(1\\)), then \\[\n\\rho_0 = 1, \\quad\n\\rho_1 = \\rho_{-1} \\neq 0, \\quad \\text{and }\n\\rho_k = 0 \\text{ for } |k| \\geq 2.\n\\] For \\(c_{11}\\), we have \\[\n\\begin{split}\nc_{11} &= \\sum_{k=-\\infty}^{+\\infty} \\left( \\rho_{k+1}^2 + \\rho_{k-1} \\rho_{k+1} - 4 \\rho_1 \\rho_k \\rho_{k+1} + 2 \\rho_1^2 \\rho_k^2 \\right) \\\\\n&= (\\rho_0^2 + 2\\rho_1^2) + (\\rho_{-1} \\rho_1) - 4\\rho_1(\\rho_{-1}\\rho_0+ \\rho_0 \\rho_1) + 2\\rho_1^2 (\\rho_0^2 + 2\\rho_1^2)\\\\\n&= 1 + 2\\rho_1^2 + \\rho_1^2 - 4\\rho_1^2 - 4\\rho_1^2 + 2\\rho_1^2 + 4\\rho_1^4 \\\\\n&=  1 - 3\\rho_1^2 + 4\\rho_1^4.\n\\end{split}\n\\] By Bartlett’s Theorem, we have the following (for large \\(n\\)) \\[\nr_1 \\sim \\mathcal{N}\\left( \\rho_1,\\ \\frac{1 - 3\\rho_1^2 + 4\\rho_1^4}{n} \\right).\n\\]\nFor \\(i\\ge 2\\), we can also derive that \\[\nc_{ii}= 1+ 2\\rho_1^2, \\quad  r_i \\sim \\mathcal{N}\\left(0,\\ \\frac{1 + 2\\rho_1^2}{n}\\right).\n\\] Exercise: verify that \\(c_{ii}= 1+ 2\\rho_1^2\\) for any \\(i\\ge 2\\) (under the MA(\\(1\\)) setting).",
    "crumbs": [
      "Lecture 11"
    ]
  },
  {
    "objectID": "TSA-Lecture11.html#example-maq",
    "href": "TSA-Lecture11.html#example-maq",
    "title": "25 Spring 439/639 TSA: Lecture 11",
    "section": "3.4 Example: MA(\\(q\\))",
    "text": "3.4 Example: MA(\\(q\\))\nFor MA(\\(q\\)), we can show that: (for large \\(n\\)) \\[\nr_i \\sim \\mathcal{N}\\left(0,\\ \\frac{1 + 2\\sum_{j=1}^q \\rho_j^2}{n}\\right), \\text{ for any lag } i\\ge q+1,\n\\] which is similar to the \\(i\\ge 2\\) case in MA(\\(1\\)).",
    "crumbs": [
      "Lecture 11"
    ]
  },
  {
    "objectID": "TSA-Lecture13.html",
    "href": "TSA-Lecture13.html",
    "title": "25 Spring 439/639 TSA: Lecture 13",
    "section": "",
    "text": "1 Extended Autocorrelation Function (EACF)\nExtended Autocorrelation Function (EACF) can help us find the order \\((p,q)\\) of ARMA(\\(p,q\\)) model. (Remark: in comparison, we can use (sample) ACF for MA(\\(q\\)) and PACF for AR(\\(p\\)).)\nThe sample EACF can be computed by the TSA package in R. (No Python analogue.)\nIdea. We first note a simple fact: suppose a process \\((Y_t)\\) is ARMA, then applying the (true) AR part filter to \\((Y_t)\\) gives an MA process. This fact suggests a very natural idea: to test whether a process is ARMA given its observations, we can (i) first try to fit an AR regression using the observed data, (ii) then test if the residuals follow an MA process. The only issue of this idea is we do not know the order \\(p\\) and \\(q\\) for AR and MA part, so we need to try different pairs of \\((p,q)\\) and repeat this two-step idea.\nLet’s take a look at the algorithm to compute sample EACF in R. By its default, it tries \\(p\\) up to \\(7\\) and \\(q\\) up to \\(13\\).\n(The outer loop) For \\(p=0,1,...,7\\): fit an AR(\\(p\\)) for \\((Y_t)\\) and find the residuals \\(W_t = Y_t - \\widehat{Y}_t\\).\n(The inner loop) For \\(q=0,1,...,13\\): fit an MA(\\(q\\)) for \\((W_t)\\) and find the residuals \\(e_t = W_t - \\widehat{W}_t\\). Now do hypothesis testing. If \\((e_t)\\) is white noise, output \\(0\\); If \\((e_t)\\) is not white noise, output \\(\\times\\).\nThe output of the sample EACF is listed in a table of \\(0\\)’s and \\(\\times\\)’s. And all the zeroes in the table are in a “triangle” area. This is because\n\nFor an ARMA(\\(p,q\\)) process, we can always overfit it with an ARMA(\\(p,q'\\)) model for any \\(q'&gt;q\\).\nFor an ARMA(\\(p,q\\)) process, we can always overfit it with an ARMA(\\(p+1,q+1\\)) model.\n\nSo the upper-left corner of these zeroes reflects the correct \\((p,q)\\) order.\n\n\n2 Unit Root Test: Augmented Dickey–Fuller Test\nThe Unit Root Test (of AR polynomial) is also named as Augmented Dickey–Fuller Test (ADF test). It tests the stationarity of a time series given its observed samples.\nSuppose we want to do the following hypothesis testing \\[\n\\begin{split}\nH_0 : \\text{AR polynomial has a unit root} \\quad &\\text{vs.} \\quad H_a : \\text{AR polynomial does not have a unit root} \\\\\n\\text{or}\\qquad H_0 : (Y_t) \\text{ is nonstationary} \\quad &\\text{vs.} \\quad H_a : (Y_t) \\text{ is stationary}\n\\end{split}\n\\] This can be done by the command \\(adf.test()\\) in R. A small p-value of this test implies stationarity, and a large p-value implies non-stationarity.",
    "crumbs": [
      "Lecture 13"
    ]
  },
  {
    "objectID": "TSA-Lecture14.html",
    "href": "TSA-Lecture14.html",
    "title": "25 Spring 439/639 TSA: Lecture 14",
    "section": "",
    "text": "1 Parameter estimation\nUsing all the tools we have seen (sample ACF/PACF/EACF, transformations, ADF test, ARMA subsets, etc.) we arrive at a few candidate models.\nNext goal: Estimate the parameters \\(\\phi_i\\) (\\(i=1,...,p\\)), \\(\\theta_j\\) (\\(j=1,...,p\\)). And maybe the variance of the noise \\(\\sigma_e^2\\), the mean of the time series \\(\\mu\\).\n\n\n2 Method of Moments (MoM)\nRecall that the task is estimating the parameters given the observed samples. The idea of Method of Moments (MoM) is to solve the parameters from the equation(s) \\[\n\\text{theoretical moment} = \\text{sample moment}\n\\] where the theoretical \\(k\\)-th moment \\(\\mu_k = \\mathbb{E}[Y^k]\\) is a function of the parameters, and the sample \\(k\\)-th moment \\(m_k = \\frac{1}{n} \\sum_{i=1}^n Y_i^k\\) is a function of the observed data.\nExample 0. Suppose \\(Y_1, \\cdots, Y_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\). Then we know that \\(\\mu_1 = \\mathbb{E}[Y] = \\mu\\), and \\(\\mu_2 = \\mathbb{E}[Y^2] = \\mu^2 + \\sigma^2\\). The MoM method considers \\[\n\\begin{cases}\n\\mu_1 = m_1 \\\\\n\\mu_2 = m_2\n\\end{cases}\n\\implies\n\\begin{cases}\n\\mathbb{E} [Y] = \\dfrac{1}{n} \\sum_{i=1}^{n} Y_i \\\\\n\\mathbb{E} [Y^2] = \\dfrac{1}{n} \\sum_{i=1}^{n} Y_i^2\n\\end{cases}\n\\implies\n\\begin{cases}\n\\mu = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\overline{Y} \\\\\n\\mu^2 + \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} Y_i^2\n\\end{cases}\n\\] Solving the system gives \\[\n\\begin{cases}\n\\widehat{\\mu}_{\\text{MOM}} = \\overline{Y} \\\\\n\\widehat{\\sigma}^2_{\\text{MOM}} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i^2 - \\overline{Y}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\overline{Y})^2\n\\end{cases}\n\\] Exercise: verify the last step above.\nThere is a variant method of MoM, called generalized method of moments (GMoM). (For simplicity, we may also call it MoM.) The basic idea is, if we want to estimate some quantity \\(g(Y)\\), then we can directly use \\(\\frac{1}{n}\\sum_{i=1}^n g(Y_i)\\). This idea is useful in time series parameter estimation, since we can utilize the sequential structure of the observed data.\nExample 1. Suppose \\(Y_1, \\dots, Y_n\\) are from an AR(\\(1\\)) model with mean zero: \\[\nY_t - \\phi Y_{t-1} = e_t, \\quad e_t \\sim \\text{iid} (0, \\sigma_e^2).\n\\] We can apply the generalized MoM here by solving the equation(s) \\[\n\\text{theoretical ACF} = \\text{sample ACF}.\n\\] Note that the theoretical ACF is \\(\\rho_k = \\phi^k\\), and in particular \\(\\rho_1 = \\phi\\). To estimate \\(\\phi\\), we can solve the equation \\(\\rho_1 = r_1\\) where \\(r_1\\) is the sample ACF at lag \\(1\\): \\[\nr_1 = \\frac{\\sum_{t=1}^{n-1} (Y_{t+1} - \\overline{Y})(Y_t - \\overline{Y})}\n{\\sum_{t=1}^{n} (Y_t - \\overline{Y})^2}.\n\\] Solving the equation \\(\\rho_1 = r_1\\) gives the MoM (GMoM) estimate \\[\n\\widehat{\\phi}_\\text{MOM} = \\frac{\\sum_{t=1}^{n-1} (Y_{t+1} - \\overline{Y})(Y_t - \\overline{Y})}\n{\\sum_{t=1}^{n} (Y_t - \\overline{Y})^2}.\n\\]\nExample 2. Suppose \\(Y_1, \\dots, Y_n\\) are from an AR(\\(2\\)) model with mean zero: \\[\nY_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2} = e_t.\n\\] The first two YW equations are \\[\n\\begin{cases}\n\\gamma_1 - \\phi_1 \\gamma_0 - \\phi_2 \\gamma_1 = 0 \\\\\n\\gamma_2 - \\phi_1 \\gamma_1 - \\phi_2 \\gamma_0 = 0\n\\end{cases}\n\\implies\n\\begin{cases}\n\\rho_1 = \\phi_1 + \\phi_2 \\rho_1 \\\\\n\\rho_2 = \\phi_1 \\rho_1 + \\phi_2\n\\end{cases}\n\\implies\n\\begin{cases}\n\\rho_1 = \\frac{\\phi_1}{1 - \\phi_2} \\\\\n\\rho_2 = \\frac{\\phi_1^2 - \\phi_2^2 + \\phi_2}{1 - \\phi_2}\n\\end{cases}\n\\] Using MoM, we need to solve \\(\\phi_1, \\phi_2\\) from the equations \\[\n\\begin{cases}\n\\rho_1(\\phi_1, \\phi_2) = r_1 \\\\\n\\rho_2(\\phi_1, \\phi_2) = r_2\n\\end{cases} \\quad\\text{i.e.,}\\quad\n\\begin{cases}\n\\frac{\\phi_1}{1 - \\phi_2} = r_1 \\\\\n\\frac{\\phi_1^2 - \\phi_2^2 + \\phi_2}{1 - \\phi_2} = r_2\n\\end{cases}\n\\] Alternatively, we can also replace the theoretical ACF with sample ACF in the YW equations, and then solve for \\(\\phi_1, \\phi_2\\): (this is equivalent to the procedure above) \\[\n\\begin{cases}\nr_1 = \\phi_1 + \\phi_2 r_1 \\\\\nr_2 = \\phi_1 r_1 + \\phi_2\n\\end{cases}\n\\] Exercise: verify that \\[\n\\begin{cases}\n\\widehat{\\phi}_1^{\\text{MOM}} = \\frac{r_1 (1 - r_2)}{1 - r_1^2} \\\\\n\\widehat{\\phi}_2^{\\text{MOM}} = \\frac{r_2 - r_1^2}{1 - r_1^2}\n\\end{cases}\n\\]\nExample 3. Suppose \\(Y_1, \\dots, Y_n\\) are from an AR(\\(p\\)) model with mean zero: \\[\nY_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2} - \\cdots - \\phi_p Y_{t-p}= e_t.\n\\] Similar to the AR(\\(2\\)) example, we start from the first \\(p\\) YW equations \\[\n\\begin{cases}\n\\rho_1 = \\phi_1 + \\phi_2 \\rho_1 + \\cdots + \\phi_p \\rho_{p-1} \\\\\n\\rho_2 = \\phi_1 \\rho_1 + \\phi_2 + \\cdots + \\phi_p \\rho_{p-2} \\\\\n\\quad\\vdots \\\\\n\\rho_p = \\phi_1 \\rho_{p-1} + \\phi_2 \\rho_{p-2} + \\cdots + \\phi_p\n\\end{cases}\n\\] which can be written in the matrix form \\[\n\\begin{bmatrix}\n\\rho_0 & \\rho_1 & \\cdots & \\rho_{p-1} \\\\\n\\rho_1 & \\rho_0 & \\cdots & \\rho_{p-2} \\\\\n\\vdots & \\vdots &        & \\vdots \\\\\n\\rho_{p-1} & \\rho_{p-2} & \\cdots & \\rho_0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\phi_{1} \\\\\n\\phi_{2} \\\\\n\\vdots \\\\\n\\phi_{p}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\vdots \\\\\n\\rho_k\n\\end{bmatrix}.\n\\] Replace the theoretical ACF \\(\\rho_k\\) by the sample ACF \\(r_k\\), and solve the MoM estimate for \\((\\phi_1,...,\\phi_p)\\). So the MoM estimates satisfy \\[\n\\widehat{R}_p \\ \\widehat{\\vec\\phi} = \\widehat{\\vec\\rho}_p,\\quad\n\\text{where}\\quad\n\\widehat{R}_p = \\begin{bmatrix}\nr_0 & r_1 & \\cdots & r_{p-1} \\\\\nr_1 & r_0 & \\cdots & r_{p-2} \\\\\n\\vdots & \\vdots &        & \\vdots \\\\\nr_{p-1} & r_{p-2} & \\cdots & r_0\n\\end{bmatrix},\\quad\n\\widehat{\\vec\\phi} = \\begin{bmatrix}\n\\widehat{\\phi}_{1} \\\\\n\\widehat{\\phi}_{2} \\\\\n\\vdots \\\\\n\\widehat{\\phi}_{p}\n\\end{bmatrix},\\quad\n\\widehat{\\vec\\rho}_p = \\begin{bmatrix}\nr_1 \\\\\nr_2 \\\\\n\\vdots \\\\\nr_p\n\\end{bmatrix}.\n\\] Then we get the MoM estimate \\[\n\\widehat{\\vec\\phi}_\\text{MOM} = \\widehat{R}_p^{-1}\\ \\widehat{\\vec\\rho}_p.\n\\] Remark: The matrix \\(\\widehat{R}_p\\) defined above is always invertible, which is guaranteed by the particular way (and details) we used to construct the sample ACF \\(r_k\\) in lecture 9. In fact, we remarked in lecture 9 that our construction of \\(r_k\\) makes the “sample ACF matrix” invertible, and this matrix is nothing but the \\(\\widehat{R}_p\\) we have just seen.",
    "crumbs": [
      "Lecture 14"
    ]
  },
  {
    "objectID": "TSA-Lecture16-parameter-estimation.html",
    "href": "TSA-Lecture16-parameter-estimation.html",
    "title": "439/639 TSA: Parameter Estimation",
    "section": "",
    "text": "Step 1: Model Specification\nStep 2: Parameter Estimation\nStep 3: Model Checking\nStep 4: Forecasting",
    "crumbs": [
      "R notebook: Parameter Estimation"
    ]
  },
  {
    "objectID": "TSA-Lecture16-parameter-estimation.html#steps-in-time-series-analysis",
    "href": "TSA-Lecture16-parameter-estimation.html#steps-in-time-series-analysis",
    "title": "439/639 TSA: Parameter Estimation",
    "section": "",
    "text": "Step 1: Model Specification\nStep 2: Parameter Estimation\nStep 3: Model Checking\nStep 4: Forecasting",
    "crumbs": [
      "R notebook: Parameter Estimation"
    ]
  },
  {
    "objectID": "TSA-Lecture16-parameter-estimation.html#method-of-moments",
    "href": "TSA-Lecture16-parameter-estimation.html#method-of-moments",
    "title": "439/639 TSA: Parameter Estimation",
    "section": "1.1 Method of Moments",
    "text": "1.1 Method of Moments\n\n1.1.1 MA(1)\nThe method of moments estimator of the MA(1) coefficient \\(\\theta\\) is given by the solution of the following quadratic equation: \\[\nr_1\\theta^2 + \\theta  + r_1 = 0,\n\\] where \\(r_1\\) is the sample lag 1 ACF of the time series.\n\n\nCode\n# Below is a function that computes the method of moments estimator of\n# the MA(1) coefficient of an MA(1) model.\nestimate.ma1.mom = function(x){\n    r=acf(x,plot=F)$acf[1] \n    if (abs(r)&lt;0.5) \n        return((-1+sqrt(1-4*r^2))/(2*r)) \n    else \n        return(NA)\n        }\n\n# Exhibit 7.1\ndata(ma1.2.s)\nestimate.ma1.mom(ma1.2.s)\n\n\n[1] -0.5554273\n\n\nCode\ndata(ma1.1.s)\nestimate.ma1.mom(ma1.1.s)\n\n\n[1] 0.7196756\n\n\nCode\nset.seed(1234)\nma1.3.s=arima.sim(list(ma=c(-0.9)),n=60)\nestimate.ma1.mom(ma1.3.s)\n\n\n[1] NA\n\n\nCode\nma1.4.s=arima.sim(list(ma=c(-0.5)),n=60) \nestimate.ma1.mom(ma1.4.s)\n\n\n[1] 0.2571377\n\n\n\n\nCode\n# Display the results of the method of moments for all MA(1) models above in a table format\nlibrary(knitr)\nkable(data.frame(\n    c('ma1.1.s','ma1.2.s','ma1.3.s','ma1.4.s'),\n    c(0.9,-0.9,0.9,0.5),\n    c(estimate.ma1.mom(ma1.1.s),estimate.ma1.mom(ma1.2.s),\n      estimate.ma1.mom(ma1.3.s),estimate.ma1.mom(ma1.4.s)),\n    c(120,120,60,60)),\n    col.names=c('Data','True Value theta','Estimate of MA(1)', 'Sample size')\n    )  \n\n\n\n\n\nData\nTrue Value theta\nEstimate of MA(1)\nSample size\n\n\n\n\nma1.1.s\n0.9\n0.7196756\n120\n\n\nma1.2.s\n-0.9\n-0.5554273\n120\n\n\nma1.3.s\n0.9\nNA\n60\n\n\nma1.4.s\n0.5\n0.2571377\n60\n\n\n\n\n\n\n\n1.1.2 AR(p)\nMethod of moments estimator of the AR(1) coefficient \\(\\phi\\) is given by \\[\n\\widehat\\phi_{MoM}=r_1.\n\\]\n\n\nCode\n# Fitting an AR(1) model to the data\ndata(ar1.s)\nar(ar1.s,order.max=1,AIC=F,method='yw')\n\n\n\nCall:\nar(x = ar1.s, order.max = 1, method = \"yw\", AIC = F)\n\nCoefficients:\n     1  \n0.8314  \n\nOrder selected 1  sigma^2 estimated as  1.382\n\n\nCode\ndata(ar1.2.s)\nar(ar1.2.s,order.max=1,AIC=F,method='yw')\n\n\n\nCall:\nar(x = ar1.2.s, order.max = 1, method = \"yw\", AIC = F)\n\nCoefficients:\n     1  \n0.4699  \n\nOrder selected 1  sigma^2 estimated as  0.9198\n\n\nCode\ndata(ar2.s)\nar(ar2.s,order.max=2,AIC=F,method='yw')\n\n\n\nCall:\nar(x = ar2.s, order.max = 2, method = \"yw\", AIC = F)\n\nCoefficients:\n      1        2  \n 1.4694  -0.7646  \n\nOrder selected 2  sigma^2 estimated as  1.051\n\n\n\n\nCode\n# Display the results of the method of moments for all AR(1) models above in a table format\nkable(data.frame(\n    c('ar1.s','ar1.2.s','ar2.s','--'),\n    c('0.9','0.4','1.5','-0.75'),\n    c(ar(ar1.s,order.max=1,AIC=F,method='yw')$ar, \n      ar(ar1.2.s,order.max=1,AIC=F,method='yw')$ar,\n      ar(ar2.s,order.max=2,AIC=F,method='yw')$ar),\n    c(120,120,120, '--')),\n    col.names=c('Data','True Value phi','Estimate of AR(1)', 'Sample size')\n    )\n\n\n\n\n\nData\nTrue Value phi\nEstimate of AR(1)\nSample size\n\n\n\n\nar1.s\n0.9\n0.8313820\n120\n\n\nar1.2.s\n0.4\n0.4699186\n120\n\n\nar2.s\n1.5\n1.4694476\n120\n\n\n–\n-0.75\n-0.7646034\n–",
    "crumbs": [
      "R notebook: Parameter Estimation"
    ]
  },
  {
    "objectID": "TSA-Lecture16-parameter-estimation.html#comparison-of-estimation-methods",
    "href": "TSA-Lecture16-parameter-estimation.html#comparison-of-estimation-methods",
    "title": "439/639 TSA: Parameter Estimation",
    "section": "1.2 Comparison of Estimation methods",
    "text": "1.2 Comparison of Estimation methods\nWe will compare Method of Moments, Conditional Sum of Squares, and Maximum Likelihood Methods\n\n1.2.1 AR(1)\n\n\nCode\n# Exhibit 7.4\ndata(ar1.s)\nar(ar1.s,order.max=1,AIC=F,method='yw') # method of moments\n\n\n\nCall:\nar(x = ar1.s, order.max = 1, method = \"yw\", AIC = F)\n\nCoefficients:\n     1  \n0.8314  \n\nOrder selected 1  sigma^2 estimated as  1.382\n\n\nCode\nar(ar1.s,order.max=1,AIC=F,method='ols') # conditional sum of squares\n\n\n\nCall:\nar(x = ar1.s, order.max = 1, method = \"ols\", AIC = F)\n\nCoefficients:\n    1  \n0.857  \n\nIntercept: 0.02499 (0.1308) \n\nOrder selected 1  sigma^2 estimated as  1.008\n\n\nCode\nar(ar1.s,order.max=1,AIC=F,method='mle') # maximum likelihood\n\n\n\nCall:\nar(x = ar1.s, order.max = 1, method = \"mle\", AIC = F)\n\nCoefficients:\n     1  \n0.8924  \n\nOrder selected 1  sigma^2 estimated as  1.041\n\n\nCode\n# The AIC option is set to be False otherwise the function will choose\n# the AR order by minimizing AIC, so that zero order might be chosen.\n\ndata(ar1.2.s)\nar(ar1.2.s,order.max=1,AIC=F,method='yw') # method of moments\n\n\n\nCall:\nar(x = ar1.2.s, order.max = 1, method = \"yw\", AIC = F)\n\nCoefficients:\n     1  \n0.4699  \n\nOrder selected 1  sigma^2 estimated as  0.9198\n\n\nCode\nar(ar1.2.s,order.max=1,AIC=F,method='ols') # conditional sum of squares\n\n\n\nCall:\nar(x = ar1.2.s, order.max = 1, method = \"ols\", AIC = F)\n\nCoefficients:\n     1  \n0.4731  \n\nIntercept: -0.006084 (0.1237) \n\nOrder selected 1  sigma^2 estimated as  0.9024\n\n\nCode\nar(ar1.2.s,order.max=1,AIC=F,method='mle') # maximum likelihood\n\n\n\nCall:\nar(x = ar1.2.s, order.max = 1, method = \"mle\", AIC = F)\n\nCoefficients:\n     1  \n0.4654  \n\nOrder selected 1  sigma^2 estimated as  0.8875\n\n\n\n\nCode\n# Display the results of the method of moments, conditional sum of squares, and maximum likelihood for AR(1) models above in a table format\nkable(data.frame(\n    c('ar1.s','ar1.2.s'),\n    c('0.9','0.4'),\n    c(ar(ar1.s,order.max=1,AIC=F,method='yw')$ar, \n      ar(ar1.2.s,order.max=1,AIC=F,method='yw')$ar),\n    c(ar(ar1.s,order.max=1,AIC=F,method='ols')$ar, \n      ar(ar1.2.s,order.max=1,AIC=F,method='ols')$ar),\n    c(ar(ar1.s,order.max=1,AIC=F,method='mle')$ar, \n      ar(ar1.2.s,order.max=1,AIC=F,method='mle')$ar),\n    c(60,60)),\n    col.names=c('Data','True Value phi','MoM of AR(1)', 'CSS of of AR(1)', 'MLE of AR(1)', 'Sample size')\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nData\nTrue Value phi\nMoM of AR(1)\nCSS of of AR(1)\nMLE of AR(1)\nSample size\n\n\n\n\nar1.s\n0.9\n0.8313820\n0.8570411\n0.8923649\n60\n\n\nar1.2.s\n0.4\n0.4699186\n0.4730960\n0.4653505\n60\n\n\n\n\n\nThe standard errors of the estimated AR(1) coefficients are given by the following formula: \\[\nSE(\\hat\\phi)=\\sqrt{\\widehat{\\text{Var}}(\\hat\\phi)}=\\frac{1-\\hat\\phi^2}{n}\n\\]\nFor example, for the first AR(1) model, we have \\(\\hat\\phi=0.89\\) and \\(n=60\\), so that \\(SE(\\hat\\phi)=\\approx 0.06\\).\nAnd for the second AR(1) model, we have \\(\\hat\\phi=0.465\\) and \\(n=60\\), so that \\(SE(\\hat\\phi)=\\approx 0.11\\).\nAll the estimates are comparable.\n\n\n1.2.2 AR(2)\n\n\nCode\n# Exhibit 7.5\ndata(ar2.s)\nar(ar2.s,order.max=2,AIC=F,method='yw') # method of moments\n\n\n\nCall:\nar(x = ar2.s, order.max = 2, method = \"yw\", AIC = F)\n\nCoefficients:\n      1        2  \n 1.4694  -0.7646  \n\nOrder selected 2  sigma^2 estimated as  1.051\n\n\nCode\nar(ar2.s,order.max=2,AIC=F,method='ols') # conditional sum of squares\n\n\n\nCall:\nar(x = ar2.s, order.max = 2, method = \"ols\", AIC = F)\n\nCoefficients:\n      1        2  \n 1.5137  -0.8050  \n\nIntercept: 0.02043 (0.08594) \n\nOrder selected 2  sigma^2 estimated as  0.8713\n\n\nCode\nar(ar2.s,order.max=2,AIC=F,method='mle') # maximum likelihood\n\n\n\nCall:\nar(x = ar2.s, order.max = 2, method = \"mle\", AIC = F)\n\nCoefficients:\n      1        2  \n 1.5061  -0.7964  \n\nOrder selected 2  sigma^2 estimated as  0.862\n\n\nCode\n# Display the results of the method of moments, conditional sum of squares, and maximum likelihood for AR(2) models above in a table format\nkable(data.frame(\n    c('ar2.s'),\n    c('1.5','-0.75'),\n    c(ar(ar2.s,order.max=2,AIC=F,method='yw')$ar),\n    c(ar(ar2.s,order.max=2,AIC=F,method='ols')$ar),\n    c(ar(ar2.s,order.max=2,AIC=F,method='mle')$ar),\n    c(120)),\n    col.names=c('Data','True Value phi_i','MoM of AR(2)', 'CSS of AR(2)', 'MLE of AR(2)', 'Sample size')\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nData\nTrue Value phi_i\nMoM of AR(2)\nCSS of AR(2)\nMLE of AR(2)\nSample size\n\n\n\n\nar2.s\n1.5\n1.4694476\n1.5137146\n1.5061369\n120\n\n\nar2.s\n-0.75\n-0.7646034\n-0.8049905\n-0.7964453\n120\n\n\n\n\n\nStandard errors of the estimated AR(2) coefficients are given by the following formula:\n\\[\nSE(\\hat\\phi_1)\\approx SE(\\hat\\phi_2)\\approx\\sqrt{\\frac{1-\\hat\\phi_2^2}{n}}\\approx 0.06\n\\]",
    "crumbs": [
      "R notebook: Parameter Estimation"
    ]
  },
  {
    "objectID": "TSA-Lecture16-parameter-estimation.html#conditional-least-squaressum-of-squares",
    "href": "TSA-Lecture16-parameter-estimation.html#conditional-least-squaressum-of-squares",
    "title": "439/639 TSA: Parameter Estimation",
    "section": "1.3 Conditional Least Squares/Sum of Squares",
    "text": "1.3 Conditional Least Squares/Sum of Squares\n\n1.3.1 MA(1)\n\n\nCode\n# Using conditinal sum of squares method\narima(ma1.4.s,order=c(0,0,1),method='CSS',include.mean=F)\n\n\n\nCall:\narima(x = ma1.4.s, order = c(0, 0, 1), include.mean = F, method = \"CSS\")\n\nCoefficients:\n         ma1\n      -0.474\ns.e.   0.191\n\nsigma^2 estimated as 0.7329:  part log likelihood = -75.81\n\n\n\n\n1.3.2 ARMA(1,1)\n\n\nCode\n# Fitting an ARMA(1,1) model to the data\n# Exhibit 7.6\ndata(arma11.s)\narima(arma11.s, order=c(1,0,1),method='CSS') # conditional sum of squares\n\n\n\nCall:\narima(x = arma11.s, order = c(1, 0, 1), method = \"CSS\")\n\nCoefficients:\n         ar1     ma1  intercept\n      0.5586  0.3669     0.3928\ns.e.  0.1219  0.1564     0.3380\n\nsigma^2 estimated as 1.199:  part log likelihood = -150.98\n\n\nCode\narima(arma11.s, order=c(1,0,1),method='ML') # maximum likelihood\n\n\n\nCall:\narima(x = arma11.s, order = c(1, 0, 1), method = \"ML\")\n\nCoefficients:\n         ar1     ma1  intercept\n      0.5647  0.3557     0.3216\ns.e.  0.1205  0.1585     0.3358\n\nsigma^2 estimated as 1.197:  log likelihood = -151.33,  aic = 308.65\n\n\nCode\n# \n# Recall that R uses the plus convention whereas our book uses the minus \n# convention in the specification of the MA part, i.e. R specifies an\n# ARMA(1,1) model as z_t=theta_0+phi*z_{t-1}+e_t+theta_1*e_{t-1} \n# versus our convention\n# z_t=theta_0+phi*z_{t-1}+e_t-theta_1*e_{t-1} \n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMOM\nCSS\nUCSS\nMLE\n\\(n\\)\n\n\n\n\n\\(\\phi = 0.6\\)\n0.637\n0.5586\n0.5691\n0.5647\n100\n\n\n\\(\\theta = −0.3\\)\n−0.2066\n−0.3669\n−0.3618\n−0.3557\n100",
    "crumbs": [
      "R notebook: Parameter Estimation"
    ]
  },
  {
    "objectID": "TSA-Lecture17-model-diagnostics.html",
    "href": "TSA-Lecture17-model-diagnostics.html",
    "title": "R notebook: Model Diagnostics",
    "section": "",
    "text": "Step 1: Model Specification\nStep 2: Parameter Estimation\nStep 3: Model Checking\nStep 4: Forecasting",
    "crumbs": [
      "R notebook: Diagnostics"
    ]
  },
  {
    "objectID": "TSA-Lecture17-model-diagnostics.html#steps-in-time-series-analysis",
    "href": "TSA-Lecture17-model-diagnostics.html#steps-in-time-series-analysis",
    "title": "R notebook: Model Diagnostics",
    "section": "",
    "text": "Step 1: Model Specification\nStep 2: Parameter Estimation\nStep 3: Model Checking\nStep 4: Forecasting",
    "crumbs": [
      "R notebook: Diagnostics"
    ]
  },
  {
    "objectID": "TSA-Lecture17-model-diagnostics.html#color-property-dataset",
    "href": "TSA-Lecture17-model-diagnostics.html#color-property-dataset",
    "title": "R notebook: Model Diagnostics",
    "section": "2.1 Color Property Dataset",
    "text": "2.1 Color Property Dataset\n\n\nCode\n# Exhibit 8.1\ndata(color)\nm1.color=arima(color,order=c(1,0,0), method = \"ML\")\nm1.color\n\n\n\nCall:\narima(x = color, order = c(1, 0, 0), method = \"ML\")\n\nCoefficients:\n         ar1  intercept\n      0.5706    74.3293\ns.e.  0.1435     1.9151\n\nsigma^2 estimated as 24.83:  log likelihood = -106.07,  aic = 216.15\n\n\nCode\nplot(rstandard(m1.color),ylab='Standardized residuals',\n    type='b', \n    main = 'Standardized Residuals from AR(1) Model for Color Property')\nabline(h=0)\n\n\n\n\n\n\n\n\n\n\n2.1.1 Assessing Normality of Residuals\n\n\nCode\nqqnorm(residuals(m1.color), main='QQ Plot of Residuals from AR(1) Model for Color Property')\nqqline(residuals(m1.color))\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro-Wilk test for normality, H0: normal, H1: not normal\nshapiro.test(residuals(m1.color)) \n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m1.color)\nW = 0.97536, p-value = 0.6057\n\n\n\n\n2.1.2 Assessing Independence of Residuals\nTo distinguish sample ACF of the time series from the sample ACF of the residuals we use the following notation: \\[\n\\hat{r}_k = \\text{sample ACF of the residuals},\\qquad\n{r}_k = \\text{sample ACF of $Y_t$}\n\\]\nUnfortunately, the sample autocorrelation function of residuals has a complex structure. We will compare residual ACF with the theoretical ACF of white noise, even though this is not entirely correct.\n\n\nCode\nacf(residuals(m1.color),main='Sample ACF of Residuals from AR(1) Model for Color')\n\n\n\n\n\n\n\n\n\n\n\n2.1.3 Ljung-Box Test\nThe Ljung-Box test is a statistical test that checks for the presence of autocorrelation as a whole in a time series. The null hypothesis of the test is that there is no autocorrelation in the residuals.\n\\[\nH_0: \\text{all residual ACF is zero for lags $k, k&gt;0$}\\quad\\text{ vs }\\quad H_1: \\text{at least one residual ACF is not zero}\n\\]\nThe test statistic for a specific parameter value \\(K\\) is given by: \\[\nQ = n(n+2)\\sum_{k=1}^{h}\\left[\\frac{\\hat{r}_1^2}{n-1}+\\frac{\\hat{r}_2^2}{n-2}+\\ldots+\\frac{\\hat{r}_K^2}{n-K}\\right]\\sim \\chi^2(K-p-q)\n\\] where \\(n\\) is the number of observations, \\(K\\) is the number of lags, and \\(\\hat{r}_k\\) is the sample autocorrelation of residuals at lag \\(k\\).\nHere the maximum lag \\(K\\) is selected somewhat arbitrarily but large enough that the \\(\\psi\\)-weights are negligible for \\(j &gt; K\\).\n\n\nCode\nlibrary(stats)\nBox.test(residuals(m1.color), lag=6, type=\"Ljung-Box\", fitdf=1) # fitdf is the number of parameters in the model\n\n\n\n    Box-Ljung test\n\ndata:  residuals(m1.color)\nX-squared = 0.28032, df = 5, p-value = 0.998\n\n\nWe can show all the diagnostics at once using the tsdiag function. The gof argument specifies the number of lags to be used in the Ljung-Box test.\n\n\nCode\ntsdiag(m1.color,gof=15,omit.initial=F)",
    "crumbs": [
      "R notebook: Diagnostics"
    ]
  },
  {
    "objectID": "TSA-Lecture17-model-diagnostics.html#hare-dataset",
    "href": "TSA-Lecture17-model-diagnostics.html#hare-dataset",
    "title": "R notebook: Model Diagnostics",
    "section": "2.2 Hare Dataset",
    "text": "2.2 Hare Dataset\nRecall, that the most appropriate model for the hare dataset is AR(3) model with \\(\\phi_2=0\\) fitted to the square root of the data. The model is given by: \\[  \n\\sqrt{Y_t} = \\phi_0 + \\phi_1 \\sqrt{Y_{t-1}} + \\phi_3 \\sqrt{Y_{t-3}} + e_t\n\\] where \\(\\phi_0\\) is the intercept term.\n\n\nCode\ndata(hare)\nm1.hare=arima(sqrt(hare),order=c(3,0,0))\nm1.hare # the AR(2) coefficient is not significant; it is second in the\n\n\n\nCall:\narima(x = sqrt(hare), order = c(3, 0, 0))\n\nCoefficients:\n         ar1      ar2      ar3  intercept\n      1.0519  -0.2292  -0.3931     5.6923\ns.e.  0.1877   0.2942   0.1915     0.3371\n\nsigma^2 estimated as 1.066:  log likelihood = -46.54,  aic = 101.08\n\n\nCode\n# list of coefficients.\nm2.hare=arima(sqrt(hare),order=c(3,0,0),fixed=c(NA,0,NA,NA)) # fixed the AR(2)\n# coefficient to be 0 via the fixed argument.\nm2.hare\n\n\n\nCall:\narima(x = sqrt(hare), order = c(3, 0, 0), fixed = c(NA, 0, NA, NA))\n\nCoefficients:\n         ar1  ar2      ar3  intercept\n      0.9190    0  -0.5313     5.6889\ns.e.  0.0791    0   0.0697     0.3179\n\nsigma^2 estimated as 1.088:  log likelihood = -46.85,  aic = 99.69\n\n\nCode\n# Note that the intercept term is actually the mean in the centered form\n# of the ARMA model, i.e. if Y(t)=sqrt(hare)-intercept, then the model is\n# Y(t)=0.919*Y(t-1)-0.5313*Y(t-3)+e(t) \n# So the \"true\" intercept equals 5.6889*(1-0.919+0.5313)=3.483, as stated in the book\nplot(rstandard(m2.hare),ylab='Standardized residuals',\n    type='b',\n    main = 'Standardized Residuals from AR(3) Model for sqrt(Hare)')\nabline(h=0)\n\n\n\n\n\n\n\n\n\nCode\n# Bonferoni correction for the number of tests\nalpha=0.05/length(hare) # Bonferroni correction for the number of tests\n# critical value for the two-tailed test\nqnorm(1-alpha/2) \n\n\n[1] 3.153563\n\n\n\n2.2.1 Assessing Normality of Residuals\n\n\nCode\n# Exhibit 8.5\nqqnorm(residuals(m1.hare), main = 'QQ Plot of Residuals from AR(3) Model for sqrt(Hare)')\nqqline(residuals(m1.hare))\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro-Wilk test for normality, H0: normal, H1: not normal\nshapiro.test(residuals(m1.hare)) \n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m1.hare)\nW = 0.93509, p-value = 0.06043\n\n\n\n\n2.2.2 Residual ACF\n\n\nCode\ntsdiag(m1.hare,gof=15,omit.initial=F)",
    "crumbs": [
      "R notebook: Diagnostics"
    ]
  },
  {
    "objectID": "TSA-Lecture17-model-diagnostics.html#oil-prices",
    "href": "TSA-Lecture17-model-diagnostics.html#oil-prices",
    "title": "R notebook: Model Diagnostics",
    "section": "2.3 Oil prices",
    "text": "2.3 Oil prices\nRecall that the most appropriate model for the oil prices dataset is ARIMA(0,1,1) model, aka IMA(1,1) model, fitted to the \\(\\log Y_t\\). The model is given by:\n\\[\n\\nabla \\log(Y_t) =  e_t - \\theta e_{t-1}\n\\] where \\(\\nabla \\log(Y_t) = \\log(Y_t) - \\log(Y_{t-1})\\) is the first difference of the data.\n\n\nCode\ndata(oil.price)\nm1.oil=arima(log(oil.price),order=c(0,1,1))\nm1.oil # IMA(1,1) model\n\n\n\nCall:\narima(x = log(oil.price), order = c(0, 1, 1))\n\nCoefficients:\n         ma1\n      0.2956\ns.e.  0.0693\n\nsigma^2 estimated as 0.006689:  log likelihood = 260.29,  aic = -518.58\n\n\nCode\n# Get standardized residuals\nstd_resid &lt;- rstandard(m1.oil)\n\n# Bonferroni-corrected alpha\nalpha &lt;- 0.05 / length(std_resid)  # number of tests = number of residuals\ncrit &lt;- qnorm(1 - alpha / 2)\n\n# Plot\nplot(std_resid, ylab = 'Standardized residuals', type = 'l',\n     main = 'Standardized Residuals from IMA(1,1) Model for log(Oil Price)')\nabline(h = 0)\nabline(h = crit, col = 'red', lty = 2)\nabline(h = -crit, col = 'red', lty = 2)\n\n\n\n\n\n\n\n\n\n\n2.3.1 Assessing Normality of Residuals\n\n\nCode\n# Exhibit 8.6\nqqnorm(residuals(m1.oil), main = 'QQ Plot of Residuals from IMA(1,1) Model for log(Oil Price)')\nqqline(residuals(m1.oil))\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro-Wilk test for normality, H0: normal, H1: not normal\nshapiro.test(residuals(m1.oil)) \n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m1.oil)\nW = 0.96883, p-value = 3.937e-05\n\n\nOutliers seem to affect the normality of the residuals.\n\n\n2.3.2 Residual ACF\n\n\nCode\ntsdiag(m1.oil,gof=15,omit.initial=F)",
    "crumbs": [
      "R notebook: Diagnostics"
    ]
  },
  {
    "objectID": "TSA-Lecture17-model-diagnostics.html#color-property-dataset-1",
    "href": "TSA-Lecture17-model-diagnostics.html#color-property-dataset-1",
    "title": "R notebook: Model Diagnostics",
    "section": "3.1 Color Property Dataset",
    "text": "3.1 Color Property Dataset\nLet’s fit an AR(2) model to the color property dataset. The original suggested model is AR(1).\n\n\nCode\n# Exhibit 8.13\nm1.color \n\n\n\nCall:\narima(x = color, order = c(1, 0, 0), method = \"ML\")\n\nCoefficients:\n         ar1  intercept\n      0.5706    74.3293\ns.e.  0.1435     1.9151\n\nsigma^2 estimated as 24.83:  log likelihood = -106.07,  aic = 216.15\n\n\nCode\n# Exhibit 8.14\nm2.color=arima(color,order=c(2,0,0))\nm2.color\n\n\n\nCall:\narima(x = color, order = c(2, 0, 0))\n\nCoefficients:\n         ar1     ar2  intercept\n      0.5173  0.1005    74.1551\ns.e.  0.1717  0.1815     2.1463\n\nsigma^2 estimated as 24.6:  log likelihood = -105.92,  aic = 217.84\n\n\nWe can alternatively fit an ARMA(1,1) model to the data. The original suggested model is AR(1).\n\n\nCode\n# Exhibit 8.15\nm3.color=arima(color,order=c(1,0,1))\nm3.color\n\n\n\nCall:\narima(x = color, order = c(1, 0, 1))\n\nCoefficients:\n         ar1      ma1  intercept\n      0.6721  -0.1467    74.1730\ns.e.  0.2147   0.2742     2.1357\n\nsigma^2 estimated as 24.63:  log likelihood = -105.94,  aic = 217.88\n\n\nFor AR(2) and ARMA(1,1) models, the estimates of \\(\\phi_2\\) and \\(\\theta\\) are not significantly different from zero. The estimates of \\(\\phi_1\\) are not significantly different from the estimates of \\(\\phi_1\\) from the original AR(1) model.",
    "crumbs": [
      "R notebook: Diagnostics"
    ]
  },
  {
    "objectID": "TSA-Lecture17-model-diagnostics.html#parameter-redundancy-and-lack-of-identifiability",
    "href": "TSA-Lecture17-model-diagnostics.html#parameter-redundancy-and-lack-of-identifiability",
    "title": "R notebook: Model Diagnostics",
    "section": "3.2 Parameter Redundancy and Lack of Identifiability",
    "text": "3.2 Parameter Redundancy and Lack of Identifiability\nAny ARMA(\\(p,q\\)) model can be expressed as an unidentifiable ARMA(\\(p+1,q+1\\)) model. \\[\n\\Phi(B)Y_t=\\Theta(B)e_t\n\\]\nMultiplying both sides by \\((1-cB)\\) for any constant \\(c\\) gives us an ARMA(\\(p+1,q+1\\)) model: \\[\n(1-cB)\\Phi(B)Y_t=(1-cB)\\Theta(B)e_t\n\\]\nParameters in this model are not unique (we can take any \\(c\\)), and thus are not identifiable.\nFollow the rules of a good Time Series Analyst:\n\nSpecify the original model carefully. Do not overparameterize the model. Start with simpler models.\nWhen overfitting, do not increase the order of both AR and MA terms at the same time. Increase one at a time.\nExtend the model in the direction suggested by diagnostics of residuals. E.g., if after fitting an MA(1) model, substantial correlation remains at lag 2, then try an MA(2) model, not an ARMA(1,1) model.\n\n\n3.2.1 Color Property Dataset\nThe original suggested model is AR(1). Let’s fit an ARMA(2,1) model to the data.\n\n\nCode\n# Exhibit 8.16\nm4.color=arima(color,order=c(2,0,1))\nm4.color\n\n\n\nCall:\narima(x = color, order = c(2, 0, 1))\n\nCoefficients:\n         ar1     ar2     ma1  intercept\n      0.2189  0.2735  0.3036    74.1653\ns.e.  2.0056  1.1376  2.0650     2.1121\n\nsigma^2 estimated as 24.58:  log likelihood = -105.91,  aic = 219.82\n\n\nEven though AIC values are close to the original AR(1) model, the estimates of \\(\\phi_1, \\phi_2, \\theta\\) are not significantly different from zero.",
    "crumbs": [
      "R notebook: Diagnostics"
    ]
  },
  {
    "objectID": "TSA-Lecture22-spurious.html",
    "href": "TSA-Lecture22-spurious.html",
    "title": "439/639: Vector Time Series and Spurious Correlation",
    "section": "",
    "text": "Cross-covariance and cross-correlation functions are used to analyze the relationship between two time series. The cross-covariance function measures the covariance between two time series at different lags, while the cross-correlation function measures the correlation between two time series at different lags. \\[\n\\gamma_m(x,y) = Cov(X_t, Y_{t+m})\n\\]\nCross-correlation function is defined as: \\[\nr_m(x,y) = \\frac{\\gamma_m(x,y)}{\\sqrt{\\gamma_0(x,x)\\gamma_0(y,y)}}\n\\]\nwhere \\(\\gamma_0(x,x)\\) and \\(\\gamma_0(y,y)\\) are the variances of the time series \\(x\\) and \\(y\\), respectively. The cross-correlation function is a normalized version of the cross-covariance function, which allows for easier interpretation and comparison between different time series.\nOften cross-correlation function is displayed in a grid of plots, where each plot shows the cross-correlation function between two time series at different lags. This allows for a visual representation of the relationship between the two time series and can help identify any patterns or trends in the data.\n\n\nCode\n# Load required packages\nlibrary(TSA)\n\nset.seed(639)\nX=rnorm(105) \nY=zlag(X,2)+0.5*rnorm(105)\nX=ts(X[-(1:5)],start=1,freq=1)\nY=ts(Y[-(1:5)],start=1,freq=1)\n\n# Make X, Y a multivariate time series\nlibrary(astsa)\nX=ts(X,start=1,freq=1)\nY=ts(Y,start=1,freq=1)\nXY &lt;- ts(cbind(X, Y), start = 1, frequency = 1)\n# Cross-covariance function\nacfm(XY)\n\n\n\n\n\n\n\n\n\n\n\nBartlett’s theorem states that the sample cross-correlation function converges to the true cross-correlation function as the sample size increases. This means that as the number of observations in the time series increases, the sample cross-correlation function will become a more accurate estimate of the true cross-correlation function.\n\\[\nr_m(X,Y)\\sim N(\\rho_m(X,Y),\\frac{1}{n}\\left[1+2\\sum_{k=1}^\\infty\\rho_k(X)\\rho_k(Y)\\right])\n\\]\nwhere \\(\\rho_k(X)\\) and \\(\\rho_k(Y)\\) are the autocorrelation functions of the time series \\(X\\) and \\(Y\\), respectively.\nThe term \\(2\\sum_{k=1}^\\infty\\rho_k(X)\\rho_k(Y)\\) often result in inflation of the variance of the sample cross-correlation function, which can lead to misleading conclusions about the relationship between the two time series. This is particularly important when analyzing time series data with long memory or strong autocorrelation, as these characteristics can significantly affect the sample cross-correlation function.\nIn the example of \\(X_t\\sim AR(1)\\) and \\(Y_t\\sim AR(1)\\) the variance becomes: \\[\n\\frac{1}{n}\\frac{1+\\phi_X\\phi_Y}{1-\\phi_X\\phi_Y}\n\\]\n\n\nCode\nset.seed(639)\nlibrary(astsa)\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nxy &lt;- ts(cbind(x, y), start = 1, frequency = 1)\nacfm(xy)\n\n\n\n\n\n\n\n\n\nCode\nccf(xy[,1], xy[,2], lag.max = 20, main = 'Cross-correlation function of X and Y')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- arima.sim(model = list(order = c(1, 0, 0), ar = 0.8), n = 100)\ny &lt;- arima.sim(model = list(order = c(1, 0, 0), ar = 0.8), n = 100)\nxy &lt;- ts(cbind(x, y), start = 1, frequency = 1)\nacfm(xy)\n\n\n\n\n\n\n\n\n\n\n\n\nPrewhitening is a technique used to remove the effects of autocorrelation from a time series before analyzing the cross-correlation between two time series. This is particularly important when the two time series have strong autocorrelation, as this can lead to misleading conclusions about the relationship between the two time series.\n\n\nCode\nset.seed(123)\nx &lt;- arima.sim(n = 100, model = list(ar = 0.9))\ny &lt;- 0.5 * x + arima.sim(n = 100, model = list(ar = 0.9))\n\nccf(x,y)\n\n\n\n\n\n\n\n\n\nCode\n# Prewhiten x and filter y\nprewhiten(x, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata(milk) \ndata(electricity)\nmilk.electricity &lt;- ts.intersect(milk,log(electricity))\nplot(milk.electricity,yax.flip=T)\n\n\n\n\n\n\n\n\n\nCode\nacfm(milk.electricity)\n\n\n\n\n\n\n\n\n\nCode\nccf(milk.electricity[,1],milk.electricity[,2],\n    lag.max=20,main='CCF of Milk and Electricity')\n\n\n\n\n\n\n\n\n\nLet’s prewhiten the data\n\n\nCode\nme.dif &lt;- ts.intersect(diff(diff(milk,12)),\n                    diff(diff(log(electricity),12)))\nprewhiten(as.vector(me.dif[,1]),\n        as.vector(me.dif[,2]),\n        ylab='CCF')\n\n\n\n\n\n\n\n\n\n\n\n\nOur first example of this section is a sales and price dataset of a certain potato chip from Bluebird Foods Ltd., New Zealand. The data consist of the log-transformed weekly unit sales of large packages of standard potato chips sold and the weekly average price over a period of 104 weeks.\n\n\nCode\ndata(bluebird)\nplot(bluebird,yax.flip=T)\n\n\n\n\n\n\n\n\n\nCode\nacfm(bluebird)\n\n\n\n\n\n\n\n\n\nCode\nprewhiten(y=diff(bluebird)[,1],x=diff(bluebird)[,2],ylab='CCF')\n\n\n\n\n\n\n\n\n\nWe see strong contemporaneous correlation between the two series. Therefore the model should be \\[\nLog.sales_t = \\beta_0 + \\beta_1 Price_t + \\epsilon_t\n\\] The model can be estimated using OLS regression.\n\n\nCode\nsales=bluebird[,1] \nprice=bluebird[,2]\nchip.m1=lm(sales~price,data=bluebird)\nsummary(chip.m1)\n\n\n\nCall:\nlm(formula = sales ~ price, data = bluebird)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54950 -0.12373  0.00667  0.13136  0.45170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.890      0.217   73.22   &lt;2e-16 ***\nprice         -2.489      0.126  -19.75   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.188 on 102 degrees of freedom\nMultiple R-squared:  0.7926,    Adjusted R-squared:  0.7906 \nF-statistic: 389.9 on 1 and 102 DF,  p-value: &lt; 2.2e-16\n\n\nAnalyzing the residuals of the model, we see that they are not white noise. This indicates that there is still some autocorrelation present in the data, which suggests that the model may not be fully capturing the relationship between the two time series.\n\n\nCode\nacf(residuals(chip.m1),ci.type='ma')\n\n\n\n\n\n\n\n\n\nCode\npacf(residuals(chip.m1))\n\n\n\n\n\n\n\n\n\nCode\neacf(residuals(chip.m1))\n\n\nAR/MA\n  0 1 2 3 4 5 6 7 8 9 10 11 12 13\n0 x x x x o o x x o o o  o  o  o \n1 x o o x o o o o o o o  o  o  o \n2 x x o x o o o o o o o  o  o  o \n3 x x o x o o o o o o o  o  o  o \n4 o x x o o o o o o o o  o  o  o \n5 x x x o x o o o o o o  o  o  o \n6 x x o x x x o o o o o  o  o  o \n7 x o x o o o o o o o o  o  o  o \n\n\nThe diagnostics suggest the model: _______\n\n\n\nCode\nchip.m2=arima(sales,order=c(1,0,4),xreg=data.frame(price))\nchip.m2\n\n\n\nCall:\narima(x = sales, order = c(1, 0, 4), xreg = data.frame(price))\n\nCoefficients:\n         ar1      ma1     ma2     ma3     ma4  intercept    price\n      0.1989  -0.0554  0.2521  0.0735  0.5269    15.7792  -2.4234\ns.e.  0.1843   0.1660  0.0865  0.1084  0.1376     0.2166   0.1247\n\nsigma^2 estimated as 0.02556:  log likelihood = 42.35,  aic = -70.69\n\n\nCode\nchip.m3=arima(sales,order=c(1,0,4),xreg=data.frame(price),\nfixed=c(NA,0,NA,0,NA,NA,NA)) \nchip.m3\n\n\n\nCall:\narima(x = sales, order = c(1, 0, 4), xreg = data.frame(price), fixed = c(NA, \n    0, NA, 0, NA, NA, NA))\n\nCoefficients:\n         ar1  ma1     ma2  ma3     ma4  intercept    price\n      0.1444    0  0.2676    0  0.5210    15.8396  -2.4588\ns.e.  0.0985    0  0.0858    0  0.1171     0.2027   0.1166\n\nsigma^2 estimated as 0.02572:  log likelihood = 42.09,  aic = -74.18\n\n\nCode\nchip.m4=arima(sales,order=c(0,0,4),xreg=data.frame(price),\nfixed=c(0,NA,0,NA,NA,NA)) \nchip.m4\n\n\n\nCall:\narima(x = sales, order = c(0, 0, 4), xreg = data.frame(price), fixed = c(0, \n    NA, 0, NA, NA, NA))\n\nCoefficients:\n      ma1     ma2  ma3     ma4  intercept    price\n        0  0.2884    0  0.5416    15.8559  -2.4682\ns.e.    0  0.0794    0  0.1167     0.1909   0.1100\n\nsigma^2 estimated as 0.02623:  log likelihood = 41.02,  aic = -74.05\n\n\nNote that the regression coefficient estimate on Price is similar to that from the OLS regression fit earlier, but the standard error of the estimate is about 10% lower than that from the simple OLS regression. This illustrates the general result that the simple OLS estimator is consistent but the associated standard error is generally not trustworthy.\nDiagnosing the residuals from this model\n\n\nCode\ntsdiag(chip.m4)",
    "crumbs": [
      "R notebook: Advanced Topics"
    ]
  },
  {
    "objectID": "TSA-Lecture22-spurious.html#bartletts-theorem-for-sample-cross-correlation-function",
    "href": "TSA-Lecture22-spurious.html#bartletts-theorem-for-sample-cross-correlation-function",
    "title": "439/639: Vector Time Series and Spurious Correlation",
    "section": "",
    "text": "Bartlett’s theorem states that the sample cross-correlation function converges to the true cross-correlation function as the sample size increases. This means that as the number of observations in the time series increases, the sample cross-correlation function will become a more accurate estimate of the true cross-correlation function.\n\\[\nr_m(X,Y)\\sim N(\\rho_m(X,Y),\\frac{1}{n}\\left[1+2\\sum_{k=1}^\\infty\\rho_k(X)\\rho_k(Y)\\right])\n\\]\nwhere \\(\\rho_k(X)\\) and \\(\\rho_k(Y)\\) are the autocorrelation functions of the time series \\(X\\) and \\(Y\\), respectively.\nThe term \\(2\\sum_{k=1}^\\infty\\rho_k(X)\\rho_k(Y)\\) often result in inflation of the variance of the sample cross-correlation function, which can lead to misleading conclusions about the relationship between the two time series. This is particularly important when analyzing time series data with long memory or strong autocorrelation, as these characteristics can significantly affect the sample cross-correlation function.\nIn the example of \\(X_t\\sim AR(1)\\) and \\(Y_t\\sim AR(1)\\) the variance becomes: \\[\n\\frac{1}{n}\\frac{1+\\phi_X\\phi_Y}{1-\\phi_X\\phi_Y}\n\\]\n\n\nCode\nset.seed(639)\nlibrary(astsa)\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nxy &lt;- ts(cbind(x, y), start = 1, frequency = 1)\nacfm(xy)\n\n\n\n\n\n\n\n\n\nCode\nccf(xy[,1], xy[,2], lag.max = 20, main = 'Cross-correlation function of X and Y')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- arima.sim(model = list(order = c(1, 0, 0), ar = 0.8), n = 100)\ny &lt;- arima.sim(model = list(order = c(1, 0, 0), ar = 0.8), n = 100)\nxy &lt;- ts(cbind(x, y), start = 1, frequency = 1)\nacfm(xy)",
    "crumbs": [
      "R notebook: Advanced Topics"
    ]
  },
  {
    "objectID": "TSA-Lecture22-spurious.html#prewhitening",
    "href": "TSA-Lecture22-spurious.html#prewhitening",
    "title": "439/639: Vector Time Series and Spurious Correlation",
    "section": "",
    "text": "Prewhitening is a technique used to remove the effects of autocorrelation from a time series before analyzing the cross-correlation between two time series. This is particularly important when the two time series have strong autocorrelation, as this can lead to misleading conclusions about the relationship between the two time series.\n\n\nCode\nset.seed(123)\nx &lt;- arima.sim(n = 100, model = list(ar = 0.9))\ny &lt;- 0.5 * x + arima.sim(n = 100, model = list(ar = 0.9))\n\nccf(x,y)\n\n\n\n\n\n\n\n\n\nCode\n# Prewhiten x and filter y\nprewhiten(x, y)",
    "crumbs": [
      "R notebook: Advanced Topics"
    ]
  },
  {
    "objectID": "TSA-Lecture22-spurious.html#milk-vs-electricity-production-example",
    "href": "TSA-Lecture22-spurious.html#milk-vs-electricity-production-example",
    "title": "439/639: Vector Time Series and Spurious Correlation",
    "section": "",
    "text": "Code\ndata(milk) \ndata(electricity)\nmilk.electricity &lt;- ts.intersect(milk,log(electricity))\nplot(milk.electricity,yax.flip=T)\n\n\n\n\n\n\n\n\n\nCode\nacfm(milk.electricity)\n\n\n\n\n\n\n\n\n\nCode\nccf(milk.electricity[,1],milk.electricity[,2],\n    lag.max=20,main='CCF of Milk and Electricity')\n\n\n\n\n\n\n\n\n\nLet’s prewhiten the data\n\n\nCode\nme.dif &lt;- ts.intersect(diff(diff(milk,12)),\n                    diff(diff(log(electricity),12)))\nprewhiten(as.vector(me.dif[,1]),\n        as.vector(me.dif[,2]),\n        ylab='CCF')",
    "crumbs": [
      "R notebook: Advanced Topics"
    ]
  },
  {
    "objectID": "TSA-Lecture22-spurious.html#bluebird-potato-chips-example",
    "href": "TSA-Lecture22-spurious.html#bluebird-potato-chips-example",
    "title": "439/639: Vector Time Series and Spurious Correlation",
    "section": "",
    "text": "Our first example of this section is a sales and price dataset of a certain potato chip from Bluebird Foods Ltd., New Zealand. The data consist of the log-transformed weekly unit sales of large packages of standard potato chips sold and the weekly average price over a period of 104 weeks.\n\n\nCode\ndata(bluebird)\nplot(bluebird,yax.flip=T)\n\n\n\n\n\n\n\n\n\nCode\nacfm(bluebird)\n\n\n\n\n\n\n\n\n\nCode\nprewhiten(y=diff(bluebird)[,1],x=diff(bluebird)[,2],ylab='CCF')\n\n\n\n\n\n\n\n\n\nWe see strong contemporaneous correlation between the two series. Therefore the model should be \\[\nLog.sales_t = \\beta_0 + \\beta_1 Price_t + \\epsilon_t\n\\] The model can be estimated using OLS regression.\n\n\nCode\nsales=bluebird[,1] \nprice=bluebird[,2]\nchip.m1=lm(sales~price,data=bluebird)\nsummary(chip.m1)\n\n\n\nCall:\nlm(formula = sales ~ price, data = bluebird)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54950 -0.12373  0.00667  0.13136  0.45170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   15.890      0.217   73.22   &lt;2e-16 ***\nprice         -2.489      0.126  -19.75   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.188 on 102 degrees of freedom\nMultiple R-squared:  0.7926,    Adjusted R-squared:  0.7906 \nF-statistic: 389.9 on 1 and 102 DF,  p-value: &lt; 2.2e-16\n\n\nAnalyzing the residuals of the model, we see that they are not white noise. This indicates that there is still some autocorrelation present in the data, which suggests that the model may not be fully capturing the relationship between the two time series.\n\n\nCode\nacf(residuals(chip.m1),ci.type='ma')\n\n\n\n\n\n\n\n\n\nCode\npacf(residuals(chip.m1))\n\n\n\n\n\n\n\n\n\nCode\neacf(residuals(chip.m1))\n\n\nAR/MA\n  0 1 2 3 4 5 6 7 8 9 10 11 12 13\n0 x x x x o o x x o o o  o  o  o \n1 x o o x o o o o o o o  o  o  o \n2 x x o x o o o o o o o  o  o  o \n3 x x o x o o o o o o o  o  o  o \n4 o x x o o o o o o o o  o  o  o \n5 x x x o x o o o o o o  o  o  o \n6 x x o x x x o o o o o  o  o  o \n7 x o x o o o o o o o o  o  o  o \n\n\nThe diagnostics suggest the model: _______\n\n\n\nCode\nchip.m2=arima(sales,order=c(1,0,4),xreg=data.frame(price))\nchip.m2\n\n\n\nCall:\narima(x = sales, order = c(1, 0, 4), xreg = data.frame(price))\n\nCoefficients:\n         ar1      ma1     ma2     ma3     ma4  intercept    price\n      0.1989  -0.0554  0.2521  0.0735  0.5269    15.7792  -2.4234\ns.e.  0.1843   0.1660  0.0865  0.1084  0.1376     0.2166   0.1247\n\nsigma^2 estimated as 0.02556:  log likelihood = 42.35,  aic = -70.69\n\n\nCode\nchip.m3=arima(sales,order=c(1,0,4),xreg=data.frame(price),\nfixed=c(NA,0,NA,0,NA,NA,NA)) \nchip.m3\n\n\n\nCall:\narima(x = sales, order = c(1, 0, 4), xreg = data.frame(price), fixed = c(NA, \n    0, NA, 0, NA, NA, NA))\n\nCoefficients:\n         ar1  ma1     ma2  ma3     ma4  intercept    price\n      0.1444    0  0.2676    0  0.5210    15.8396  -2.4588\ns.e.  0.0985    0  0.0858    0  0.1171     0.2027   0.1166\n\nsigma^2 estimated as 0.02572:  log likelihood = 42.09,  aic = -74.18\n\n\nCode\nchip.m4=arima(sales,order=c(0,0,4),xreg=data.frame(price),\nfixed=c(0,NA,0,NA,NA,NA)) \nchip.m4\n\n\n\nCall:\narima(x = sales, order = c(0, 0, 4), xreg = data.frame(price), fixed = c(0, \n    NA, 0, NA, NA, NA))\n\nCoefficients:\n      ma1     ma2  ma3     ma4  intercept    price\n        0  0.2884    0  0.5416    15.8559  -2.4682\ns.e.    0  0.0794    0  0.1167     0.1909   0.1100\n\nsigma^2 estimated as 0.02623:  log likelihood = 41.02,  aic = -74.05\n\n\nNote that the regression coefficient estimate on Price is similar to that from the OLS regression fit earlier, but the standard error of the estimate is about 10% lower than that from the simple OLS regression. This illustrates the general result that the simple OLS estimator is consistent but the associated standard error is generally not trustworthy.\nDiagnosing the residuals from this model\n\n\nCode\ntsdiag(chip.m4)",
    "crumbs": [
      "R notebook: Advanced Topics"
    ]
  },
  {
    "objectID": "TSA-Lecture23-arch-garch.html",
    "href": "TSA-Lecture23-arch-garch.html",
    "title": "439/639: ARCH and GARCH Models",
    "section": "",
    "text": "ARCH (Autoregressive Conditional Heteroskedasticity) models are used to model time series data with changing variance over time. They are particularly useful in financial applications where volatility is not constant. The basic idea is to model the variance of the error term as a function of past squared errors.\nFor comparison, the ARMA models are conditionally homoscedastic, meaning that the variance of the error term is constant over time. In contrast, ARCH models allow for the variance to change over time, which is more realistic for many financial time series. The ARCH model was introduced by Robert Engle in 1982. The basic ARCH(q) model can be expressed as:\n\\[\nY_t = \\mu + \\sigma^2_t\\epsilon_t\n\\]\nwhere \\(\\epsilon_t\\) is the error term, and it is assumed to be normally distributed with mean 0 and variance 1. The variance \\(\\sigma^2_t\\) is modeled as a function of past squared errors:\n\\[\n\\sigma^2_t = \\alpha_0 + \\alpha_1 \\epsilon^2_{t-1} + \\alpha_2 \\epsilon^2_{t-2} + ... + \\alpha_q \\epsilon^2_{t-q}\n\\]\nwhere \\(\\alpha_0 &gt; 0\\) and \\(\\alpha_i \\geq 0\\) for \\(i=1,2,...,q\\). The parameters \\(\\alpha_0, \\alpha_1, ..., \\alpha_q\\) are estimated from the data.\nFor example, ARCH(1) can be expressed as: \\[\nY_t = \\mu + \\sigma^2_t\\epsilon_t\n\\]\n\\[\n\\sigma^2_t = \\alpha_0 + \\alpha_1 \\epsilon^2_{t-1}\n\\]\nwhere \\(\\alpha_0 &gt; 0\\) and \\(\\alpha_1 \\geq 0\\). The ARCH(1) model captures the idea that the current variance is a function of the previous squared error.\n\n\nCode\nlibrary(TSA)\n# Load the CREF data\ndata(CREF) \nplot(CREF)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate the returns\nr.cref &lt;- diff(log(CREF))*100\nplot(r.cref) \nabline(h=0)\ntitle(main=\"CREF Returns\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot ACF and PACF\nacf(r.cref, main=\"ACF of CREF Returns\")\n\n\n\n\n\n\n\n\n\nCode\npacf(r.cref, main=\"PACF of CREF Returns\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot ACF and PACF of absolute returns\nacf(abs(r.cref))\ntitle(main=\"ACF of Absolute CREF Returns\")\n\n\n\n\n\n\n\n\n\nCode\npacf(abs(r.cref))\ntitle(main=\"PACF of Absolute CREF Returns\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot ACF and PACF of squared returns\nacf(r.cref^2, main=\"ACF of Squared CREF Returns\")\n\n\n\n\n\n\n\n\n\nCode\npacf(r.cref^2, main=\"PACF of Squared CREF Returns\")\n\n\n\n\n\n\n\n\n\nNote, that the ACF and PACF of the squared returns show significant spikes, indicating the presence of ARCH effects, but ACF and PACF of the original returns shows white noise.\n\n\nCode\n# Perform McLeod-Li test for ARCH effects\nMcLeod.Li.test(y=r.cref)\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(r.cref)\nqqline(r.cref)\n\n\n\n\n\n\n\n\n\nCode\nshapiro.test(r.cref)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  r.cref\nW = 0.99324, p-value = 0.02412",
    "crumbs": [
      "R notebook: Advanced Topics"
    ]
  },
  {
    "objectID": "TSA-Lecture23-arch-garch.html#fitting-arima-and-garch-models-to-simulated-garch11",
    "href": "TSA-Lecture23-arch-garch.html#fitting-arima-and-garch-models-to-simulated-garch11",
    "title": "439/639: ARCH and GARCH Models",
    "section": "2.1 Fitting ARIMA and GARCH models to simulated GARCH(1,1)",
    "text": "2.1 Fitting ARIMA and GARCH models to simulated GARCH(1,1)\n\n\nCode\narima(abs(garch11.sim),order=c(1,0,1))\n\n\n\nCall:\narima(x = abs(garch11.sim), order = c(1, 0, 1))\n\nCoefficients:\n         ar1      ma1  intercept\n      0.9821  -0.9445     0.5077\ns.e.  0.0134   0.0220     0.0499\n\nsigma^2 estimated as 0.1486:  log likelihood = -232.97,  aic = 471.94\n\n\nCode\ng1 &lt;- garch(garch11.sim,order=c(2,2))\n\n\n\n ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** \n\n     I     INITIAL X(I)        D(I)\n\n     1     3.294364e-01     1.000e+00\n     2     5.000000e-02     1.000e+00\n     3     5.000000e-02     1.000e+00\n     4     5.000000e-02     1.000e+00\n     5     5.000000e-02     1.000e+00\n\n    IT   NF      F         RELDF    PRELDF    RELDX   STPPAR   D*STEP   NPRELDF\n     0    1  2.661e+01\n     1    4  2.641e+01  7.54e-03  1.69e-02  3.7e-02  5.1e+02  3.0e-02  4.31e+00\n     2    5  2.609e+01  1.21e-02  1.87e-02  4.3e-02  2.0e+00  3.0e-02  1.50e+01\n     3    6  2.573e+01  1.40e-02  1.46e-02  3.3e-02  2.0e+00  3.0e-02  6.06e+00\n     4    9  2.279e+01  1.14e-01  1.06e-01  3.4e-01  1.8e+00  2.4e-01  6.46e+00\n     5   11  2.225e+01  2.39e-02  2.80e-02  6.9e-02  2.0e+00  4.7e-02  2.82e+02\n     6   12  2.157e+01  3.03e-02  3.31e-02  5.4e-02  2.0e+00  4.7e-02  3.50e+02\n     7   13  2.111e+01  2.17e-02  2.66e-02  5.4e-02  2.0e+00  4.7e-02  1.76e+02\n     8   15  2.105e+01  2.88e-03  1.23e-02  1.9e-02  2.0e+00  1.7e-02  3.37e+01\n     9   16  2.080e+01  1.15e-02  1.42e-02  2.0e-02  2.0e+00  1.7e-02  5.77e+00\n    10   17  2.056e+01  1.15e-02  2.28e-02  4.4e-02  2.0e+00  3.5e-02  4.10e+00\n    11   20  2.052e+01  1.88e-03  3.51e-03  2.3e-03  6.1e+00  2.7e-03  1.16e+01\n    12   21  2.049e+01  1.79e-03  1.79e-03  2.5e-03  2.2e+00  2.7e-03  1.86e+01\n    13   22  2.042e+01  3.35e-03  3.41e-03  6.1e-03  2.0e+00  5.4e-03  1.91e+01\n    14   25  2.002e+01  1.98e-02  2.64e-02  5.0e-02  2.0e+00  5.4e-02  1.85e+01\n    15   29  1.997e+01  2.07e-03  3.54e-03  1.8e-03  4.2e+00  2.0e-03  8.73e-01\n    16   30  1.994e+01  1.59e-03  1.61e-03  2.1e-03  2.7e+00  2.0e-03  5.52e-01\n    17   31  1.989e+01  2.73e-03  2.79e-03  3.8e-03  2.0e+00  4.0e-03  5.91e-01\n    18   34  1.958e+01  1.53e-02  2.11e-02  3.6e-02  1.9e+00  3.7e-02  5.99e-01\n    19   35  1.930e+01  1.45e-02  2.02e-02  3.3e-02  1.9e+00  3.7e-02  6.31e-01\n    20   36  1.907e+01  1.21e-02  3.55e-02  2.9e-02  1.8e+00  3.7e-02  3.27e-01\n    21   37  1.865e+01  2.17e-02  3.50e-02  2.5e-02  1.9e+00  3.7e-02  4.75e-01\n    22   39  1.857e+01  4.21e-03  5.14e-03  2.5e-03  2.0e+00  3.7e-03  1.23e-01\n    23   42  1.847e+01  5.60e-03  5.64e-03  7.2e-03  1.6e+00  9.6e-03  1.21e-01\n    24   44  1.826e+01  1.13e-02  1.17e-02  1.7e-02  1.3e+00  1.9e-02  1.66e-01\n    25   47  1.825e+01  3.99e-04  4.53e-04  2.1e-04  2.9e+00  3.8e-04  2.11e-01\n    26   51  1.808e+01  9.32e-03  1.40e-02  1.3e-02  3.3e+00  2.3e-02  2.30e-01\n    27   53  1.803e+01  2.97e-03  5.38e-03  3.1e-03  2.0e+00  4.6e-03  1.77e-01\n    28   54  1.798e+01  2.94e-03  3.01e-03  4.0e-03  2.0e+00  4.6e-03  1.72e-01\n    29   56  1.797e+01  6.34e-04  6.77e-04  7.7e-04  2.2e+00  9.2e-04  1.66e-01\n    30   57  1.795e+01  1.10e-03  1.12e-03  1.6e-03  2.0e+00  1.8e-03  1.62e-01\n    31   59  1.792e+01  1.49e-03  1.59e-03  3.0e-03  2.0e+00  3.7e-03  1.58e-01\n    32   61  1.791e+01  5.48e-04  5.63e-04  6.0e-04  2.0e+00  7.3e-04  1.30e-01\n    33   62  1.790e+01  6.99e-04  7.34e-04  1.1e-03  2.0e+00  1.5e-03  1.22e-01\n    34   64  1.789e+01  2.65e-04  2.50e-04  2.3e-04  2.3e+00  2.9e-04  1.18e-01\n    35   66  1.789e+01  5.43e-05  5.50e-05  5.2e-05  1.5e+01  5.9e-05  1.17e-01\n    36   68  1.789e+01  1.03e-04  1.03e-04  1.0e-04  4.4e+00  1.2e-04  1.19e-01\n    37   70  1.789e+01  2.08e-05  2.03e-05  2.0e-05  4.4e+02  2.3e-05  1.19e-01\n    38   72  1.789e+01  4.14e-05  4.05e-05  4.1e-05  4.7e+01  4.7e-05  1.28e-01\n    39   74  1.789e+01  8.27e-06  8.07e-06  8.1e-06  9.6e+02  9.4e-06  1.28e-01\n    40   76  1.789e+01  1.65e-06  1.61e-06  1.6e-06  4.8e+03  1.9e-06  1.30e-01\n    41   78  1.789e+01  3.30e-07  3.23e-07  3.3e-07  2.4e+04  3.8e-07  1.30e-01\n    42   80  1.789e+01  6.61e-07  6.45e-07  6.5e-07  3.0e+03  7.5e-07  1.30e-01\n    43   82  1.789e+01  1.32e-07  1.29e-07  1.3e-07  6.0e+04  1.5e-07  1.30e-01\n    44   84  1.789e+01  2.64e-07  2.58e-07  2.6e-07  7.6e+03  3.0e-07  1.30e-01\n    45   86  1.789e+01  5.29e-07  5.16e-07  5.2e-07  3.8e+03  6.0e-07  1.30e-01\n    46   88  1.789e+01  1.06e-07  1.03e-07  1.0e-07  7.6e+04  1.2e-07  1.30e-01\n    47   90  1.789e+01  2.11e-08  2.06e-08  2.1e-08  3.8e+05  2.4e-08  1.30e-01\n    48   92  1.789e+01  4.23e-08  4.13e-08  4.2e-08  4.7e+04  4.8e-08  1.30e-01\n    49   94  1.789e+01  8.46e-09  8.26e-09  8.3e-09  9.4e+05  9.6e-09  1.30e-01\n    50   96  1.789e+01  1.69e-08  1.65e-08  1.7e-08  1.2e+05  1.9e-08  1.30e-01\n    51   98  1.789e+01  3.38e-09  3.30e-09  3.3e-09  2.4e+06  3.8e-09  1.30e-01\n    52  100  1.789e+01  6.77e-10  6.61e-10  6.7e-10  1.2e+07  7.7e-10  1.30e-01\n    53  102  1.789e+01  1.35e-09  1.32e-09  1.3e-09  1.5e+06  1.5e-09  1.30e-01\n    54  104  1.789e+01  2.71e-10  2.64e-10  2.7e-10  3.0e+07  3.1e-10  1.30e-01\n    55  106  1.789e+01  5.41e-10  5.28e-10  5.3e-10  3.7e+06  6.2e-10  1.30e-01\n    56  108  1.789e+01  1.08e-10  1.06e-10  1.1e-10  7.4e+07  1.2e-10  1.30e-01\n    57  111  1.789e+01  8.66e-10  8.46e-10  8.5e-10  2.3e+06  9.9e-10  1.30e-01\n    58  114  1.789e+01  1.73e-11  1.69e-11  1.7e-11  4.6e+08  2.0e-11  1.30e-01\n    59  116  1.789e+01  3.46e-11  3.38e-11  3.4e-11  5.8e+07  3.9e-11  1.30e-01\n    60  119  1.789e+01  6.91e-13  6.76e-13  6.8e-13  1.2e+10  7.9e-13  1.30e-01\n    61  121  1.789e+01  1.39e-12  1.35e-12  1.4e-12  1.4e+09  1.6e-12  1.30e-01\n    62  123  1.789e+01  2.75e-13  2.71e-13  2.7e-13  2.9e+10  3.2e-13  1.30e-01\n    63  125  1.789e+01  5.34e-14  5.41e-14  5.5e-14  1.4e+11  6.3e-14  1.30e-01\n    64  127  1.789e+01  1.12e-13  1.08e-13  1.1e-13  1.8e+10  1.3e-13  1.30e-01\n    65  129  1.789e+01  2.03e-14  2.16e-14  2.2e-14  3.6e+11  2.5e-14  1.30e-01\n    66  131  1.789e+01  6.16e-15  4.33e-15  4.4e-15  1.8e+12  5.0e-15  1.31e-01\n    67  133  1.789e+01  8.54e-15  8.66e-15  8.7e-15  2.3e+11  1.0e-14  1.36e-01\n    68  134  1.789e+01 -5.59e+08  1.73e-14  1.7e-14  4.5e+11  2.0e-14  1.29e-01\n\n ***** FALSE CONVERGENCE *****\n\n FUNCTION     1.788786e+01   RELDX        1.746e-14\n FUNC. EVALS     134         GRAD. EVALS      68\n PRELDF       1.732e-14      NPRELDF      1.288e-01\n\n     I      FINAL X(I)        D(I)          G(I)\n\n     1    1.835300e-02     1.000e+00    -3.499e+00\n     2    1.728230e-16     1.000e+00     5.063e+00\n     3    1.135831e-01     1.000e+00     1.355e+01\n     4    3.368745e-01     1.000e+00    -2.752e+00\n     5    5.099761e-01     1.000e+00    -2.577e+00\n\n\nCode\nsummary(g1)\n\n\n\nCall:\ngarch(x = garch11.sim, order = c(2, 2))\n\nModel:\nGARCH(2,2)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-3.346827 -0.631881  0.008473  0.736112  3.202344 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)  \na0 1.835e-02   1.515e-02    1.211   0.2257  \na1 1.728e-16   4.723e-02    0.000   1.0000  \na2 1.136e-01   5.855e-02    1.940   0.0524 .\nb1 3.369e-01   3.696e-01    0.911   0.3621  \nb2 5.100e-01   3.575e-01    1.426   0.1538  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 0.41859, df = 2, p-value = 0.8112\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.005298, df = 1, p-value = 0.942\n\n\nCode\ng2 &lt;- garch(garch11.sim,order=c(1,1))\n\n\n\n ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** \n\n     I     INITIAL X(I)        D(I)\n\n     1     3.706160e-01     1.000e+00\n     2     5.000000e-02     1.000e+00\n     3     5.000000e-02     1.000e+00\n\n    IT   NF      F         RELDF    PRELDF    RELDX   STPPAR   D*STEP   NPRELDF\n     0    1  2.773e+01\n     1    5  2.773e+01  2.16e-04  4.35e-04  4.2e-03  7.0e+02  4.2e-03  1.52e-01\n     2    6  2.772e+01  2.97e-04  3.53e-04  5.2e-03  2.0e+00  4.2e-03  2.85e-01\n     3    7  2.770e+01  5.63e-04  6.26e-04  8.8e-03  2.0e+00  8.3e-03  1.97e-01\n     4   11  2.652e+01  4.27e-02  1.90e-02  3.8e-01  9.4e-01  2.7e-01  1.56e-01\n     5   12  2.385e+01  1.01e-01  1.53e-01  4.4e-01  2.0e+00  5.3e-01  4.52e+01\n     6   15  2.356e+01  1.24e-02  1.10e-01  6.4e-03  1.3e+01  1.0e-02  3.35e+00\n     7   16  2.290e+01  2.81e-02  2.74e-02  6.5e-03  2.0e+00  1.0e-02  9.28e+00\n     8   18  2.060e+01  1.00e-01  1.90e-01  4.7e-02  6.2e+00  7.5e-02  1.05e+00\n     9   20  1.952e+01  5.22e-02  4.29e-02  3.5e-02  2.0e+00  5.9e-02  5.80e+00\n    10   22  1.809e+01  7.32e-02  8.53e-02  6.2e-02  2.0e+00  1.2e-01  3.74e+01\n    11   26  1.785e+01  1.32e-02  2.55e-02  7.9e-04  3.4e+00  1.7e-03  1.17e+01\n    12   30  1.765e+01  1.13e-02  1.15e-02  6.9e-03  2.0e+00  1.4e-02  9.52e+00\n    13   31  1.746e+01  1.08e-02  2.06e-02  1.4e-02  2.0e+00  2.8e-02  1.10e+00\n    14   34  1.744e+01  9.37e-04  2.19e-03  1.4e-04  1.1e+01  2.8e-04  1.37e-01\n    15   35  1.744e+01  1.00e-04  1.03e-04  1.4e-04  2.0e+00  2.8e-04  3.18e-02\n    16   36  1.744e+01  7.34e-05  9.97e-05  2.7e-04  2.0e+00  5.6e-04  2.24e-02\n    17   37  1.743e+01  2.06e-04  3.04e-04  5.1e-04  2.0e+00  1.1e-03  1.40e-02\n    18   38  1.743e+01  1.16e-04  1.59e-04  4.8e-04  1.9e+00  1.1e-03  3.80e-03\n    19   39  1.743e+01  4.11e-05  7.69e-05  5.1e-04  1.3e+00  1.1e-03  2.31e-04\n    20   40  1.743e+01  8.47e-06  1.29e-05  1.3e-04  0.0e+00  3.0e-04  1.29e-05\n    21   41  1.743e+01  2.90e-06  1.57e-05  9.8e-05  0.0e+00  2.4e-04  1.57e-05\n    22   42  1.743e+01  7.81e-07  7.47e-07  9.0e-06  0.0e+00  2.1e-05  7.47e-07\n    23   43  1.743e+01  2.69e-07  4.49e-08  1.6e-06  0.0e+00  4.1e-06  4.49e-08\n    24   44  1.743e+01 -2.34e-08  5.16e-11  1.9e-07  0.0e+00  4.6e-07  5.16e-11\n\n ***** RELATIVE FUNCTION CONVERGENCE *****\n\n FUNCTION     1.743183e+01   RELDX        1.897e-07\n FUNC. EVALS      44         GRAD. EVALS      24\n PRELDF       5.158e-11      NPRELDF      5.158e-11\n\n     I      FINAL X(I)        D(I)          G(I)\n\n     1    7.574525e-03     1.000e+00     9.582e-03\n     2    4.718358e-02     1.000e+00    -5.087e-04\n     3    9.353769e-01     1.000e+00     9.737e-04\n\n\nCode\nsummary(g2)\n\n\n\nCall:\ngarch(x = garch11.sim, order = c(1, 1))\n\nModel:\nGARCH(1,1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-3.307030 -0.637977  0.009156  0.741977  3.019441 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0  0.007575    0.007590    0.998   0.3183    \na1  0.047184    0.022308    2.115   0.0344 *  \nb1  0.935377    0.035839   26.100   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 0.82911, df = 2, p-value = 0.6606\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.53659, df = 1, p-value = 0.4638\n\n\n\n\nCode\nm1=garch(x=r.cref,order=c(1,1))\n\n\n\n ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** \n\n     I     INITIAL X(I)        D(I)\n\n     1     3.744782e-01     1.000e+00\n     2     5.000000e-02     1.000e+00\n     3     5.000000e-02     1.000e+00\n\n    IT   NF      F         RELDF    PRELDF    RELDX   STPPAR   D*STEP   NPRELDF\n     0    1  3.221e+01\n     1    4  3.210e+01  3.24e-03  4.13e-03  9.8e-03  1.3e+03  1.0e-02  2.74e+00\n     2    6  3.201e+01  2.94e-03  6.22e-03  2.6e-02  3.4e+02  2.0e-02  3.50e+00\n     3    7  3.199e+01  6.93e-04  2.39e-03  2.6e-02  1.9e+00  2.0e-02  1.17e-02\n     4    8  3.194e+01  1.41e-03  1.29e-03  2.2e-02  1.9e+00  2.0e-02  8.47e-03\n     5   12  2.932e+01  8.20e-02  1.41e-02  7.6e-01  0.0e+00  6.4e-01  1.79e-02\n     6   14  2.612e+01  1.09e-01  5.98e-02  8.2e-02  2.0e+00  1.3e-01  5.17e+01\n     7   16  2.593e+01  7.50e-03  4.26e-02  1.6e-02  2.0e+00  2.6e-02  1.15e+04\n     8   17  2.518e+01  2.89e-02  3.40e-02  1.3e-02  2.0e+00  2.6e-02  3.98e+03\n     9   19  2.512e+01  2.48e-03  5.40e-03  5.7e-03  2.0e+00  9.9e-03  3.35e+02\n    10   20  2.504e+01  3.15e-03  3.76e-03  5.4e-03  2.0e+00  9.9e-03  7.80e-01\n    11   21  2.483e+01  8.37e-03  1.32e-02  1.1e-02  2.0e+00  2.0e-02  1.47e+02\n    12   23  2.447e+01  1.42e-02  2.22e-02  8.4e-03  2.0e+00  1.7e-02  1.37e+03\n    13   24  2.446e+01  4.63e-04  5.05e-03  9.0e-03  2.0e+00  1.7e-02  5.17e+00\n    14   25  2.442e+01  1.63e-03  7.22e-03  3.6e-03  2.0e+00  8.6e-03  1.94e+02\n    15   26  2.410e+01  1.34e-02  1.50e-02  4.9e-03  2.0e+00  8.6e-03  8.93e+02\n    16   27  2.398e+01  4.73e-03  5.46e-03  4.4e-03  2.0e+00  8.6e-03  5.30e+02\n    17   29  2.395e+01  1.12e-03  4.00e-03  3.6e-03  2.0e+00  6.5e-03  1.05e+02\n    18   30  2.391e+01  1.72e-03  3.84e-03  3.1e-03  2.0e+00  6.5e-03  5.76e+00\n    19   31  2.386e+01  2.40e-03  2.98e-03  3.2e-03  2.0e+00  6.5e-03  6.55e+00\n    20   32  2.377e+01  3.76e-03  4.09e-03  6.6e-03  2.0e+00  1.3e-02  1.88e-01\n    21   35  2.373e+01  1.34e-03  3.22e-03  5.7e-04  2.3e+00  1.1e-03  2.23e-01\n    22   36  2.370e+01  1.65e-03  1.71e-03  5.3e-04  2.0e+00  1.1e-03  9.28e+00\n    23   37  2.366e+01  1.35e-03  1.77e-03  8.2e-04  2.0e+00  2.2e-03  4.07e+00\n    24   38  2.365e+01  6.20e-04  1.88e-03  2.4e-03  2.0e+00  4.3e-03  2.06e-01\n    25   39  2.361e+01  1.73e-03  1.73e-03  2.3e-03  2.0e+00  4.3e-03  1.72e+00\n    26   41  2.359e+01  7.23e-04  1.74e-03  1.6e-03  2.0e+00  3.6e-03  1.14e+00\n    27   42  2.355e+01  1.70e-03  2.69e-03  3.6e-03  2.0e+00  7.1e-03  8.97e-02\n    28   43  2.345e+01  4.12e-03  4.71e-03  3.7e-03  2.1e+00  7.1e-03  8.91e-01\n    29   44  2.334e+01  4.99e-03  6.54e-03  7.3e-03  2.0e+00  1.4e-02  4.65e-01\n    30   46  2.329e+01  2.11e-03  2.64e-03  1.5e-03  5.5e+00  2.9e-03  2.96e-02\n    31   49  2.328e+01  1.49e-04  2.61e-04  9.1e-05  5.6e+00  1.8e-04  1.32e+00\n    32   50  2.328e+01  7.36e-05  7.65e-05  9.7e-05  2.9e+00  1.8e-04  7.34e-01\n    33   51  2.328e+01  1.49e-04  1.55e-04  1.7e-04  2.0e+00  3.7e-04  6.83e-01\n    34   55  2.323e+01  2.02e-03  3.49e-03  4.3e-03  2.0e+00  9.3e-03  5.61e-01\n    35   59  2.323e+01  1.39e-04  2.41e-04  7.3e-05  5.1e+00  1.5e-04  2.64e-03\n    36   60  2.323e+01  1.12e-05  1.37e-05  7.1e-05  1.9e+00  1.5e-04  4.98e-04\n    37   63  2.323e+01  1.20e-04  1.67e-04  9.3e-04  9.8e-01  1.9e-03  4.28e-04\n    38   64  2.323e+01  1.90e-05  4.79e-05  9.5e-04  0.0e+00  1.9e-03  4.79e-05\n    39   65  2.323e+01  4.43e-06  1.61e-06  6.2e-05  0.0e+00  1.5e-04  1.61e-06\n    40   81  2.323e+01 -6.12e-16  2.77e-16  2.1e-14  4.9e+08  4.1e-14  9.63e-08\n\n ***** FALSE CONVERGENCE *****\n\n FUNCTION     2.322510e+01   RELDX        2.067e-14\n FUNC. EVALS      81         GRAD. EVALS      40\n PRELDF       2.771e-16      NPRELDF      9.635e-08\n\n     I      FINAL X(I)        D(I)          G(I)\n\n     1    1.632722e-02     1.000e+00     4.747e-02\n     2    4.414103e-02     1.000e+00    -1.480e-01\n     3    9.170401e-01     1.000e+00    -3.126e-02\n\n\nCode\nsummary(m1)\n\n\n\nCall:\ngarch(x = r.cref, order = c(1, 1))\n\nModel:\nGARCH(1,1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.78577 -0.61949  0.08695  0.67933  3.30810 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0   0.01633     0.01237    1.320   0.1869    \na1   0.04414     0.02097    2.105   0.0353 *  \nb1   0.91704     0.04570   20.066   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 1.0875, df = 2, p-value = 0.5806\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.77654, df = 1, p-value = 0.3782",
    "crumbs": [
      "R notebook: Advanced Topics"
    ]
  },
  {
    "objectID": "TSA-Lecture23-arch-garch.html#diagnostics-of-the-model",
    "href": "TSA-Lecture23-arch-garch.html#diagnostics-of-the-model",
    "title": "439/639: ARCH and GARCH Models",
    "section": "2.2 Diagnostics of the model",
    "text": "2.2 Diagnostics of the model\n\n\nCode\nplot(residuals(m1),type='h',ylab='Standardized Residuals')\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(residuals(m1), main=\"Normal Q-Q Plot of Residuals\") \nqqline(residuals(m1))\n\n\n\n\n\n\n\n\n\nCode\nacf(residuals(m1)^2,na.action=na.omit, main=\"ACF of Squared Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nacf(abs(residuals(m1)),na.action=na.omit, main=\"ACF of Absolute Residuals\")\n\n\n\n\n\n\n\n\n\nWe do not detect significant ARCH effects in the residuals. Thus the model is fitted adequately.",
    "crumbs": [
      "R notebook: Advanced Topics"
    ]
  }
]